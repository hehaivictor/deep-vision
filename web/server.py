#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = ["flask", "flask-cors", "anthropic", "requests"]
# ///
"""
Deep Vision Web Server - AI é©±åŠ¨ç‰ˆæœ¬

å®Œæ•´å®ç° deep-vision æŠ€èƒ½çš„æ‰€æœ‰åŠŸèƒ½ï¼š
- åŠ¨æ€ç”Ÿæˆé—®é¢˜å’Œé€‰é¡¹ï¼ˆåŸºäºä¸Šä¸‹æ–‡å’Œè¡Œä¸šçŸ¥è¯†ï¼‰
- æ™ºèƒ½è¿½é—®ï¼ˆè¯†åˆ«è¡¨é¢éœ€æ±‚ï¼ŒæŒ–æ˜æœ¬è´¨ï¼‰
- å†²çªæ£€æµ‹ï¼ˆæ£€æµ‹å›ç­”ä¸å‚è€ƒæ–‡æ¡£çš„å†²çªï¼‰
- çŸ¥è¯†å¢å¼ºï¼ˆä¸“ä¸šé¢†åŸŸä¿¡æ¯èå…¥é€‰é¡¹ï¼‰
- ç”Ÿæˆä¸“ä¸šè®¿è°ˆæŠ¥å‘Š
"""

import base64
import json
import os
import secrets
import threading
import time as _time
import requests
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# åŠ è½½é…ç½®æ–‡ä»¶
try:
    from config import (
        ANTHROPIC_API_KEY,
        ANTHROPIC_BASE_URL,
        MODEL_NAME,
        MAX_TOKENS_DEFAULT,
        MAX_TOKENS_QUESTION,
        MAX_TOKENS_REPORT,
        SERVER_HOST,
        SERVER_PORT,
        DEBUG_MODE,
        ENABLE_AI,
        ENABLE_DEBUG_LOG,
        ENABLE_WEB_SEARCH,
        ZHIPU_API_KEY,
        ZHIPU_SEARCH_ENGINE,
        SEARCH_MAX_RESULTS,
        SEARCH_TIMEOUT,
        VISION_MODEL_NAME,
        VISION_API_URL,
        ENABLE_VISION,
        MAX_IMAGE_SIZE_MB,
        SUPPORTED_IMAGE_TYPES,
    )
    print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸  æœªæ‰¾åˆ° config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    print("   è¯·å¤åˆ¶ config.example.py ä¸º config.py å¹¶å¡«å…¥å®é™…é…ç½®")
    # é»˜è®¤é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
    ANTHROPIC_BASE_URL = os.environ.get("ANTHROPIC_BASE_URL", "")
    ZHIPU_API_KEY = os.environ.get("ZHIPU_API_KEY", "")
    MODEL_NAME = os.environ.get("MODEL_NAME", "")
    MAX_TOKENS_DEFAULT = 2000
    MAX_TOKENS_QUESTION = 800
    MAX_TOKENS_REPORT = 4000
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 5001
    DEBUG_MODE = True
    ENABLE_AI = True
    ENABLE_DEBUG_LOG = True
    ENABLE_WEB_SEARCH = False
    ZHIPU_API_KEY = ""
    ZHIPU_SEARCH_ENGINE = "search_pro"
    SEARCH_MAX_RESULTS = 3
    SEARCH_TIMEOUT = 10
    # Vision é»˜è®¤é…ç½®
    VISION_MODEL_NAME = os.environ.get("VISION_MODEL_NAME", "")
    VISION_API_URL = os.environ.get("VISION_API_URL", "")
    ENABLE_VISION = True
    MAX_IMAGE_SIZE_MB = 10
    SUPPORTED_IMAGE_TYPES = ['.jpg', '.jpeg', '.png', '.gif', '.webp']

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False
    print("è­¦å‘Š: anthropic åº“æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨ AI åŠŸèƒ½")

app = Flask(__name__, static_folder='.')
CORS(app)

# è·¯å¾„é…ç½®
SKILL_DIR = Path(__file__).parent.parent.resolve()
DATA_DIR = SKILL_DIR / "data"
SESSIONS_DIR = DATA_DIR / "sessions"
REPORTS_DIR = DATA_DIR / "reports"
CONVERTED_DIR = DATA_DIR / "converted"
TEMP_DIR = DATA_DIR / "temp"
METRICS_DIR = DATA_DIR / "metrics"
SUMMARIES_DIR = DATA_DIR / "summaries"  # æ–‡æ¡£æ‘˜è¦ç¼“å­˜ç›®å½•
DELETED_REPORTS_FILE = REPORTS_DIR / ".deleted_reports.json"
DELETED_DOCS_FILE = DATA_DIR / ".deleted_docs.json"  # è½¯åˆ é™¤è®°å½•æ–‡ä»¶

for d in [SESSIONS_DIR, REPORTS_DIR, CONVERTED_DIR, TEMP_DIR, METRICS_DIR, SUMMARIES_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# ============ åœºæ™¯é…ç½®åŠ è½½å™¨ ============
import sys
sys.path.insert(0, str(SKILL_DIR))
from scripts.scenario_loader import get_scenario_loader
scenario_loader = get_scenario_loader(DATA_DIR / "scenarios")

# Web Search çŠ¶æ€è¿½è¸ªï¼ˆç”¨äºå‰ç«¯å‘¼å¸ç¯æ•ˆæœï¼‰
web_search_active = False

# ============ æ€è€ƒè¿›åº¦çŠ¶æ€è¿½è¸ªï¼ˆæ–¹æ¡ˆBï¼‰============
thinking_status = {}           # { session_id: { stage, stage_index, total_stages, message } }
thinking_status_lock = threading.Lock()

THINKING_STAGES = {
    "analyzing": {"index": 0, "message": "æ­£åœ¨åˆ†ææ‚¨çš„å›ç­”..."},
    "searching": {"index": 1, "message": "æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™..."},
    "generating": {"index": 2, "message": "æ­£åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜..."},
}

# ============ é¢„ç”Ÿæˆç¼“å­˜ï¼ˆæ™ºèƒ½é¢„ç”Ÿæˆï¼‰============
prefetch_cache = {}            # { session_id: { dimension: { question_data, created_at, valid } } }
prefetch_cache_lock = threading.Lock()
PREFETCH_TTL = 300             # é¢„ç”Ÿæˆç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰


def safe_load_session(session_file: Path) -> dict:
    """å®‰å…¨åŠ è½½ä¼šè¯æ–‡ä»¶ï¼Œå¤„ç† JSON è§£æé”™è¯¯"""
    try:
        return json.loads(session_file.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        print(f"âš ï¸ ä¼šè¯æ–‡ä»¶æŸå: {session_file}, é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âš ï¸ è¯»å–ä¼šè¯æ–‡ä»¶å¤±è´¥: {session_file}, é”™è¯¯: {e}")
        return None


def update_thinking_status(session_id: str, stage: str, has_search: bool = True):
    """æ›´æ–°æ€è€ƒè¿›åº¦çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    stage_info = THINKING_STAGES.get(stage)
    if not stage_info:
        return

    # å§‹ç»ˆä½¿ç”¨åŸå§‹çš„ stage_indexï¼Œç¡®ä¿ index å’Œ message ä¸€è‡´
    # - æœ‰æœç´¢æ—¶ï¼šåˆ†æ(0) -> æ£€ç´¢(1) -> ç”Ÿæˆ(2)
    # - æ— æœç´¢æ—¶ï¼šåˆ†æ(0) -> ç”Ÿæˆ(2)ï¼Œæ£€ç´¢é˜¶æ®µè¢«è·³è¿‡
    index = stage_info["index"]

    with thinking_status_lock:
        thinking_status[session_id] = {
            "stage": stage,
            "stage_index": index,
            "total_stages": 3,  # æ€»æ˜¯3ä¸ªé˜¶æ®µï¼Œæ— æœç´¢æ—¶æ£€ç´¢ä¼šè¢«å¿«é€Ÿè·³è¿‡
            "message": stage_info["message"],
        }


def clear_thinking_status(session_id: str):
    """æ¸…é™¤æ€è€ƒè¿›åº¦çŠ¶æ€"""
    with thinking_status_lock:
        thinking_status.pop(session_id, None)


# ============ é¢„ç”Ÿæˆç¼“å­˜å‡½æ•° ============

def get_prefetch_result(session_id: str, dimension: str) -> Optional[dict]:
    """è·å–é¢„ç”Ÿæˆç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰ï¼Œå‘½ä¸­åˆ™æ¶ˆè´¹ï¼ˆåˆ é™¤ç¼“å­˜ï¼‰

    Args:
        session_id: ä¼šè¯ID
        dimension: ç»´åº¦åç§°

    Returns:
        å‘½ä¸­åˆ™è¿”å›é—®é¢˜æ•°æ®dictï¼Œå¦åˆ™è¿”å›None
    """
    with prefetch_cache_lock:
        session_cache = prefetch_cache.get(session_id, {})
        cached = session_cache.get(dimension)
        if cached and cached.get("valid"):
            # æ£€æŸ¥TTL
            if _time.time() - cached["created_at"] < PREFETCH_TTL:
                # æ¶ˆè´¹ç¼“å­˜ï¼ˆåˆ é™¤ï¼‰
                session_cache.pop(dimension, None)
                if ENABLE_DEBUG_LOG:
                    print(f"ğŸš€ é¢„ç”Ÿæˆç¼“å­˜å‘½ä¸­: session={session_id}, dim={dimension}")
                return cached["question_data"]
            else:
                # è¿‡æœŸï¼Œæ¸…é™¤
                session_cache.pop(dimension, None)
                if ENABLE_DEBUG_LOG:
                    print(f"â° é¢„ç”Ÿæˆç¼“å­˜è¿‡æœŸ: session={session_id}, dim={dimension}")
    return None


def invalidate_prefetch(session_id: str, dimension: str = None):
    """ä½¿é¢„ç”Ÿæˆç¼“å­˜å¤±æ•ˆ

    Args:
        session_id: ä¼šè¯ID
        dimension: ç»´åº¦åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ•´ä¸ªä¼šè¯çš„ç¼“å­˜
    """
    with prefetch_cache_lock:
        if dimension:
            prefetch_cache.get(session_id, {}).pop(dimension, None)
        else:
            prefetch_cache.pop(session_id, None)


def trigger_prefetch_if_needed(session: dict, current_dimension: str):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦é¢„ç”Ÿæˆä¸‹ä¸€ç»´åº¦é¦–é¢˜ï¼Œå¦‚æœéœ€è¦åˆ™å¯åŠ¨åå°çº¿ç¨‹

    é¢„ç”Ÿæˆè§¦å‘æ¡ä»¶ï¼šå½“å‰ç»´åº¦æ­£å¼é—®é¢˜æ•° >= 2

    Args:
        session: ä¼šè¯æ•°æ®
        current_dimension: å½“å‰ç»´åº¦
    """
    session_id = session.get("session_id")
    interview_log = session.get("interview_log", [])

    # è®¡ç®—å½“å‰ç»´åº¦çš„æ­£å¼é—®é¢˜æ•°
    dim_logs = [l for l in interview_log if l.get("dimension") == current_dimension]
    formal_count = len([l for l in dim_logs if not l.get("is_follow_up", False)])

    # å½“å‰ç»´åº¦ç¬¬2é¢˜å·²å›ç­”ï¼ˆå³å°†è¿›å…¥ç¬¬3é¢˜ï¼‰ï¼Œé¢„ç”Ÿæˆä¸‹ä¸€ç»´åº¦é¦–é¢˜
    if formal_count < 2:
        return

    # ç»´åº¦é¡ºåº
    dimension_order = ['customer_needs', 'business_process', 'tech_constraints', 'project_constraints']
    current_idx = dimension_order.index(current_dimension) if current_dimension in dimension_order else -1

    # æ‰¾ä¸‹ä¸€ä¸ªæœªå®Œæˆçš„ç»´åº¦
    next_dimension = None
    for i in range(1, len(dimension_order)):
        candidate = dimension_order[(current_idx + i) % len(dimension_order)]
        cand_logs = [l for l in interview_log if l.get("dimension") == candidate]
        cand_formal = len([l for l in cand_logs if not l.get("is_follow_up", False)])
        if cand_formal < 3:
            next_dimension = candidate
            break

    if not next_dimension:
        return

    # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰
    with prefetch_cache_lock:
        existing = prefetch_cache.get(session_id, {}).get(next_dimension)
        if existing and existing.get("valid"):
            return  # å·²æœ‰æœ‰æ•ˆç¼“å­˜ï¼Œä¸é‡å¤ç”Ÿæˆ

    # å¯åŠ¨åå°é¢„ç”Ÿæˆçº¿ç¨‹
    def do_prefetch():
        try:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ”® å¼€å§‹é¢„ç”Ÿæˆ: session={session_id}, next_dim={next_dimension}")

            # é‡æ–°è¯»å–ä¼šè¯æ•°æ®ï¼ˆå¯èƒ½å·²æ›´æ–°ï¼‰
            session_file = SESSIONS_DIR / f"{session_id}.json"
            if not session_file.exists():
                return

            session_data = json.loads(session_file.read_text(encoding="utf-8"))
            next_dim_logs = [l for l in session_data.get("interview_log", [])
                           if l.get("dimension") == next_dimension]

            # æ„å»ºé¢„ç”Ÿæˆçš„ prompt
            prompt, truncated_docs = build_interview_prompt(
                session_data, next_dimension, next_dim_logs
            )

            # è°ƒç”¨ Claude API
            response = call_claude(
                prompt,
                max_tokens=MAX_TOKENS_QUESTION,
                call_type="prefetch",
                truncated_docs=truncated_docs
            )

            if response:
                # è§£æå“åº”
                result = parse_question_response(response, debug=False)
                if result:
                    result["dimension"] = next_dimension
                    result["ai_generated"] = True

                    with prefetch_cache_lock:
                        if session_id not in prefetch_cache:
                            prefetch_cache[session_id] = {}
                        prefetch_cache[session_id][next_dimension] = {
                            "question_data": result,
                            "created_at": _time.time(),
                            "topic": session_data.get("topic"),
                            "valid": True,
                        }
                    if ENABLE_DEBUG_LOG:
                        print(f"âœ… é¢„ç”Ÿæˆå®Œæˆ: session={session_id}, dim={next_dimension}")
                else:
                    if ENABLE_DEBUG_LOG:
                        print(f"âš ï¸ é¢„ç”Ÿæˆè§£æå¤±è´¥: session={session_id}, dim={next_dimension}")
        except Exception as e:
            print(f"âš ï¸ é¢„ç”Ÿæˆå¤±è´¥: {e}")

    threading.Thread(target=do_prefetch, daemon=True).start()


def prefetch_first_question(session_id: str):
    """åå°é¢„ç”Ÿæˆä¼šè¯çš„ç¬¬ä¸€ä¸ªé—®é¢˜

    åœ¨ä¼šè¯åˆ›å»ºåè°ƒç”¨ï¼Œå¼‚æ­¥ç”Ÿæˆ customer_needs ç»´åº¦çš„é¦–é¢˜ã€‚

    Args:
        session_id: ä¼šè¯ID
    """
    def do_prefetch():
        try:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ”® å¼€å§‹é¢„ç”Ÿæˆé¦–é¢˜: session={session_id}")

            session_file = SESSIONS_DIR / f"{session_id}.json"
            if not session_file.exists():
                return

            session_data = json.loads(session_file.read_text(encoding="utf-8"))

            # è·å–ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆåŠ¨æ€åœºæ™¯æ”¯æŒï¼‰
            first_dim = get_dimension_order_for_session(session_data)[0] if get_dimension_order_for_session(session_data) else "customer_needs"

            # é¦–é¢˜ä¸ä¾èµ–ä»»ä½•å†å²è®°å½•
            prompt, truncated_docs = build_interview_prompt(
                session_data, first_dim, []
            )

            response = call_claude(
                prompt,
                max_tokens=MAX_TOKENS_QUESTION,
                call_type="prefetch_first",
                truncated_docs=truncated_docs
            )

            if response:
                result = parse_question_response(response, debug=False)
                if result:
                    result["dimension"] = first_dim
                    result["ai_generated"] = True

                    with prefetch_cache_lock:
                        if session_id not in prefetch_cache:
                            prefetch_cache[session_id] = {}
                        prefetch_cache[session_id][first_dim] = {
                            "question_data": result,
                            "created_at": _time.time(),
                            "topic": session_data.get("topic"),
                            "valid": True,
                        }
                    if ENABLE_DEBUG_LOG:
                        print(f"âœ… é¦–é¢˜é¢„ç”Ÿæˆå®Œæˆ: session={session_id}")
        except Exception as e:
            print(f"âš ï¸ é¦–é¢˜é¢„ç”Ÿæˆå¤±è´¥: {e}")

    threading.Thread(target=do_prefetch, daemon=True).start()


# ============ æ€§èƒ½ç›‘æ§ç³»ç»Ÿ ============

class MetricsCollector:
    """API æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, metrics_file: Path):
        self.metrics_file = metrics_file
        self._ensure_metrics_file()

    def _ensure_metrics_file(self):
        """ç¡®ä¿æŒ‡æ ‡æ–‡ä»¶å­˜åœ¨"""
        if not self.metrics_file.exists():
            self.metrics_file.write_text(json.dumps({
                "calls": [],
                "summary": {
                    "total_calls": 0,
                    "total_timeouts": 0,
                    "total_truncations": 0,
                    "avg_response_time": 0,
                    "avg_prompt_length": 0
                }
            }, ensure_ascii=False, indent=2), encoding="utf-8")

    def record_api_call(self, call_type: str, prompt_length: int, response_time: float,
                       success: bool, timeout: bool = False, error_msg: str = None,
                       truncated_docs: list = None, max_tokens: int = None):
        """è®°å½• API è°ƒç”¨æŒ‡æ ‡"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            data = json.loads(self.metrics_file.read_text(encoding="utf-8"))

            # æ·»åŠ æ–°è®°å½•
            call_record = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "type": call_type,  # "question" or "report"
                "prompt_length": prompt_length,
                "response_time_ms": round(response_time * 1000, 2),
                "max_tokens": max_tokens,
                "success": success,
                "timeout": timeout,
                "error": error_msg,
                "truncated_docs": truncated_docs or []
            }

            data["calls"].append(call_record)

            # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
            summary = data["summary"]
            summary["total_calls"] = summary.get("total_calls", 0) + 1
            if timeout:
                summary["total_timeouts"] = summary.get("total_timeouts", 0) + 1
            if truncated_docs:
                summary["total_truncations"] = summary.get("total_truncations", 0) + len(truncated_docs)

            # è®¡ç®—å¹³å‡å€¼
            all_calls = data["calls"]
            if all_calls:
                summary["avg_response_time"] = round(
                    sum(c["response_time_ms"] for c in all_calls) / len(all_calls), 2
                )
                summary["avg_prompt_length"] = round(
                    sum(c["prompt_length"] for c in all_calls) / len(all_calls), 2
                )

            # ä¿å­˜ï¼ˆåªä¿ç•™æœ€è¿‘ 1000 æ¡è®°å½•ï¼‰
            if len(data["calls"]) > 1000:
                data["calls"] = data["calls"][-1000:]

            self.metrics_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )

        except Exception as e:
            print(f"âš ï¸  è®°å½•æŒ‡æ ‡å¤±è´¥: {e}")

    def get_statistics(self, last_n: int = None) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            data = json.loads(self.metrics_file.read_text(encoding="utf-8"))
            calls = data["calls"]

            if last_n:
                calls = calls[-last_n:]

            if not calls:
                return {
                    "total_calls": 0,
                    "message": "æš‚æ— æ•°æ®"
                }

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_calls = len(calls)
            successful_calls = sum(1 for c in calls if c["success"])
            timeout_calls = sum(1 for c in calls if c.get("timeout", False))
            truncation_events = sum(len(c.get("truncated_docs", [])) for c in calls)

            response_times = [c["response_time_ms"] for c in calls if c["success"]]
            prompt_lengths = [c["prompt_length"] for c in calls]

            stats = {
                "period": f"æœ€è¿‘ {last_n} æ¬¡è°ƒç”¨" if last_n else "å…¨éƒ¨è°ƒç”¨",
                "total_calls": total_calls,
                "successful_calls": successful_calls,
                "failed_calls": total_calls - successful_calls,
                "timeout_calls": timeout_calls,
                "timeout_rate": round(timeout_calls / total_calls * 100, 2) if total_calls > 0 else 0,
                "truncation_events": truncation_events,
                "truncation_rate": round(truncation_events / total_calls * 100, 2) if total_calls > 0 else 0,
                "avg_response_time_ms": round(sum(response_times) / len(response_times), 2) if response_times else 0,
                "max_response_time_ms": round(max(response_times), 2) if response_times else 0,
                "min_response_time_ms": round(min(response_times), 2) if response_times else 0,
                "avg_prompt_length": round(sum(prompt_lengths) / len(prompt_lengths), 2) if prompt_lengths else 0,
                "max_prompt_length": max(prompt_lengths) if prompt_lengths else 0,
            }

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            stats["recommendations"] = self._generate_recommendations(stats)

            return stats

        except Exception as e:
            return {"error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}"}

    def _generate_recommendations(self, stats: dict) -> list:
        """åŸºäºç»Ÿè®¡æ•°æ®ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # è¶…æ—¶ç‡è¿‡é«˜
        if stats["timeout_rate"] > 10:
            recommendations.append({
                "level": "critical",
                "message": f"è¶…æ—¶ç‡è¿‡é«˜ ({stats['timeout_rate']}%)ï¼Œå»ºè®®å‡å°‘æ–‡æ¡£é•¿åº¦é™åˆ¶æˆ–å®æ–½æ™ºèƒ½æ‘˜è¦"
            })
        elif stats["timeout_rate"] > 5:
            recommendations.append({
                "level": "warning",
                "message": f"è¶…æ—¶ç‡åé«˜ ({stats['timeout_rate']}%)ï¼Œéœ€è¦å…³æ³¨"
            })

        # æˆªæ–­ç‡è¿‡é«˜
        if stats["truncation_rate"] > 50:
            recommendations.append({
                "level": "warning",
                "message": f"æ–‡æ¡£æˆªæ–­é¢‘ç¹ ({stats['truncation_rate']}%)ï¼Œå»ºè®®å®æ–½æ™ºèƒ½æ‘˜è¦åŠŸèƒ½"
            })

        # Prompt è¿‡é•¿
        if stats["avg_prompt_length"] > 8000:
            recommendations.append({
                "level": "warning",
                "message": f"å¹³å‡ Prompt é•¿åº¦è¾ƒå¤§ ({stats['avg_prompt_length']} å­—ç¬¦)ï¼Œå¯èƒ½å½±å“å“åº”é€Ÿåº¦"
            })

        # å“åº”æ—¶é—´è¿‡é•¿
        if stats["avg_response_time_ms"] > 60000:
            recommendations.append({
                "level": "warning",
                "message": f"å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ ({stats['avg_response_time_ms']/1000:.1f} ç§’)ï¼Œå»ºè®®ä¼˜åŒ– Prompt é•¿åº¦"
            })

        # ä¸€åˆ‡æ­£å¸¸
        if not recommendations:
            if stats["timeout_rate"] < 5 and stats["truncation_rate"] < 30:
                recommendations.append({
                    "level": "info",
                    "message": "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œå¯è€ƒè™‘é€‚åº¦å¢åŠ æ–‡æ¡£é•¿åº¦é™åˆ¶ä»¥æå‡è´¨é‡"
                })

        return recommendations


# åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = MetricsCollector(METRICS_DIR / "api_metrics.json")

# Claude å®¢æˆ·ç«¯åˆå§‹åŒ–
claude_client = None

# æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆ
def is_valid_api_key(api_key: str) -> bool:
    """æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯é»˜è®¤å ä½ç¬¦ï¼‰"""
    if not api_key:
        return False
    placeholder_patterns = [
        "your-", "your_", "example", "test", "placeholder",
        "api-key", "apikey", "YOUR-", "YOUR_"
    ]
    api_key_lower = api_key.lower()
    for pattern in placeholder_patterns:
        if pattern in api_key_lower:
            return False
    return True

# æ£€æŸ¥é…ç½®
api_key_valid = is_valid_api_key(ANTHROPIC_API_KEY)
base_url_valid = ANTHROPIC_BASE_URL and ANTHROPIC_BASE_URL != "https://api.anthropic.com" or api_key_valid

if not api_key_valid:
    print("âš ï¸  ANTHROPIC_API_KEY æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
    print("   è¯·åœ¨ config.py ä¸­å¡«å…¥æœ‰æ•ˆçš„ API Key")
    ENABLE_AI = False

if not base_url_valid and not ANTHROPIC_BASE_URL:
    print("âš ï¸  ANTHROPIC_BASE_URL æœªé…ç½®")
    print("   è¯·åœ¨ config.py ä¸­å¡«å…¥æœ‰æ•ˆçš„ Base URL")

if ENABLE_AI and HAS_ANTHROPIC and api_key_valid:
    try:
        claude_client = anthropic.Anthropic(
            api_key=ANTHROPIC_API_KEY,
            base_url=ANTHROPIC_BASE_URL
        )
        print(f"âœ… Claude å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        print(f"   æ¨¡å‹: {MODEL_NAME}")
        print(f"   Base URL: {ANTHROPIC_BASE_URL}")

        # æµ‹è¯• API è¿æ¥
        try:
            test_response = claude_client.messages.create(
                model=MODEL_NAME,
                max_tokens=5,
                messages=[{"role": "user", "content": "Hi"}]
            )
            print(f"âœ… API è¿æ¥æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            print("   è¯·æ£€æŸ¥ API Key å’Œ Base URL æ˜¯å¦æ­£ç¡®")
            claude_client = None
    except Exception as e:
        print(f"âŒ Claude å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        claude_client = None
    except Exception as e:
        print(f"âŒ Claude å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
else:
    if not ENABLE_AI:
        print("â„¹ï¸  AI åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_AI=Falseï¼‰")
    elif not HAS_ANTHROPIC:
        print("âŒ anthropic åº“æœªå®‰è£…")
    elif not ANTHROPIC_API_KEY:
        print("âŒ æœªé…ç½® ANTHROPIC_API_KEY")


def get_utc_now() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def generate_session_id() -> str:
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    random_suffix = secrets.token_hex(4)
    return f"dv-{timestamp}-{random_suffix}"


def get_deleted_reports() -> set:
    """è·å–å·²åˆ é™¤æŠ¥å‘Šçš„åˆ—è¡¨"""
    if not DELETED_REPORTS_FILE.exists():
        return set()
    try:
        data = json.loads(DELETED_REPORTS_FILE.read_text(encoding="utf-8"))
        return set(data.get("deleted", []))
    except Exception:
        return set()


def mark_report_as_deleted(filename: str):
    """æ ‡è®°æŠ¥å‘Šä¸ºå·²åˆ é™¤ï¼ˆä¸çœŸæ­£åˆ é™¤æ–‡ä»¶ï¼‰"""
    deleted = get_deleted_reports()
    deleted.add(filename)
    DELETED_REPORTS_FILE.write_text(
        json.dumps({"deleted": list(deleted)}, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def get_deleted_docs() -> dict:
    """è·å–å·²åˆ é™¤æ–‡æ¡£çš„è®°å½•"""
    if not DELETED_DOCS_FILE.exists():
        return {"reference_materials": []}
    try:
        data = json.loads(DELETED_DOCS_FILE.read_text(encoding="utf-8"))
        # å…¼å®¹æ—§æ ¼å¼
        materials = data.get("reference_materials", [])
        materials.extend(data.get("reference_docs", []))
        materials.extend(data.get("research_docs", []))
        return {"reference_materials": materials}
    except Exception:
        return {"reference_materials": []}


def mark_doc_as_deleted(session_id: str, doc_name: str, doc_type: str = "reference_materials"):
    """æ ‡è®°æ–‡æ¡£ä¸ºå·²åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰

    Args:
        session_id: ä¼šè¯ ID
        doc_name: æ–‡æ¡£åç§°
        doc_type: æ–‡æ¡£ç±»å‹ï¼ˆé»˜è®¤ 'reference_materials'ï¼‰
    """
    deleted = get_deleted_docs()
    record = {
        "session_id": session_id,
        "doc_name": doc_name,
        "deleted_at": get_utc_now()
    }
    if "reference_materials" not in deleted:
        deleted["reference_materials"] = []
    deleted["reference_materials"].append(record)
    DELETED_DOCS_FILE.write_text(
        json.dumps(deleted, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def migrate_session_docs(session: dict) -> dict:
    """è¿ç§»æ—§ä¼šè¯æ•°æ®ï¼šå°† reference_docs + research_docs åˆå¹¶ä¸º reference_materials

    Args:
        session: ä¼šè¯æ•°æ®å­—å…¸

    Returns:
        è¿ç§»åçš„ä¼šè¯æ•°æ®
    """
    # å¦‚æœå·²ç»æœ‰ reference_materialsï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ—§å­—æ®µéœ€è¦è¿ç§»
    if "reference_materials" not in session:
        session["reference_materials"] = []

    # è¿ç§» reference_docs
    if "reference_docs" in session:
        for doc in session["reference_docs"]:
            if "source" not in doc:
                doc["source"] = "upload"
            session["reference_materials"].append(doc)
        del session["reference_docs"]

    # è¿ç§» research_docs
    if "research_docs" in session:
        for doc in session["research_docs"]:
            if "source" not in doc:
                doc["source"] = "auto"
            session["reference_materials"].append(doc)
        del session["research_docs"]

    return session


# ============ è”ç½‘æœç´¢åŠŸèƒ½ ============

class MCPClient:
    """æ™ºè°±AI MCPå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
        self.session_id = None
        self.message_id = 0

    def _get_next_id(self):
        """è·å–ä¸‹ä¸€ä¸ªæ¶ˆæ¯ID"""
        self.message_id += 1
        return self.message_id

    def _make_request(self, method: str, params: dict = None):
        """å‘é€MCP JSON-RPCè¯·æ±‚"""
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }

        # å¦‚æœæœ‰session_idï¼Œæ·»åŠ åˆ°header
        if self.session_id:
            headers["Mcp-Session-Id"] = self.session_id

        # åœ¨URLä¸­æ·»åŠ Authorizationå‚æ•°
        url = f"{self.base_url}?Authorization={self.api_key}"

        request_data = {
            "jsonrpc": "2.0",
            "id": self._get_next_id(),
            "method": method,
            "params": params or {}
        }

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“¤ MCPè¯·æ±‚: {method}")
            print(f"   å‚æ•°: {params}")

        response = requests.post(url, json=request_data, headers=headers, timeout=SEARCH_TIMEOUT)
        response.raise_for_status()

        # æ£€æŸ¥å“åº”å¤´ä¸­çš„Session ID
        if "Mcp-Session-Id" in response.headers:
            self.session_id = response.headers["Mcp-Session-Id"]
            if ENABLE_DEBUG_LOG:
                print(f"   ğŸ“ è·å¾—Session ID: {self.session_id}")

        # è§£æSSEæ ¼å¼çš„å“åº”
        response_text = response.text.strip()

        # SSEæ ¼å¼: id:1\nevent:message\ndata:{json}\n\n
        result_data = None
        for line in response_text.split('\n'):
            line = line.strip()
            if line.startswith('data:'):
                json_str = line[5:].strip()  # å»æ‰ "data:" å‰ç¼€
                try:
                    result_data = json.loads(json_str)
                    break
                except:
                    continue

        if not result_data:
            raise Exception(f"æ— æ³•è§£æSSEå“åº”: {response_text[:200]}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in result_data:
            raise Exception(f"MCPé”™è¯¯: {result_data['error']}")

        return result_data.get("result", {})

    def initialize(self):
        """åˆå§‹åŒ–MCPè¿æ¥"""
        try:
            result = self._make_request("initialize", {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {
                    "name": "deep-vision",
                    "version": "1.0.0"
                }
            })
            if ENABLE_DEBUG_LOG:
                print(f"âœ… MCPåˆå§‹åŒ–æˆåŠŸ")
            return result
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âŒ MCPåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def call_tool(self, tool_name: str, arguments: dict):
        """è°ƒç”¨MCPå·¥å…·"""
        try:
            # ç¡®ä¿å·²åˆå§‹åŒ–
            if not self.session_id:
                self.initialize()

            result = self._make_request("tools/call", {
                "name": tool_name,
                "arguments": arguments
            })

            return result
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            raise


def web_search(query: str) -> list:
    """ä½¿ç”¨æ™ºè°±AI MCP web_search_prime è¿›è¡Œè”ç½‘æœç´¢"""
    global web_search_active

    if not ENABLE_WEB_SEARCH or not ZHIPU_API_KEY or ZHIPU_API_KEY == "your-zhipu-api-key-here":
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  æœç´¢åŠŸèƒ½æœªå¯ç”¨æˆ– API Key æœªé…ç½®ï¼Œè·³è¿‡æœç´¢: {query}")
        return []

    try:
        # è®¾ç½®æœç´¢çŠ¶æ€ä¸ºæ´»åŠ¨
        web_search_active = True

        mcp_url = "https://open.bigmodel.cn/api/mcp/web_search_prime/mcp"

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” å¼€å§‹MCPæœç´¢: {query}")

        # åˆ›å»ºMCPå®¢æˆ·ç«¯
        client = MCPClient(ZHIPU_API_KEY, mcp_url)

        # è°ƒç”¨webSearchPrimeå·¥å…·ï¼ˆæ³¨æ„ï¼šå·¥å…·åæ˜¯é©¼å³°å‘½åï¼‰
        result = client.call_tool("webSearchPrime", {
            "search_query": query,
            "search_recency_filter": "noLimit",
            "content_size": "medium"
        })

        # è§£æç»“æœ
        results = []

        # MCPè¿”å›çš„contentæ˜¯ä¸€ä¸ªåˆ—è¡¨
        content_list = result.get("content", [])

        for item in content_list:
            if item.get("type") == "text":
                # æ–‡æœ¬å†…å®¹
                text = item.get("text", "")

                # å°è¯•è§£æJSONæ ¼å¼çš„æœç´¢ç»“æœ
                try:
                    import json as json_module

                    # ç¬¬ä¸€æ¬¡è§£æï¼šå»æ‰å¤–å±‚å¼•å·å’Œè½¬ä¹‰
                    if text.startswith('"') and text.endswith('"'):
                        text = json_module.loads(text)

                    # ç¬¬äºŒæ¬¡è§£æï¼šè·å–å®é™…çš„æœç´¢ç»“æœæ•°ç»„
                    search_data = json_module.loads(text)

                    # å¦‚æœæ˜¯åˆ—è¡¨å½¢å¼çš„æœç´¢ç»“æœ
                    if isinstance(search_data, list):
                        for entry in search_data[:SEARCH_MAX_RESULTS]:
                            title = entry.get("title", "")
                            content = entry.get("content", "")
                            url = entry.get("link", entry.get("url", ""))

                            if title or content:  # ç¡®ä¿æœ‰å®é™…å†…å®¹
                                results.append({
                                    "type": "result",
                                    "title": title[:100] if title else "æœç´¢ç»“æœ",
                                    "content": content[:300],
                                    "url": url
                                })
                    # å¦‚æœæ˜¯å•ä¸ªç»“æœ
                    elif isinstance(search_data, dict):
                        title = search_data.get("title", "")
                        content = search_data.get("content", text[:300])
                        url = search_data.get("link", search_data.get("url", ""))

                        results.append({
                            "type": "result",
                            "title": title[:100] if title else "æœç´¢ç»“æœ",
                            "content": content[:300],
                            "url": url
                        })
                except Exception as parse_error:
                    if ENABLE_DEBUG_LOG:
                        print(f"âš ï¸  è§£ææœç´¢ç»“æœå¤±è´¥: {parse_error}")
                        print(f"   åŸå§‹æ–‡æœ¬å‰200å­—ç¬¦: {text[:200]}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥ä½œä¸ºæ–‡æœ¬ç»“æœ
                    results.append({
                        "type": "result",
                        "title": "æœç´¢ç»“æœ",
                        "content": text[:300],
                        "url": ""
                    })

        if ENABLE_DEBUG_LOG:
            print(f"âœ… MCPæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")

        # æœç´¢å®Œæˆï¼Œé‡ç½®çŠ¶æ€
        web_search_active = False
        return results

    except requests.exceptions.Timeout:
        print(f"â±ï¸  æœç´¢è¶…æ—¶: {query}")
        web_search_active = False
        return []
    except Exception as e:
        print(f"âŒ MCPæœç´¢å¤±è´¥: {e}")
        if ENABLE_DEBUG_LOG:
            import traceback
            traceback.print_exc()
        web_search_active = False
        return []


def should_search(topic: str, dimension: str, context: dict) -> bool:
    """
    è§„åˆ™é¢„åˆ¤ï¼šå¿«é€Ÿåˆ¤æ–­æ˜¯å¦å¯èƒ½éœ€è¦è”ç½‘æœç´¢ï¼ˆå…œåº•è§„åˆ™ï¼‰
    è¿”å› True è¡¨ç¤º"å¯èƒ½éœ€è¦"ï¼Œåç»­ä¼šäº¤ç»™ AI åšæœ€ç»ˆåˆ¤æ–­
    """
    if not ENABLE_WEB_SEARCH:
        return False

    # ========== æ‰©å±•çš„å…³é”®è¯åº“ ==========

    # æŠ€æœ¯å…³é”®è¯
    tech_keywords = [
        "æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æ¡†æ¶", "å·¥å…·", "è½¯ä»¶", "åº”ç”¨", "æ¶æ„",
        "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¤§æ¨¡å‹", "LLM", "GPT",
        "äº‘", "SaaS", "PaaS", "IaaS", "å¾®æœåŠ¡", "å®¹å™¨", "Docker", "K8s", "Kubernetes",
        "æ•°æ®åº“", "ä¸­é—´ä»¶", "API", "é›†æˆ", "éƒ¨ç½²", "è¿ç»´", "DevOps",
        "å‰ç«¯", "åç«¯", "å…¨æ ˆ", "ç§»åŠ¨ç«¯", "App", "å°ç¨‹åº"
    ]

    # å‚ç›´è¡Œä¸šå…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    industry_keywords = [
        # åŒ»ç–—å¥åº·
        "åŒ»é™¢", "åŒ»ç–—", "HIS", "LIS", "PACS", "EMR", "ç”µå­ç—…å†", "DRG", "åŒ»ä¿",
        "è¯Šæ‰€", "è¯æˆ¿", "å¤„æ–¹", "æŒ‚å·", "é—¨è¯Š", "ä½é™¢", "æŠ¤ç†", "CDSS",
        # é‡‘è
        "é“¶è¡Œ", "ä¿é™©", "è¯åˆ¸", "åŸºé‡‘", "ä¿¡æ‰˜", "æ”¯ä»˜", "æ¸…ç®—", "é£æ§",
        "åæ´—é’±", "å¾ä¿¡", "èµ„ç®¡", "ç†è´¢", "è´·æ¬¾", "ä¿¡ç”¨å¡",
        # æ•™è‚²
        "å­¦æ ¡", "æ•™è‚²", "åŸ¹è®­", "è¯¾ç¨‹", "æ•™å­¦", "å­¦ç”Ÿ", "è€ƒè¯•", "æ‹›ç”Ÿ",
        "åœ¨çº¿æ•™è‚²", "ç½‘è¯¾", "åŒå‡", "æ–°è¯¾æ ‡",
        # åˆ¶é€ 
        "å·¥å‚", "åˆ¶é€ ", "ç”Ÿäº§", "è½¦é—´", "MES", "ERP", "PLM", "SCM", "WMS",
        "å·¥ä¸šäº’è”ç½‘", "æ™ºèƒ½åˆ¶é€ ", "æ•°å­—å­ªç”Ÿ", "è´¨æ£€", "è®¾å¤‡", "äº§çº¿",
        # é›¶å”®ç”µå•†
        "é›¶å”®", "ç”µå•†", "é—¨åº—", "å•†åŸ", "è®¢å•", "åº“å­˜", "ç‰©æµ", "é…é€",
        "ä¼šå‘˜", "è¥é”€", "ä¿ƒé”€", "CRM", "POS",
        # æ”¿åŠ¡
        "æ”¿åºœ", "æ”¿åŠ¡", "å®¡æ‰¹", "åŠäº‹", "å…¬å…±æœåŠ¡", "æ™ºæ…§åŸå¸‚", "æ•°å­—æ”¿åºœ",
        # èƒ½æº
        "ç”µåŠ›", "èƒ½æº", "ç”µç½‘", "æ–°èƒ½æº", "å…‰ä¼", "é£ç”µ", "å‚¨èƒ½", "å……ç”µæ¡©",
        # äº¤é€šç‰©æµ
        "äº¤é€š", "ç‰©æµ", "è¿è¾“", "ä»“å‚¨", "TMS", "è°ƒåº¦", "è½¦é˜Ÿ"
    ]

    # åˆè§„æ”¿ç­–å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    compliance_keywords = [
        "åˆè§„", "æ ‡å‡†", "è§„èŒƒ", "è®¤è¯", "ç­‰ä¿", "ISO", "GDPR", "éšç§",
        "å®‰å…¨", "å®¡è®¡", "æ³•è§„", "æ”¿ç­–", "ç›‘ç®¡", "èµ„è´¨", "è®¸å¯è¯"
    ]

    # æ—¶æ•ˆæ€§å…³é”®è¯
    time_sensitive_keywords = [
        "æœ€æ–°", "å½“å‰", "ç°åœ¨", "è¿‘æœŸ", "ä»Šå¹´", "æ˜å¹´",
        "2024", "2025", "2026", "2027",
        "è¶‹åŠ¿", "æœªæ¥", "å‘å±•", "åŠ¨æ€", "å˜åŒ–", "æ›´æ–°",
        "å¸‚åœº", "è¡Œæƒ…", "ç«å“", "å¯¹æ‰‹", "ç°çŠ¶"
    ]

    # ä¸ç¡®å®šæ€§/ä¸“ä¸šæ€§å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    uncertainty_keywords = [
        "æ€ä¹ˆé€‰", "å¦‚ä½•é€‰æ‹©", "å“ªä¸ªå¥½", "æ¨è", "å»ºè®®", "æ¯”è¾ƒ",
        "æœ€ä½³å®è·µ", "ä¸šç•Œ", "å¤´éƒ¨", "é¢†å…ˆ", "ä¸»æµ", "æ ‡æ†"
    ]

    all_keywords = (tech_keywords + industry_keywords + compliance_keywords +
                   time_sensitive_keywords + uncertainty_keywords)

    # å¦‚æœä¸»é¢˜åŒ…å«ä»»ä½•å…³é”®è¯ï¼Œæ ‡è®°ä¸º"å¯èƒ½éœ€è¦æœç´¢"
    for keyword in all_keywords:
        if keyword.lower() in topic.lower():
            return True

    # æŠ€æœ¯çº¦æŸç»´åº¦é€šå¸¸éœ€è¦æœç´¢
    if dimension == "tech_constraints":
        return True

    return False


def ai_evaluate_search_need(topic: str, dimension: str, context: dict, recent_qa: list) -> dict:
    """
    AI è‡ªä¸»åˆ¤æ–­ï¼šè®© AI è¯„ä¼°æ˜¯å¦éœ€è¦è”ç½‘æœç´¢
    è¿”å›: { "need_search": bool, "reason": str, "search_query": str }
    """
    global claude_client

    if not ENABLE_WEB_SEARCH or not claude_client:
        return {"need_search": False, "reason": "æœç´¢åŠŸèƒ½æœªå¯ç”¨", "search_query": ""}

    search_dim_info = get_dimension_info_for_session(context) if context else DIMENSION_INFO
    dim_info = search_dim_info.get(dimension, {})
    dim_name = dim_info.get("name", dimension)

    # æ„å»ºæœ€è¿‘çš„é—®ç­”ä¸Šä¸‹æ–‡
    recent_context = ""
    if recent_qa:
        recent_context = "\n".join([
            f"Q: {qa.get('question', '')}\nA: {qa.get('answer', '')}"
            for qa in recent_qa[-3:]  # åªå–æœ€è¿‘3æ¡
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢å†³ç­–åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­åœ¨å½“å‰è®¿è°ˆåœºæ™¯ä¸‹ï¼Œæ˜¯å¦éœ€è¦è”ç½‘æœç´¢æ¥è·å–æ›´å‡†ç¡®ã€æ›´ä¸“ä¸šçš„ä¿¡æ¯ã€‚

## å½“å‰è®¿è°ˆä¿¡æ¯
- è®¿è°ˆä¸»é¢˜ï¼š{topic}
- å½“å‰ç»´åº¦ï¼š{dim_name}
- æœ€è¿‘é—®ç­”ï¼š
{recent_context if recent_context else "ï¼ˆå°šæœªå¼€å§‹é—®ç­”ï¼‰"}

## åˆ¤æ–­æ ‡å‡†
è¯·è¯„ä¼°ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ï¼š

1. **çŸ¥è¯†æ—¶æ•ˆæ€§**ï¼šæ˜¯å¦æ¶‰åŠè¿‘1-2å¹´çš„æ”¿ç­–ã€å¸‚åœºã€æŠ€æœ¯å˜åŒ–ï¼Ÿ
2. **ä¸“ä¸šé¢†åŸŸæ·±åº¦**ï¼šæ˜¯å¦æ¶‰åŠä½ å¯èƒ½ä¸å¤Ÿç†Ÿæ‚‰çš„å‚ç›´è¡Œä¸šç»†èŠ‚ï¼ˆå¦‚åŒ»ç–—ç¼–ç è§„åˆ™ã€é‡‘èç›‘ç®¡è¦æ±‚ã€è¡Œä¸šæ ‡å‡†å‚æ•°ç­‰ï¼‰ï¼Ÿ
3. **ç«å“/å¸‚åœºä¿¡æ¯**ï¼šæ˜¯å¦éœ€è¦äº†è§£å¸‚åœºç°çŠ¶ã€ç«äº‰å¯¹æ‰‹ã€è¡Œä¸šå¤´éƒ¨äº§å“ï¼Ÿ
4. **æœ€ä½³å®è·µå‚è€ƒ**ï¼šæ˜¯å¦éœ€è¦äº†è§£ä¸šç•Œçš„æœ€æ–°åšæ³•ã€æˆåŠŸæ¡ˆä¾‹ï¼Ÿ
5. **æ•°æ®/æŒ‡æ ‡å‚è€ƒ**ï¼šæ˜¯å¦éœ€è¦äº†è§£è¡Œä¸šåŸºå‡†æ•°æ®ã€å¸¸è§å‚æ•°èŒƒå›´ï¼Ÿ

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š
{{
    "need_search": trueæˆ–false,
    "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±ï¼ˆ20å­—ä»¥å†…ï¼‰",
    "search_query": "å¦‚æœéœ€è¦æœç´¢ï¼Œç»™å‡ºæœ€ä½³æœç´¢è¯ï¼ˆè¦ç²¾å‡†ã€å…·ä½“ï¼Œ15å­—ä»¥å†…ï¼‰ï¼›ä¸éœ€è¦æœç´¢åˆ™ç•™ç©º"
}}"""

    try:
        response = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=200,
            messages=[{"role": "user", "content": prompt}]
        )

        result_text = response.content[0].text.strip()

        # å°è¯•è§£æ JSON
        import json
        # å¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0].strip()
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0].strip()

        result = json.loads(result_text)

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– AIæœç´¢å†³ç­–: need_search={result.get('need_search')}, reason={result.get('reason')}")

        return {
            "need_search": result.get("need_search", False),
            "reason": result.get("reason", ""),
            "search_query": result.get("search_query", "")
        }

    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  AIæœç´¢å†³ç­–å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è¿”å›ä¸æœç´¢ï¼Œé¿å…é˜»å¡æµç¨‹
        return {"need_search": False, "reason": f"å†³ç­–å¤±è´¥: {e}", "search_query": ""}


def smart_search_decision(topic: str, dimension: str, context: dict, recent_qa: list = None) -> tuple:
    """
    æ™ºèƒ½æœç´¢å†³ç­–ï¼šè§„åˆ™é¢„åˆ¤ + AI æœ€ç»ˆåˆ¤æ–­
    è¿”å›: (need_search: bool, search_query: str, reason: str)
    """
    if not ENABLE_WEB_SEARCH:
        return (False, "", "æœç´¢åŠŸèƒ½æœªå¯ç”¨")

    # ç¬¬ä¸€æ­¥ï¼šè§„åˆ™é¢„åˆ¤
    rule_suggests_search = should_search(topic, dimension, context)

    if not rule_suggests_search:
        # è§„åˆ™åˆ¤æ–­ä¸éœ€è¦æœç´¢ï¼Œä½†è®© AI åšäºŒæ¬¡ç¡®è®¤ï¼ˆå¯èƒ½æ¼æ‰çš„åœºæ™¯ï¼‰
        ai_result = ai_evaluate_search_need(topic, dimension, context, recent_qa or [])
        if ai_result["need_search"]:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ” è§„åˆ™æœªè§¦å‘ï¼Œä½†AIå»ºè®®æœç´¢: {ai_result['reason']}")
            return (True, ai_result["search_query"], f"AIå»ºè®®: {ai_result['reason']}")
        else:
            return (False, "", "è§„åˆ™å’ŒAIå‡åˆ¤æ–­ä¸éœ€è¦æœç´¢")

    # ç¬¬äºŒæ­¥ï¼šè§„åˆ™å»ºè®®æœç´¢ï¼Œè®© AI ç”Ÿæˆç²¾å‡†æœç´¢è¯
    ai_result = ai_evaluate_search_need(topic, dimension, context, recent_qa or [])

    if ai_result["need_search"] and ai_result["search_query"]:
        # AI ç¡®è®¤éœ€è¦æœç´¢ï¼Œä½¿ç”¨ AI ç”Ÿæˆçš„æœç´¢è¯
        return (True, ai_result["search_query"], ai_result["reason"])
    elif ai_result["need_search"]:
        # AI ç¡®è®¤éœ€è¦ä½†æ²¡ç»™æœç´¢è¯ï¼Œä½¿ç”¨å…œåº•æ¨¡æ¿
        fallback_query = generate_search_query(topic, dimension, context)
        return (True, fallback_query, "AIç¡®è®¤éœ€è¦ï¼Œä½¿ç”¨æ¨¡æ¿æœç´¢è¯")
    else:
        # AI åˆ¤æ–­å®é™…ä¸éœ€è¦æœç´¢ï¼ˆè§„åˆ™è¯¯è§¦å‘ï¼‰
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” è§„åˆ™è§¦å‘ä½†AIåˆ¤æ–­ä¸éœ€è¦: {ai_result['reason']}")
        return (False, "", f"AIåˆ¤æ–­ä¸éœ€è¦: {ai_result['reason']}")


def generate_search_query(topic: str, dimension: str, context: dict) -> str:
    """ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼ˆå…œåº•æ¨¡æ¿ï¼Œå½“ AI æœªç”Ÿæˆæœç´¢è¯æ—¶ä½¿ç”¨ï¼‰"""
    gen_query_dim_info = get_dimension_info_for_session(context) if context else DIMENSION_INFO
    dim_info = gen_query_dim_info.get(dimension, {})
    dim_name = dim_info.get("name", dimension)

    # æ„å»ºæœç´¢æŸ¥è¯¢ - å¯¹äºéé»˜è®¤ç»´åº¦ä½¿ç”¨é€šç”¨æ¨¡æ¿
    if dimension == "tech_constraints":
        return f"{topic} æŠ€æœ¯é€‰å‹ æœ€ä½³å®è·µ 2026"
    elif dimension == "customer_needs":
        return f"{topic} ç”¨æˆ·éœ€æ±‚ è¡Œä¸šç—›ç‚¹ 2026"
    elif dimension == "business_process":
        return f"{topic} ä¸šåŠ¡æµç¨‹ æœ€ä½³å®è·µ"
    elif dimension == "project_constraints":
        return f"{topic} é¡¹ç›®å®æ–½ æˆæœ¬é¢„ç®— å‘¨æœŸ"
    else:
        # éé»˜è®¤ç»´åº¦ï¼Œä½¿ç”¨ç»´åº¦åç§°æ„å»ºé€šç”¨æœç´¢è¯
        return f"{topic} {dim_name}"


# ============ Deep Vision AI æ ¸å¿ƒé€»è¾‘ ============

DIMENSION_INFO = {
    "customer_needs": {
        "name": "å®¢æˆ·éœ€æ±‚",
        "description": "æ ¸å¿ƒç—›ç‚¹ã€æœŸæœ›ä»·å€¼ã€ä½¿ç”¨åœºæ™¯ã€ç”¨æˆ·è§’è‰²",
        "key_aspects": ["æ ¸å¿ƒç—›ç‚¹", "æœŸæœ›ä»·å€¼", "ä½¿ç”¨åœºæ™¯", "ç”¨æˆ·è§’è‰²"]
    },
    "business_process": {
        "name": "ä¸šåŠ¡æµç¨‹",
        "description": "å…³é”®æµç¨‹èŠ‚ç‚¹ã€è§’è‰²åˆ†å·¥ã€è§¦å‘äº‹ä»¶ã€å¼‚å¸¸å¤„ç†",
        "key_aspects": ["å…³é”®æµç¨‹", "è§’è‰²åˆ†å·¥", "è§¦å‘äº‹ä»¶", "å¼‚å¸¸å¤„ç†"]
    },
    "tech_constraints": {
        "name": "æŠ€æœ¯çº¦æŸ",
        "description": "ç°æœ‰æŠ€æœ¯æ ˆã€é›†æˆæ¥å£è¦æ±‚ã€æ€§èƒ½æŒ‡æ ‡ã€å®‰å…¨åˆè§„",
        "key_aspects": ["éƒ¨ç½²æ–¹å¼", "ç³»ç»Ÿé›†æˆ", "æ€§èƒ½è¦æ±‚", "å®‰å…¨åˆè§„"]
    },
    "project_constraints": {
        "name": "é¡¹ç›®çº¦æŸ",
        "description": "é¢„ç®—èŒƒå›´ã€æ—¶é—´èŠ‚ç‚¹ã€èµ„æºé™åˆ¶ã€å…¶ä»–çº¦æŸ",
        "key_aspects": ["é¢„ç®—èŒƒå›´", "æ—¶é—´èŠ‚ç‚¹", "èµ„æºé™åˆ¶", "ä¼˜å…ˆçº§"]
    }
}


def get_dimension_info_for_session(session: dict) -> dict:
    """
    è·å–ä¼šè¯çš„ç»´åº¦ä¿¡æ¯ï¼ˆæ”¯æŒåŠ¨æ€åœºæ™¯ï¼‰

    ä¼˜å…ˆä»ä¼šè¯çš„ scenario_config ä¸­è·å–ï¼Œ
    å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤çš„ DIMENSION_INFOã€‚

    Args:
        session: ä¼šè¯æ•°æ®

    Returns:
        ç»´åº¦ä¿¡æ¯å­—å…¸ {dim_id: {name, description, key_aspects}}
    """
    scenario_config = session.get("scenario_config")

    if scenario_config and "dimensions" in scenario_config:
        return {
            dim["id"]: {
                "name": dim.get("name", dim["id"]),
                "description": dim.get("description", ""),
                "key_aspects": dim.get("key_aspects", []),
                "weight": dim.get("weight"),
                "scoring_criteria": dim.get("scoring_criteria")
            }
            for dim in scenario_config["dimensions"]
        }

    # å‘åå…¼å®¹ï¼šè¿”å›é»˜è®¤ç»´åº¦
    return DIMENSION_INFO


def get_dimension_order_for_session(session: dict) -> list:
    """
    è·å–ä¼šè¯çš„ç»´åº¦é¡ºåºåˆ—è¡¨

    Args:
        session: ä¼šè¯æ•°æ®

    Returns:
        ç»´åº¦ ID åˆ—è¡¨
    """
    scenario_config = session.get("scenario_config")

    if scenario_config and "dimensions" in scenario_config:
        return [dim["id"] for dim in scenario_config["dimensions"]]

    # å‘åå…¼å®¹ï¼šè¿”å›é»˜è®¤é¡ºåº
    return list(DIMENSION_INFO.keys())


# ============ æ»‘åŠ¨çª—å£ä¸Šä¸‹æ–‡ç®¡ç† ============

# é…ç½®å‚æ•°
CONTEXT_WINDOW_SIZE = 5  # ä¿ç•™æœ€è¿‘Næ¡å®Œæ•´é—®ç­”
SUMMARY_THRESHOLD = 8    # è¶…è¿‡æ­¤æ•°é‡æ—¶è§¦å‘æ‘˜è¦ç”Ÿæˆ
MAX_DOC_LENGTH = 2000    # å•ä¸ªæ–‡æ¡£æœ€å¤§é•¿åº¦ï¼ˆçº¦650æ±‰å­—ï¼Œå¢åŠ 33%ï¼‰
MAX_TOTAL_DOCS = 5000    # æ‰€æœ‰æ–‡æ¡£æ€»é•¿åº¦é™åˆ¶ï¼ˆçº¦1600æ±‰å­—ï¼Œå¢åŠ 67%ï¼‰
API_TIMEOUT = 90.0       # API è°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œä»60ç§’å¢åŠ åˆ°90ç§’

# ============ æ™ºèƒ½æ–‡æ¡£æ‘˜è¦é…ç½®ï¼ˆç¬¬ä¸‰é˜¶æ®µä¼˜åŒ–ï¼‰ ============
ENABLE_SMART_SUMMARY = True       # å¯ç”¨æ™ºèƒ½æ–‡æ¡£æ‘˜è¦ï¼ˆæ›¿ä»£ç®€å•æˆªæ–­ï¼‰
SMART_SUMMARY_THRESHOLD = 1500    # è§¦å‘æ™ºèƒ½æ‘˜è¦çš„æ–‡æ¡£é•¿åº¦é˜ˆå€¼ï¼ˆå­—ç¬¦ï¼‰
SMART_SUMMARY_TARGET = 800        # æ‘˜è¦ç›®æ ‡é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
SUMMARY_CACHE_ENABLED = True      # å¯ç”¨æ‘˜è¦ç¼“å­˜ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
MAX_TOKENS_SUMMARY = 500          # æ‘˜è¦ç”Ÿæˆçš„æœ€å¤§tokenæ•°


# ============ æ™ºèƒ½æ–‡æ¡£æ‘˜è¦å®ç° ============

def get_document_hash(content: str) -> str:
    """è®¡ç®—æ–‡æ¡£å†…å®¹çš„hashå€¼ï¼Œç”¨äºæ‘˜è¦ç¼“å­˜"""
    import hashlib
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]


def get_cached_summary(doc_hash: str) -> Optional[str]:
    """è·å–ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦"""
    if not SUMMARY_CACHE_ENABLED:
        return None

    cache_file = SUMMARIES_DIR / f"{doc_hash}.txt"
    if cache_file.exists():
        try:
            summary = cache_file.read_text(encoding='utf-8')
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦: {doc_hash}")
            return summary
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸  è¯»å–æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")
    return None


def save_summary_cache(doc_hash: str, summary: str) -> None:
    """ä¿å­˜æ–‡æ¡£æ‘˜è¦åˆ°ç¼“å­˜"""
    if not SUMMARY_CACHE_ENABLED:
        return

    cache_file = SUMMARIES_DIR / f"{doc_hash}.txt"
    try:
        cache_file.write_text(summary, encoding='utf-8')
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ’¾ æ‘˜è¦å·²ç¼“å­˜: {doc_hash}")
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  ä¿å­˜æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")


def summarize_document(content: str, doc_name: str = "æ–‡æ¡£", topic: str = "") -> tuple[str, bool]:
    """
    æ™ºèƒ½æ–‡æ¡£æ‘˜è¦ç”Ÿæˆï¼ˆç¬¬ä¸‰é˜¶æ®µä¼˜åŒ–æ ¸å¿ƒåŠŸèƒ½ï¼‰

    å½“æ–‡æ¡£è¿‡é•¿æ—¶ï¼Œä½¿ç”¨AIç”Ÿæˆä¿ç•™å…³é”®ä¿¡æ¯çš„æ‘˜è¦ï¼Œè€Œéç®€å•æˆªæ–­ã€‚

    Args:
        content: æ–‡æ¡£åŸå§‹å†…å®¹
        doc_name: æ–‡æ¡£åç§°ï¼ˆç”¨äºæç¤ºï¼‰
        topic: è®¿è°ˆä¸»é¢˜ï¼ˆç”¨äºç”Ÿæˆæ›´ç›¸å…³çš„æ‘˜è¦ï¼‰

    Returns:
        tuple[str, bool]: (å¤„ç†åçš„å†…å®¹, æ˜¯å¦ç”Ÿæˆäº†æ‘˜è¦)
    """
    original_length = len(content)

    # å¦‚æœæ–‡æ¡£é•¿åº¦æœªè¶…è¿‡é˜ˆå€¼ï¼Œç›´æ¥è¿”å›åŸæ–‡
    if original_length <= SMART_SUMMARY_THRESHOLD:
        return content, False

    # å¦‚æœæœªå¯ç”¨æ™ºèƒ½æ‘˜è¦æˆ–æ²¡æœ‰AIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç®€å•æˆªæ–­
    if not ENABLE_SMART_SUMMARY or not claude_client:
        truncated = content[:MAX_DOC_LENGTH]
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“„ æ–‡æ¡£ {doc_name} ä½¿ç”¨ç®€å•æˆªæ–­: {original_length} -> {MAX_DOC_LENGTH} å­—ç¬¦")
        return truncated, False

    # æ£€æŸ¥ç¼“å­˜
    doc_hash = get_document_hash(content)
    cached = get_cached_summary(doc_hash)
    if cached:
        return cached, True

    # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
    if ENABLE_DEBUG_LOG:
        print(f"ğŸ¤– ä¸ºæ–‡æ¡£ {doc_name} ç”Ÿæˆæ™ºèƒ½æ‘˜è¦: {original_length} -> ~{SMART_SUMMARY_TARGET} å­—ç¬¦")

    # æ„å»ºæ‘˜è¦ç”Ÿæˆprompt
    summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ‘˜è¦ã€‚

## è¦æ±‚
1. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨ {SMART_SUMMARY_TARGET} å­—ç¬¦ä»¥å†…
2. ä¿ç•™æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ã€æ ¸å¿ƒè§‚ç‚¹å’Œé‡è¦æ•°æ®
3. å¦‚æœæ–‡æ¡£ä¸"{topic}"ä¸»é¢˜ç›¸å…³ï¼Œä¼˜å…ˆä¿ç•™ä¸ä¸»é¢˜ç›¸å…³çš„å†…å®¹
4. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€ï¼Œé¿å…å†—ä½™
5. ä¿æŒä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹

## æ–‡æ¡£åç§°
{doc_name}

## æ–‡æ¡£å†…å®¹
{content[:8000]}

## è¾“å‡ºæ ¼å¼
ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ "æ‘˜è¦ï¼š"ç­‰å‰ç¼€ã€‚"""

    try:
        import time
        start_time = time.time()

        response = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=MAX_TOKENS_SUMMARY,
            timeout=60.0,  # æ‘˜è¦ç”Ÿæˆç”¨è¾ƒçŸ­è¶…æ—¶
            messages=[{"role": "user", "content": summary_prompt}]
        )

        response_time = time.time() - start_time
        summary = response.content[0].text.strip()

        # è®°å½•metrics
        metrics_collector.record_api_call(
            call_type="doc_summary",
            prompt_length=len(summary_prompt),
            response_time=response_time,
            success=True,
            timeout=False,
            max_tokens=MAX_TOKENS_SUMMARY
        )

        # ä¿å­˜åˆ°ç¼“å­˜
        save_summary_cache(doc_hash, summary)

        if ENABLE_DEBUG_LOG:
            print(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ: {original_length} -> {len(summary)} å­—ç¬¦ ({response_time:.1f}s)")

        return summary, True

    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æˆªæ–­: {e}")

        # è®°å½•å¤±è´¥çš„metrics
        metrics_collector.record_api_call(
            call_type="doc_summary",
            prompt_length=len(summary_prompt) if 'summary_prompt' in locals() else 0,
            response_time=0,
            success=False,
            timeout="timeout" in str(e).lower(),
            error_msg=str(e),
            max_tokens=MAX_TOKENS_SUMMARY
        )

        # å›é€€åˆ°ç®€å•æˆªæ–­
        return content[:MAX_DOC_LENGTH], False


def process_document_for_context(doc: dict, remaining_length: int, topic: str = "") -> tuple[str, str, int, bool]:
    """
    å¤„ç†æ–‡æ¡£ä»¥ç”¨äºä¸Šä¸‹æ–‡ï¼ˆç»Ÿä¸€çš„æ–‡æ¡£å¤„ç†å…¥å£ï¼‰

    Args:
        doc: æ–‡æ¡£å­—å…¸ï¼ŒåŒ…å« name å’Œ content
        remaining_length: å‰©ä½™å¯ç”¨é•¿åº¦
        topic: è®¿è°ˆä¸»é¢˜

    Returns:
        tuple[str, str, int, bool]: (æ–‡æ¡£å, å¤„ç†åçš„å†…å®¹, ä½¿ç”¨çš„é•¿åº¦, æ˜¯å¦è¢«æ‘˜è¦/æˆªæ–­)
    """
    doc_name = doc.get('name', 'æ–‡æ¡£')
    content = doc.get('content', '')

    if not content:
        return doc_name, '', 0, False

    original_length = len(content)
    max_allowed = min(MAX_DOC_LENGTH, remaining_length)

    # å¦‚æœæ–‡æ¡£å¾ˆçŸ­ï¼ˆä¸è¶…è¿‡æ‘˜è¦é˜ˆå€¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    if original_length <= SMART_SUMMARY_THRESHOLD:
        # ä½†å¦‚æœè¶…è¿‡max_allowedï¼Œä»éœ€æˆªæ–­
        if original_length > max_allowed:
            return doc_name, content[:max_allowed], max_allowed, True
        return doc_name, content, original_length, False

    # æ–‡æ¡£è¶…è¿‡æ‘˜è¦é˜ˆå€¼ï¼Œå°è¯•æ™ºèƒ½æ‘˜è¦
    if ENABLE_SMART_SUMMARY:
        processed_content, is_summarized = summarize_document(content, doc_name, topic)

        # å¦‚æœæ‘˜è¦åä»ç„¶è¿‡é•¿ï¼Œå†æˆªæ–­
        if len(processed_content) > max_allowed:
            processed_content = processed_content[:max_allowed]

        return doc_name, processed_content, len(processed_content), True

    # æœªå¯ç”¨æ™ºèƒ½æ‘˜è¦ï¼Œç®€å•æˆªæ–­
    truncated = content[:max_allowed]
    return doc_name, truncated, len(truncated), True


def generate_history_summary(session: dict, exclude_recent: int = 5) -> Optional[str]:
    """
    ç”Ÿæˆå†å²è®¿è°ˆè®°å½•çš„æ‘˜è¦

    Args:
        session: ä¼šè¯æ•°æ®
        exclude_recent: æ’é™¤æœ€è¿‘Næ¡è®°å½•ï¼ˆè¿™äº›ä¼šä¿ç•™å®Œæ•´å†…å®¹ï¼‰

    Returns:
        æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœæ— éœ€æ‘˜è¦åˆ™è¿”å› None
    """
    interview_log = session.get("interview_log", [])

    # å¦‚æœè®°å½•å¤ªå°‘ï¼Œä¸éœ€è¦æ‘˜è¦
    if len(interview_log) <= exclude_recent:
        return None

    # è·å–éœ€è¦æ‘˜è¦çš„å†å²è®°å½•
    history_logs = interview_log[:-exclude_recent] if exclude_recent > 0 else interview_log

    if not history_logs:
        return None

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ‘˜è¦
    cached_summary = session.get("context_summary", {})
    cached_count = cached_summary.get("log_count", 0)

    # å¦‚æœç¼“å­˜çš„æ‘˜è¦è¦†ç›–äº†ç›¸åŒæ•°é‡çš„è®°å½•ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
    if cached_count == len(history_logs) and cached_summary.get("text"):
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ï¼ˆè¦†ç›– {cached_count} æ¡è®°å½•ï¼‰")
        return cached_summary["text"]

    # éœ€è¦ç”Ÿæˆæ–°æ‘˜è¦
    if not claude_client:
        # æ—  AI æ—¶ä½¿ç”¨ç®€å•æ‘˜è¦
        return _generate_simple_summary(history_logs, session)

    # æ„å»ºæ‘˜è¦ç”Ÿæˆ prompt
    summary_prompt = _build_summary_prompt(session.get("topic", ""), history_logs, session)

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ—œï¸ æ­£åœ¨ç”Ÿæˆå†å²æ‘˜è¦ï¼ˆ{len(history_logs)} æ¡è®°å½•ï¼‰...")

        summary_text = call_claude(summary_prompt, max_tokens=300, call_type="summary")

        if summary_text:
            if ENABLE_DEBUG_LOG:
                print(f"âœ… å†å²æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary_text)} å­—ç¬¦")
            return summary_text
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")

    # å¤±è´¥æ—¶å›é€€åˆ°ç®€å•æ‘˜è¦
    return _generate_simple_summary(history_logs, session)


def _build_summary_prompt(topic: str, logs: list, session: dict = None) -> str:
    """æ„å»ºæ‘˜è¦ç”Ÿæˆçš„ prompt"""
    # è·å–ç»´åº¦ä¿¡æ¯
    summary_dim_info = get_dimension_info_for_session(session) if session else DIMENSION_INFO

    # æŒ‰ç»´åº¦æ•´ç†
    by_dim = {}
    for log in logs:
        dim = log.get("dimension", "other")
        if dim not in by_dim:
            by_dim[dim] = []
        by_dim[dim].append(log)

    logs_text = ""
    for dim, dim_logs in by_dim.items():
        dim_name = summary_dim_info.get(dim, {}).get("name", dim)
        logs_text += f"\nã€{dim_name}ã€‘\n"
        for log in dim_logs:
            logs_text += f"Q: {log['question'][:80]}\nA: {log['answer'][:100]}\n"

    return f"""è¯·å°†ä»¥ä¸‹è®¿è°ˆè®°å½•å‹ç¼©ä¸ºç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ç‚¹ã€‚

è®¿è°ˆä¸»é¢˜ï¼š{topic}

è®¿è°ˆè®°å½•ï¼š
{logs_text}

è¦æ±‚ï¼š
1. æŒ‰ç»´åº¦æ•´ç†å…³é”®ä¿¡æ¯
2. æ¯ä¸ªç»´åº¦ç”¨1-2å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹
3. ä¿ç•™å…·ä½“çš„æ•°æ®ã€æŒ‡æ ‡ã€é€‰æ‹©
4. æ€»é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜

æ‘˜è¦ï¼š"""


def _generate_simple_summary(logs: list, session: dict = None) -> str:
    """ç”Ÿæˆç®€å•æ‘˜è¦ï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    simple_sum_dim_info = get_dimension_info_for_session(session) if session else DIMENSION_INFO
    by_dim = {}
    for log in logs:
        dim = log.get("dimension", "other")
        dim_name = simple_sum_dim_info.get(dim, {}).get("name", dim)
        if dim_name not in by_dim:
            by_dim[dim_name] = []
        # åªä¿ç•™ç­”æ¡ˆçš„å…³é”®éƒ¨åˆ†
        answer = log.get("answer", "")[:50]
        by_dim[dim_name].append(answer)

    parts = []
    for dim_name, answers in by_dim.items():
        parts.append(f"ã€{dim_name}ã€‘: {'; '.join(answers[:3])}")

    return " | ".join(parts)


def update_context_summary(session: dict, session_file) -> None:
    """
    æ›´æ–°ä¼šè¯çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆåœ¨æäº¤å›ç­”åè°ƒç”¨ï¼‰

    åªæœ‰å½“å†å²è®°å½•è¶…è¿‡é˜ˆå€¼æ—¶æ‰ç”Ÿæˆæ‘˜è¦
    """
    interview_log = session.get("interview_log", [])

    # æœªè¶…è¿‡é˜ˆå€¼ï¼Œä¸éœ€è¦æ‘˜è¦
    if len(interview_log) < SUMMARY_THRESHOLD:
        return

    # è®¡ç®—éœ€è¦æ‘˜è¦çš„è®°å½•æ•°
    history_count = len(interview_log) - CONTEXT_WINDOW_SIZE
    if history_count <= 0:
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ‘˜è¦
    cached_summary = session.get("context_summary", {})
    if cached_summary.get("log_count", 0) >= history_count:
        return  # ç¼“å­˜ä»ç„¶æœ‰æ•ˆ

    # ç”Ÿæˆæ–°æ‘˜è¦
    history_logs = interview_log[:history_count]

    if claude_client:
        summary_prompt = _build_summary_prompt(session.get("topic", ""), history_logs, session)
        try:
            summary_text = call_claude(summary_prompt, max_tokens=300, call_type="summary")
            if summary_text:
                session["context_summary"] = {
                    "text": summary_text,
                    "log_count": history_count,
                    "updated_at": get_utc_now()
                }
                # ä¿å­˜æ›´æ–°
                session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")
                if ENABLE_DEBUG_LOG:
                    print(f"ğŸ“ å·²æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆè¦†ç›– {history_count} æ¡å†å²è®°å½•ï¼‰")
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦å¤±è´¥: {e}")
    else:
        # æ—  AI æ—¶ä½¿ç”¨ç®€å•æ‘˜è¦
        session["context_summary"] = {
            "text": _generate_simple_summary(history_logs, session),
            "log_count": history_count,
            "updated_at": get_utc_now()
        }
        session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")


# ============ æ™ºèƒ½è¿½é—®è¯„ä¼° ============

# ç»´åº¦è¿½é—®æ•æ„Ÿåº¦ï¼ˆè¶Šé«˜è¶Šå®¹æ˜“è§¦å‘è¿½é—®ï¼‰
DIMENSION_FOLLOW_UP_SENSITIVITY = {
    "customer_needs": 0.8,       # å®¢æˆ·éœ€æ±‚æœ€éœ€è¦æ·±æŒ–
    "business_process": 0.6,     # ä¸šåŠ¡æµç¨‹éœ€è¦ä¸€å®šæ·±åº¦
    "tech_constraints": 0.5,     # æŠ€æœ¯çº¦æŸç›¸å¯¹æ˜ç¡®
    "project_constraints": 0.4,  # é¡¹ç›®çº¦æŸé€šå¸¸è¾ƒç›´æ¥
}

# ============ è¿½é—®ä¼˜åŒ–ç³»ç»Ÿé…ç½® ============

# è®¿è°ˆæ¨¡å¼é…ç½®
INTERVIEW_MODES = {
    "quick": {
        "name": "å¿«é€Ÿæ¨¡å¼",
        "formal_questions_per_dim": 2,
        "follow_up_budget_per_dim": 2,
        "total_follow_up_budget": 8,
        "max_questions_per_formal": 1,  # æ¯ä¸ªæ­£å¼é—®é¢˜æœ€å¤šè¿½é—®æ¬¡æ•°
        "estimated_questions": "12-16"
    },
    "standard": {
        "name": "æ ‡å‡†æ¨¡å¼",
        "formal_questions_per_dim": 3,
        "follow_up_budget_per_dim": 4,
        "total_follow_up_budget": 16,
        "max_questions_per_formal": 2,
        "estimated_questions": "20-28"
    },
    "deep": {
        "name": "æ·±åº¦æ¨¡å¼",
        "formal_questions_per_dim": 4,
        "follow_up_budget_per_dim": 6,
        "total_follow_up_budget": 24,
        "max_questions_per_formal": 3,
        "estimated_questions": "28-40"
    }
}

# é»˜è®¤æ¨¡å¼
DEFAULT_INTERVIEW_MODE = "standard"

# ç–²åŠ³åº¦ä¿¡å·æƒé‡
FATIGUE_SIGNALS = {
    "consecutive_short": {
        "description": "è¿ç»­ 3 ä¸ªå›ç­”å°‘äº 30 å­—ç¬¦",
        "threshold": 3,
        "weight": 0.3
    },
    "option_only_streak": {
        "description": "è¿ç»­ 3 æ¬¡åªé€‰é€‰é¡¹ä¸è¡¥å……",
        "threshold": 3,
        "weight": 0.25
    },
    "same_dimension_too_long": {
        "description": "åŒä¸€ç»´åº¦å·²é—® 8+ é—®é¢˜",
        "threshold": 8,
        "weight": 0.25
    },
    "total_questions_high": {
        "description": "æ€»é—®é¢˜æ•°è¶…è¿‡ 25",
        "threshold": 25,
        "weight": 0.2
    }
}

# ä¿¡æ¯é¥±å’Œåº¦é˜ˆå€¼
SATURATION_THRESHOLDS = {
    "high": 0.8,       # é«˜é¥±å’Œåº¦ï¼Œåœæ­¢è¿½é—®
    "medium": 0.6,     # ä¸­ç­‰é¥±å’Œåº¦ï¼Œæœ€å¤šå†è¿½é—®1æ¬¡
    "low": 0.4         # ä½é¥±å’Œåº¦ï¼Œæ­£å¸¸è¿½é—®
}


def get_interview_mode_config(session: dict) -> dict:
    """è·å–ä¼šè¯çš„è®¿è°ˆæ¨¡å¼é…ç½®"""
    mode = session.get("interview_mode", DEFAULT_INTERVIEW_MODE)
    return INTERVIEW_MODES.get(mode, INTERVIEW_MODES[DEFAULT_INTERVIEW_MODE])


def calculate_dimension_coverage(session: dict, dimension: str) -> int:
    """è®¡ç®—ç»´åº¦è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼‰"""
    formal_count = len([log for log in session.get("interview_log", [])
                       if log.get("dimension") == dimension and not log.get("is_follow_up", False)])
    mode_config = get_interview_mode_config(session)
    required_questions = mode_config.get("formal_questions_per_dim", 3)
    if required_questions <= 0:
        return 100
    return min(100, int(formal_count / required_questions * 100))


def get_follow_up_budget_status(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—è¿½é—®é¢„ç®—ä½¿ç”¨æƒ…å†µ

    Returns:
        {
            "total_used": int,           # å·²ä½¿ç”¨çš„æ€»è¿½é—®æ¬¡æ•°
            "total_budget": int,         # æ€»é¢„ç®—
            "dimension_used": int,       # å½“å‰ç»´åº¦å·²ä½¿ç”¨è¿½é—®æ¬¡æ•°
            "dimension_budget": int,     # å½“å‰ç»´åº¦é¢„ç®—
            "current_question_used": int, # å½“å‰æ­£å¼é—®é¢˜å·²è¿½é—®æ¬¡æ•°
            "current_question_budget": int, # å½“å‰æ­£å¼é—®é¢˜è¿½é—®é¢„ç®—
            "can_follow_up": bool,       # æ˜¯å¦è¿˜èƒ½è¿½é—®
            "budget_exhausted_reason": str or None  # é¢„ç®—è€—å°½åŸå› 
        }
    """
    mode_config = get_interview_mode_config(session)
    interview_log = session.get("interview_log", [])

    # è®¡ç®—æ€»è¿½é—®æ¬¡æ•°
    total_follow_ups = len([log for log in interview_log if log.get("is_follow_up", False)])
    total_budget = mode_config["total_follow_up_budget"]

    # è®¡ç®—å½“å‰ç»´åº¦çš„è¿½é—®æ¬¡æ•°
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]
    dim_follow_ups = len([log for log in dim_logs if log.get("is_follow_up", False)])
    dim_budget = mode_config["follow_up_budget_per_dim"]

    # è®¡ç®—å½“å‰æ­£å¼é—®é¢˜çš„è¿½é—®æ¬¡æ•°
    # æ‰¾åˆ°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜çš„ç´¢å¼•
    formal_indices = [i for i, log in enumerate(dim_logs) if not log.get("is_follow_up", False)]
    if formal_indices:
        last_formal_idx = formal_indices[-1]
        # ç»Ÿè®¡è¿™ä¸ªæ­£å¼é—®é¢˜ä¹‹åçš„è¿½é—®æ•°
        current_question_follow_ups = len([
            log for log in dim_logs[last_formal_idx + 1:]
            if log.get("is_follow_up", False)
        ])
    else:
        current_question_follow_ups = 0
    current_question_budget = mode_config["max_questions_per_formal"]

    # åˆ¤æ–­æ˜¯å¦èƒ½ç»§ç»­è¿½é—®
    can_follow_up = True
    budget_exhausted_reason = None

    if total_follow_ups >= total_budget:
        can_follow_up = False
        budget_exhausted_reason = "total_budget_exhausted"
    elif dim_follow_ups >= dim_budget:
        can_follow_up = False
        budget_exhausted_reason = "dimension_budget_exhausted"
    elif current_question_follow_ups >= current_question_budget:
        can_follow_up = False
        budget_exhausted_reason = "question_budget_exhausted"

    return {
        "total_used": total_follow_ups,
        "total_budget": total_budget,
        "dimension_used": dim_follow_ups,
        "dimension_budget": dim_budget,
        "current_question_used": current_question_follow_ups,
        "current_question_budget": current_question_budget,
        "can_follow_up": can_follow_up,
        "budget_exhausted_reason": budget_exhausted_reason
    }


def calculate_dimension_saturation(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—ç»´åº¦çš„ä¿¡æ¯é¥±å’Œåº¦

    Returns:
        {
            "saturation_score": float,   # 0-1 é¥±å’Œåº¦åˆ†æ•°
            "coverage_score": float,     # å…³é”®æ–¹é¢è¦†ç›–åº¦
            "depth_score": float,        # ä¿¡æ¯æ·±åº¦
            "volume_score": float,       # ä¿¡æ¯é‡
            "covered_aspects": list,     # å·²è¦†ç›–çš„å…³é”®æ–¹é¢
            "level": str                 # "high", "medium", "low"
        }
    """
    interview_log = session.get("interview_log", [])
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]

    if not dim_logs:
        return {
            "saturation_score": 0,
            "coverage_score": 0,
            "depth_score": 0,
            "volume_score": 0,
            "covered_aspects": [],
            "level": "low"
        }

    session_dim_info = get_dimension_info_for_session(session)
    dim_info = session_dim_info.get(dimension, {})
    key_aspects = dim_info.get("key_aspects", [])

    # 1. ä¿¡æ¯è¦†ç›–åº¦ï¼šæ£€æŸ¥å…³é”®æ–¹é¢æ˜¯å¦è¢«æåŠ
    all_answers = " ".join([log.get("answer", "") for log in dim_logs])
    all_questions = " ".join([log.get("question", "") for log in dim_logs])
    combined_text = all_answers + all_questions

    covered_aspects = []
    for aspect in key_aspects:
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨é—®ç­”ä¸­
        if aspect in combined_text:
            covered_aspects.append(aspect)
        else:
            # æ£€æŸ¥ç›¸å…³è¯
            aspect_keywords = {
                "æ ¸å¿ƒç—›ç‚¹": ["ç—›ç‚¹", "é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "å›°æ‰°"],
                "æœŸæœ›ä»·å€¼": ["ä»·å€¼", "æ”¶ç›Š", "æ•ˆæœ", "ç›®æ ‡", "æœŸæœ›"],
                "ä½¿ç”¨åœºæ™¯": ["åœºæ™¯", "æƒ…å†µ", "ä½¿ç”¨", "åº”ç”¨", "ä½•æ—¶"],
                "ç”¨æˆ·è§’è‰²": ["ç”¨æˆ·", "è§’è‰²", "äººå‘˜", "è°", "ä½¿ç”¨è€…"],
                "å…³é”®æµç¨‹": ["æµç¨‹", "æ­¥éª¤", "ç¯èŠ‚", "è¿‡ç¨‹"],
                "è§’è‰²åˆ†å·¥": ["åˆ†å·¥", "èŒè´£", "è´Ÿè´£", "éƒ¨é—¨"],
                "è§¦å‘äº‹ä»¶": ["è§¦å‘", "å¼€å§‹", "å¯åŠ¨", "ä½•æ—¶"],
                "å¼‚å¸¸å¤„ç†": ["å¼‚å¸¸", "é”™è¯¯", "å¤±è´¥", "ä¾‹å¤–"],
                "éƒ¨ç½²æ–¹å¼": ["éƒ¨ç½²", "äº‘", "æœ¬åœ°", "æœåŠ¡å™¨"],
                "ç³»ç»Ÿé›†æˆ": ["é›†æˆ", "å¯¹æ¥", "æ¥å£", "ç³»ç»Ÿ"],
                "æ€§èƒ½è¦æ±‚": ["æ€§èƒ½", "å“åº”", "å¹¶å‘", "é€Ÿåº¦"],
                "å®‰å…¨åˆè§„": ["å®‰å…¨", "åˆè§„", "æƒé™", "åŠ å¯†"],
                "é¢„ç®—èŒƒå›´": ["é¢„ç®—", "è´¹ç”¨", "æˆæœ¬", "ä»·æ ¼"],
                "æ—¶é—´èŠ‚ç‚¹": ["æ—¶é—´", "æœŸé™", "å‘¨æœŸ", "ä½•æ—¶"],
                "èµ„æºé™åˆ¶": ["èµ„æº", "äººåŠ›", "å›¢é˜Ÿ", "é™åˆ¶"],
                "ä¼˜å…ˆçº§": ["ä¼˜å…ˆ", "é‡è¦", "ç´§æ€¥", "å…ˆå"]
            }
            for keyword in aspect_keywords.get(aspect, []):
                if keyword in combined_text:
                    covered_aspects.append(aspect)
                    break

    coverage_score = len(covered_aspects) / len(key_aspects) if key_aspects else 0

    # 2. ä¿¡æ¯æ·±åº¦ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡åŒ–ã€å…·ä½“åœºæ™¯ã€å¯¹æ¯”ç­‰æ·±åº¦ä¿¡å·
    depth_signals = 0
    # æ£€æŸ¥æ•°å­—ï¼ˆé‡åŒ–ä¿¡æ¯ï¼‰
    if any(c.isdigit() for c in all_answers):
        depth_signals += 1
    # æ£€æŸ¥å…·ä½“åœºæ™¯æè¿°ï¼ˆåŒ…å«"æ¯”å¦‚"ã€"ä¾‹å¦‚"ã€"å½“...æ—¶"ç­‰ï¼‰
    scenario_keywords = ["æ¯”å¦‚", "ä¾‹å¦‚", "å½“", "å¦‚æœ", "åœºæ™¯", "æƒ…å†µä¸‹"]
    if any(kw in all_answers for kw in scenario_keywords):
        depth_signals += 1
    # æ£€æŸ¥å¯¹æ¯”æˆ–é€‰æ‹©ï¼ˆ"è€Œä¸æ˜¯"ã€"ä¼˜å…ˆ"ã€"ç›¸æ¯”"ï¼‰
    comparison_keywords = ["è€Œä¸æ˜¯", "ä¼˜å…ˆ", "ç›¸æ¯”", "æ›´é‡è¦", "é¦–å…ˆ"]
    if any(kw in all_answers for kw in comparison_keywords):
        depth_signals += 1
    # æ£€æŸ¥åŸå› è¯´æ˜ï¼ˆ"å› ä¸º"ã€"ç”±äº"ã€"æ‰€ä»¥"ï¼‰
    reason_keywords = ["å› ä¸º", "ç”±äº", "æ‰€ä»¥", "åŸå› æ˜¯"]
    if any(kw in all_answers for kw in reason_keywords):
        depth_signals += 1
    # æ£€æŸ¥å¤šç‚¹å›ç­”
    if "ï¼›" in all_answers or "ã€" in all_answers:
        depth_signals += 1

    depth_score = min(1.0, depth_signals / 5)

    # 3. ä¿¡æ¯é‡ï¼šåŸºäºæ€»å­—ç¬¦æ•°
    total_chars = sum(len(log.get("answer", "")) for log in dim_logs)
    # æœŸæœ›æ¯ä¸ªç»´åº¦è‡³å°‘æ”¶é›† 300 å­—ç¬¦çš„æœ‰æ•ˆä¿¡æ¯
    volume_score = min(1.0, total_chars / 300)

    # ç»¼åˆé¥±å’Œåº¦
    saturation_score = coverage_score * 0.4 + depth_score * 0.3 + volume_score * 0.3

    # ç¡®å®šé¥±å’Œåº¦çº§åˆ«
    if saturation_score >= SATURATION_THRESHOLDS["high"]:
        level = "high"
    elif saturation_score >= SATURATION_THRESHOLDS["medium"]:
        level = "medium"
    else:
        level = "low"

    return {
        "saturation_score": round(saturation_score, 2),
        "coverage_score": round(coverage_score, 2),
        "depth_score": round(depth_score, 2),
        "volume_score": round(volume_score, 2),
        "covered_aspects": covered_aspects,
        "level": level
    }


def calculate_user_fatigue(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—ç”¨æˆ·ç–²åŠ³åº¦

    Returns:
        {
            "fatigue_score": float,      # 0-1 ç–²åŠ³åº¦åˆ†æ•°
            "detected_signals": list,    # æ£€æµ‹åˆ°çš„ç–²åŠ³ä¿¡å·
            "sensitivity_modifier": float, # è¿½é—®æ•æ„Ÿåº¦è°ƒæ•´ç³»æ•° (0.5-1.0)
            "should_force_progress": bool  # æ˜¯å¦åº”è¯¥å¼ºåˆ¶æ¨è¿›
        }
    """
    interview_log = session.get("interview_log", [])
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]

    detected_signals = []
    fatigue_score = 0

    # 1. æ£€æŸ¥è¿ç»­ç®€çŸ­å›ç­”
    recent_answers = [log.get("answer", "") for log in interview_log[-5:]]
    short_count = sum(1 for ans in recent_answers if len(ans.strip()) < 30)
    if short_count >= FATIGUE_SIGNALS["consecutive_short"]["threshold"]:
        detected_signals.append("consecutive_short")
        fatigue_score += FATIGUE_SIGNALS["consecutive_short"]["weight"]

    # 2. æ£€æŸ¥è¿ç»­åªé€‰é€‰é¡¹
    recent_logs = interview_log[-5:]
    option_only_count = 0
    for log in recent_logs:
        options = log.get("options", [])
        answer = log.get("answer", "")
        if options and answer in options and len(answer) < 40:
            option_only_count += 1
    if option_only_count >= FATIGUE_SIGNALS["option_only_streak"]["threshold"]:
        detected_signals.append("option_only_streak")
        fatigue_score += FATIGUE_SIGNALS["option_only_streak"]["weight"]

    # 3. æ£€æŸ¥åŒä¸€ç»´åº¦é—®é¢˜è¿‡å¤š
    if len(dim_logs) >= FATIGUE_SIGNALS["same_dimension_too_long"]["threshold"]:
        detected_signals.append("same_dimension_too_long")
        fatigue_score += FATIGUE_SIGNALS["same_dimension_too_long"]["weight"]

    # 4. æ£€æŸ¥æ€»é—®é¢˜æ•°
    if len(interview_log) >= FATIGUE_SIGNALS["total_questions_high"]["threshold"]:
        detected_signals.append("total_questions_high")
        fatigue_score += FATIGUE_SIGNALS["total_questions_high"]["weight"]

    fatigue_score = min(1.0, fatigue_score)

    # è®¡ç®—æ•æ„Ÿåº¦è°ƒæ•´ç³»æ•°ï¼ˆç–²åŠ³åº¦è¶Šé«˜ï¼Œè¿½é—®æ•æ„Ÿåº¦è¶Šä½ï¼‰
    # å½“ fatigue_score = 0 æ—¶ï¼Œmodifier = 1.0
    # å½“ fatigue_score = 1 æ—¶ï¼Œmodifier = 0.5
    sensitivity_modifier = 1.0 - (fatigue_score * 0.5)

    # åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶æ¨è¿›
    should_force_progress = fatigue_score >= 0.8

    return {
        "fatigue_score": round(fatigue_score, 2),
        "detected_signals": detected_signals,
        "sensitivity_modifier": round(sensitivity_modifier, 2),
        "should_force_progress": should_force_progress
    }


def should_follow_up_comprehensive(session: dict, dimension: str,
                                    rule_based_result: dict) -> dict:
    """
    ç»¼åˆå†³ç­–æ˜¯å¦åº”è¯¥è¿½é—®

    æ•´åˆï¼šè§„åˆ™è¯„ä¼° + é¢„ç®—æ£€æŸ¥ + é¥±å’Œåº¦ + ç–²åŠ³åº¦

    Returns:
        {
            "should_follow_up": bool,
            "reason": str,
            "budget_status": dict,
            "saturation": dict,
            "fatigue": dict,
            "decision_factors": list  # å½±å“å†³ç­–çš„å› ç´ 
        }
    """
    decision_factors = []

    # 1. æ£€æŸ¥é¢„ç®—
    budget_status = get_follow_up_budget_status(session, dimension)
    if not budget_status["can_follow_up"]:
        reason_map = {
            "total_budget_exhausted": "ä¼šè¯è¿½é—®é¢„ç®—å·²ç”¨å®Œ",
            "dimension_budget_exhausted": "å½“å‰ç»´åº¦è¿½é—®é¢„ç®—å·²ç”¨å®Œ",
            "question_budget_exhausted": "å½“å‰é—®é¢˜è¿½é—®æ¬¡æ•°å·²è¾¾ä¸Šé™"
        }
        return {
            "should_follow_up": False,
            "reason": reason_map.get(budget_status["budget_exhausted_reason"], "é¢„ç®—å·²ç”¨å®Œ"),
            "budget_status": budget_status,
            "saturation": {},
            "fatigue": {},
            "decision_factors": ["budget_exhausted"]
        }

    # 2. æ£€æŸ¥é¥±å’Œåº¦
    saturation = calculate_dimension_saturation(session, dimension)
    if saturation["level"] == "high":
        decision_factors.append("high_saturation")
        return {
            "should_follow_up": False,
            "reason": f"ä¿¡æ¯å·²å……åˆ†ï¼ˆé¥±å’Œåº¦ {saturation['saturation_score']:.0%}ï¼‰",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": None,
            "decision_factors": decision_factors
        }

    # 3. æ£€æŸ¥ç–²åŠ³åº¦
    fatigue = calculate_user_fatigue(session, dimension)
    if fatigue["should_force_progress"]:
        decision_factors.append("user_fatigue")
        return {
            "should_follow_up": False,
            "reason": "æ£€æµ‹åˆ°ç”¨æˆ·ç–²åŠ³ï¼Œæš‚åœè¿½é—®",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": fatigue,
            "decision_factors": decision_factors
        }

    # 4. åŸºäºè§„åˆ™è¯„ä¼°ç»“æœï¼Œä½†åº”ç”¨ç–²åŠ³åº¦è°ƒæ•´
    original_needs_follow_up = rule_based_result.get("needs_follow_up", False)

    if not original_needs_follow_up:
        return {
            "should_follow_up": False,
            "reason": "å›ç­”å·²å……åˆ†",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": fatigue,
            "decision_factors": ["sufficient_answer"]
        }

    # ä¸­ç­‰é¥±å’Œåº¦æ—¶é™åˆ¶è¿½é—®
    if saturation["level"] == "medium":
        # æ£€æŸ¥æ˜¯å¦å·²ç»è¿½é—®è¿‡
        if budget_status["current_question_used"] >= 1:
            decision_factors.append("medium_saturation_limit")
            return {
                "should_follow_up": False,
                "reason": "ä¿¡æ¯æ¥è¿‘å……åˆ†ï¼Œä¸å†è¿½é—®",
                "budget_status": budget_status,
                "saturation": saturation,
                "fatigue": fatigue,
                "decision_factors": decision_factors
            }

    # ç–²åŠ³åº¦è¾ƒé«˜æ—¶ï¼Œæé«˜è¿½é—®é—¨æ§›
    if fatigue["fatigue_score"] >= 0.5:
        decision_factors.append("elevated_threshold")
        # åªæœ‰éå¸¸æ˜æ˜¾éœ€è¦è¿½é—®çš„æƒ…å†µæ‰è¿½é—®
        if len(rule_based_result.get("signals", [])) < 2:
            return {
                "should_follow_up": False,
                "reason": "ç”¨æˆ·å¯èƒ½ç–²åŠ³ï¼Œè·³è¿‡éå…³é”®è¿½é—®",
                "budget_status": budget_status,
                "saturation": saturation,
                "fatigue": fatigue,
                "decision_factors": decision_factors
            }

    # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œå¯ä»¥è¿½é—®
    decision_factors.append("rule_based_follow_up")
    return {
        "should_follow_up": True,
        "reason": rule_based_result.get("reason", "éœ€è¦è¿›ä¸€æ­¥äº†è§£"),
        "budget_status": budget_status,
        "saturation": saturation,
        "fatigue": fatigue,
        "decision_factors": decision_factors
    }


def score_assessment_answer(session: dict, dimension: str, question: str, answer: str) -> Optional[float]:
    """
    ä¸ºè¯„ä¼°åœºæ™¯çš„å›ç­”æ‰“åˆ†ï¼ˆ1-5åˆ†ï¼‰

    Args:
        session: ä¼šè¯æ•°æ®
        dimension: ç»´åº¦ID
        question: é—®é¢˜
        answer: å›ç­”

    Returns:
        float: 1.0-5.0 çš„åˆ†æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if not claude_client:
        return None

    # è·å–ç»´åº¦é…ç½®
    scenario_config = session.get("scenario_config", {})
    dim_config = None
    for dim in scenario_config.get("dimensions", []):
        if dim.get("id") == dimension:
            dim_config = dim
            break

    if not dim_config:
        return None

    # æ„å»ºè¯„åˆ†æ ‡å‡†æ–‡æœ¬
    scoring_criteria = dim_config.get("scoring_criteria", {})
    criteria_text = "\n".join(
        f"  {score}åˆ†: {desc}"
        for score, desc in sorted(scoring_criteria.items(), key=lambda x: int(x[0]), reverse=True)
    )

    if not criteria_text:
        criteria_text = """  5åˆ†: å›ç­”éå¸¸ä¼˜ç§€ï¼Œå±•ç°æ·±åšä¸“ä¸šèƒ½åŠ›
  4åˆ†: å›ç­”è‰¯å¥½ï¼Œæœ‰æ¸…æ™°çš„æ€è·¯å’Œè§è§£
  3åˆ†: å›ç­”åŸºæœ¬åˆæ ¼ï¼Œä½†ç¼ºä¹æ·±åº¦
  2åˆ†: å›ç­”æœ‰æ˜æ˜¾ä¸è¶³æˆ–åå·®
  1åˆ†: å›ç­”å¾ˆå·®ï¼Œæ— æ³•å±•ç°ç›¸å…³èƒ½åŠ›"""

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šé¢è¯•å®˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ†æ ‡å‡†ï¼Œå¯¹å€™é€‰äººçš„å›ç­”è¿›è¡Œè¯„åˆ†ã€‚

ã€è¯„ä¼°ç»´åº¦ã€‘{dim_config.get("name", dimension)}
ã€ç»´åº¦è¯´æ˜ã€‘{dim_config.get("description", "")}

ã€è¯„åˆ†æ ‡å‡†ã€‘
{criteria_text}

ã€é¢è¯•é—®é¢˜ã€‘
{question}

ã€å€™é€‰äººå›ç­”ã€‘
{answer}

è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œåªè¿”å›ä¸€ä¸ªæ•°å­—ï¼ˆ1-5ä¹‹é—´çš„æ•´æ•°æˆ–å°æ•°ï¼Œå¦‚ 3.5ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼š"""

    try:
        response = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=10,
            timeout=15.0,
            messages=[{"role": "user", "content": prompt}]
        )
        raw = response.content[0].text.strip()
        # æå–æ•°å­—
        import re
        match = re.search(r'(\d+\.?\d*)', raw)
        if match:
            score = float(match.group(1))
            return max(1.0, min(5.0, score))  # é™åˆ¶åœ¨ 1-5 èŒƒå›´
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ è¯„åˆ†å¤±è´¥: {e}")

    return None


def evaluate_answer_depth(question: str, answer: str, dimension: str,
                          options: list = None, is_follow_up: bool = False) -> dict:
    """
    è¯„ä¼°å›ç­”æ·±åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿½é—®

    ä¸‰å±‚åˆ¤æ–­ï¼š
    1. æ˜ç¡®éœ€è¦è¿½é—®ï¼ˆå›ç­”å¤ªå¼±ï¼‰
    2. æ˜ç¡®ä¸éœ€è¦è¿½é—®ï¼ˆå›ç­”å·²å……åˆ†ï¼‰
    3. å»ºè®®AIè¯„ä¼°ï¼ˆäº¤ç»™AIåœ¨ç”Ÿæˆä¸‹ä¸€é¢˜æ—¶åˆ¤æ–­ï¼‰

    Returns:
        {
            "needs_follow_up": bool,       # è§„åˆ™å±‚åˆ¤æ–­ç»“æœ
            "suggest_ai_eval": bool,       # æ˜¯å¦å»ºè®®AIå†æ¬¡è¯„ä¼°
            "reason": str or None,         # è¿½é—®åŸå› 
            "signals": list                # æ£€æµ‹åˆ°çš„ä¿¡å·
        }
    """
    # è¿½é—®çš„å›ç­”ä¸å†è¿½é—®ï¼ˆé¿å…æ— é™è¿½é—®ï¼‰
    if is_follow_up:
        return {"needs_follow_up": False, "suggest_ai_eval": False,
                "reason": None, "signals": []}

    signals = []
    answer_stripped = answer.strip()
    answer_len = len(answer_stripped)
    sensitivity = DIMENSION_FOLLOW_UP_SENSITIVITY.get(dimension, 0.5)

    # ---- ç¬¬ä¸€å±‚ï¼šæ˜ç¡®éœ€è¦è¿½é—®çš„æƒ…å†µ ----

    # 1. å›ç­”è¿‡çŸ­ï¼ˆæ ¹æ®ç»´åº¦æ•æ„Ÿåº¦è°ƒæ•´é˜ˆå€¼ï¼‰
    short_threshold = int(20 + sensitivity * 20)  # å®¢æˆ·éœ€æ±‚36å­—ç¬¦ï¼Œé¡¹ç›®çº¦æŸ28å­—ç¬¦
    if answer_len < short_threshold:
        signals.append("too_short")

    # 2. æ¨¡ç³Šè¡¨è¾¾æ£€æµ‹ï¼ˆæ‰©å±•è¯åº“ï¼‰
    vague_indicators = [
        # ä¸ç¡®å®šç±»
        "çœ‹æƒ…å†µ", "ä¸ä¸€å®š", "å¯èƒ½", "æˆ–è®¸", "å¤§æ¦‚", "å·®ä¸å¤š", "åˆ°æ—¶å€™",
        "å†è¯´", "è¿˜æ²¡æƒ³å¥½", "ä¸ç¡®å®š", "çœ‹å…·ä½“", "æ ¹æ®æƒ…å†µ", "å¾…å®š",
        "ä»¥åå†è¯´", "æš‚æ—¶ä¸æ¸…æ¥š", "ç›®å‰è¿˜ä¸å¥½è¯´",
        # ç¬¼ç»Ÿç±»
        "éƒ½å¯ä»¥", "éƒ½è¡Œ", "éšä¾¿", "æ— æ‰€è°“", "å·®ä¸å¤š", "ä¸€èˆ¬",
        # å›é¿ç±»
        "ä¸å¤ªäº†è§£", "æ²¡æƒ³è¿‡", "ä¸çŸ¥é“", "è¯´ä¸å¥½", "å¾ˆéš¾è¯´",
    ]
    matched_vague = [v for v in vague_indicators if v in answer_stripped]
    if matched_vague:
        signals.append("vague_expression")

    # 3. å®Œå…¨åŒ¹é…æ³›æ³›å›ç­”
    generic_answers = [
        "å¥½çš„", "æ˜¯çš„", "å¯ä»¥", "æ²¡é—®é¢˜", "éœ€è¦", "åº”è¯¥è¦",
        "å¯¹", "å—¯", "è¡Œ", "åŒæ„", "æ²¡æœ‰", "ä¸éœ€è¦",
    ]
    if answer_stripped in generic_answers:
        signals.append("generic_answer")

    # 4. ä»…é€‰æ‹©äº†é¢„è®¾é€‰é¡¹æ²¡æœ‰è¡¥å……ï¼ˆç­”æ¡ˆç­‰äºæŸä¸ªé€‰é¡¹åŸæ–‡ï¼‰
    if options:
        is_exact_option = answer_stripped in options
        # å•é€‰ä¸”ç­”æ¡ˆå°±æ˜¯é€‰é¡¹åŸæ–‡ï¼Œç¼ºä¹è‡ªå·±çš„æ€è€ƒ
        if is_exact_option and answer_len < 40:
            signals.append("option_only")

    # 5. ç¼ºä¹é‡åŒ–ä¿¡æ¯ï¼ˆå¯¹æŸäº›ç»´åº¦é‡è¦ï¼‰
    has_numbers = any(c.isdigit() for c in answer_stripped)
    quantitative_dimensions = ["tech_constraints", "project_constraints"]
    if dimension in quantitative_dimensions and not has_numbers and answer_len < 60:
        signals.append("no_quantification")

    # 6. å¤šé€‰ä½†åªé€‰äº†ä¸€ä¸ªï¼ˆå¯èƒ½éœ€è¦è¡¥å……ï¼‰
    if options and "ï¼›" not in answer_stripped and len(options) >= 3:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šé€‰é¢˜ä½†åªé€‰äº†ä¸€ä¸ª
        selected_count = sum(1 for opt in options if opt in answer_stripped)
        if selected_count <= 1 and answer_len < 30:
            signals.append("single_selection")

    # ---- ç¬¬äºŒå±‚ï¼šåˆ¤æ–­æ˜¯å¦æ˜ç¡®ä¸éœ€è¦è¿½é—® ----

    # å›ç­”è¶³å¤Ÿè¯¦ç»†ï¼Œä¸éœ€è¦è¿½é—®
    sufficient_signals = []
    if answer_len > 80:
        sufficient_signals.append("detailed_answer")
    if "ï¼›" in answer_stripped and answer_len > 40:
        sufficient_signals.append("multi_point_answer")
    if has_numbers and answer_len > 30:
        sufficient_signals.append("quantified_answer")

    # ---- ç¬¬ä¸‰å±‚ï¼šç»¼åˆåˆ¤æ–­ ----

    # è®¡ç®—è¿½é—®å¾—åˆ†ï¼ˆä¿¡å·è¶Šå¤šè¶Šéœ€è¦è¿½é—®ï¼‰
    signal_weights = {
        "too_short": 0.4,
        "vague_expression": 0.5,
        "generic_answer": 0.8,
        "option_only": 0.3,
        "no_quantification": 0.2,
        "single_selection": 0.2,
    }
    follow_up_score = sum(signal_weights.get(s, 0.1) for s in signals)
    follow_up_score *= sensitivity  # åº”ç”¨ç»´åº¦æ•æ„Ÿåº¦

    # å‡å»å……åˆ†åº¦ä¿¡å·
    sufficient_weights = {
        "detailed_answer": 0.5,
        "multi_point_answer": 0.3,
        "quantified_answer": 0.2,
    }
    sufficient_score = sum(sufficient_weights.get(s, 0) for s in sufficient_signals)
    follow_up_score -= sufficient_score

    # åˆ¤æ–­ç»“æœ
    if follow_up_score >= 0.4:
        # æ˜ç¡®éœ€è¦è¿½é—®
        reason = _build_follow_up_reason(signals)
        return {"needs_follow_up": True, "suggest_ai_eval": False,
                "reason": reason, "signals": signals}
    elif follow_up_score >= 0.15 and not sufficient_signals:
        # è¾¹ç•Œæƒ…å†µï¼Œå»ºè®®è®©AIè¯„ä¼°
        reason = _build_follow_up_reason(signals)
        return {"needs_follow_up": False, "suggest_ai_eval": True,
                "reason": reason, "signals": signals}
    else:
        # ä¸éœ€è¦è¿½é—®
        return {"needs_follow_up": False, "suggest_ai_eval": False,
                "reason": None, "signals": signals}


def _build_follow_up_reason(signals: list) -> str:
    """æ ¹æ®æ£€æµ‹åˆ°çš„ä¿¡å·æ„å»ºè¿½é—®åŸå› """
    reason_map = {
        "too_short": "å›ç­”è¿‡äºç®€çŸ­ï¼Œéœ€è¦è¡¥å……å…·ä½“ç»†èŠ‚",
        "vague_expression": "å›ç­”åŒ…å«æ¨¡ç³Šè¡¨è¿°ï¼Œéœ€è¦æ˜ç¡®å…·ä½“è¦æ±‚",
        "generic_answer": "å›ç­”è¿‡äºç¬¼ç»Ÿï¼Œéœ€è¦æ·±å…¥äº†è§£å…·ä½“éœ€æ±‚",
        "option_only": "ä»…é€‰æ‹©äº†é¢„è®¾é€‰é¡¹ï¼Œéœ€è¦äº†è§£å…·ä½“åœºæ™¯å’Œè€ƒé‡",
        "no_quantification": "ç¼ºå°‘é‡åŒ–æŒ‡æ ‡ï¼Œéœ€è¦æ˜ç¡®å…·ä½“æ•°æ®è¦æ±‚",
        "single_selection": "åªé€‰æ‹©äº†å•ä¸€é€‰é¡¹ï¼Œéœ€è¦äº†è§£æ˜¯å¦è¿˜æœ‰å…¶ä»–éœ€æ±‚",
    }
    reasons = [reason_map.get(s, "") for s in signals if s in reason_map]
    return reasons[0] if reasons else "éœ€è¦è¿›ä¸€æ­¥äº†è§£è¯¦ç»†éœ€æ±‚"


def build_interview_prompt(session: dict, dimension: str, all_dim_logs: list,
                           session_id: str = None) -> tuple[str, list]:
    """æ„å»ºè®¿è°ˆ promptï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£ + æ‘˜è¦å‹ç¼© + æ™ºèƒ½è¿½é—®ï¼‰

    Args:
        session: ä¼šè¯æ•°æ®
        dimension: å½“å‰ç»´åº¦
        all_dim_logs: å½“å‰ç»´åº¦çš„æ‰€æœ‰è®¿è°ˆè®°å½•
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºæ›´æ–°æ€è€ƒè¿›åº¦çŠ¶æ€ï¼‰

    Returns:
        tuple[str, list]: (promptå­—ç¬¦ä¸², è¢«æˆªæ–­çš„æ–‡æ¡£åˆ—è¡¨)
    """
    topic = session.get("topic", "æœªçŸ¥é¡¹ç›®")
    description = session.get("description")
    # å…¼å®¹æ—§æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ reference_materialsï¼Œå¦åˆ™åˆå¹¶æ—§å­—æ®µ
    reference_materials = session.get("reference_materials", [])
    if not reference_materials:
        reference_materials = session.get("reference_docs", []) + session.get("research_docs", [])
    interview_log = session.get("interview_log", [])
    session_dim_info = get_dimension_info_for_session(session)
    dim_info = session_dim_info.get(dimension, {})

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = [f"å½“å‰è®¿è°ˆä¸»é¢˜ï¼š{topic}"]

    # å¦‚æœæœ‰ä¸»é¢˜æè¿°ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
    if description:
        context_parts.append(f"\nä¸»é¢˜æè¿°ï¼š{description[:500]}")

    # æ·»åŠ å‚è€ƒèµ„æ–™å†…å®¹ï¼ˆä½¿ç”¨æ€»é•¿åº¦é™åˆ¶ + æ™ºèƒ½æ‘˜è¦ï¼‰
    total_doc_length = 0
    truncated_docs = []  # è®°å½•è¢«å¤„ç†çš„æ–‡æ¡£ï¼ˆæ‘˜è¦æˆ–æˆªæ–­ï¼‰
    summarized_docs = []  # è®°å½•ä½¿ç”¨æ™ºèƒ½æ‘˜è¦çš„æ–‡æ¡£
    if reference_materials:
        context_parts.append("\n## å‚è€ƒèµ„æ–™ï¼š")
        for doc in reference_materials:
            if doc.get("content") and total_doc_length < MAX_TOTAL_DOCS:
                remaining = MAX_TOTAL_DOCS - total_doc_length
                original_length = len(doc["content"])

                # ä½¿ç”¨æ™ºèƒ½æ‘˜è¦å¤„ç†æ–‡æ¡£
                doc_name, processed_content, used_length, was_processed = process_document_for_context(
                    doc, remaining, topic
                )

                if processed_content:
                    # æ ¹æ® source æ·»åŠ æ ‡è®°
                    source_marker = "ğŸ”„ " if doc.get("source") == "auto" else ""
                    context_parts.append(f"### {source_marker}{doc_name}")
                    context_parts.append(processed_content)
                    total_doc_length += used_length

                    # è®°å½•å¤„ç†æƒ…å†µ
                    if was_processed:
                        if used_length < original_length * 0.6:  # å¦‚æœå†…å®¹å‡å°‘è¶…è¿‡40%ï¼Œå¯èƒ½æ˜¯æ‘˜è¦
                            summarized_docs.append(f"{doc_name}ï¼ˆåŸ{original_length}å­—ç¬¦ï¼Œæ‘˜è¦è‡³{used_length}å­—ç¬¦ï¼‰")
                        else:
                            truncated_docs.append(f"{doc_name}ï¼ˆåŸ{original_length}å­—ç¬¦ï¼Œæˆªå–{used_length}å­—ç¬¦ï¼‰")

    # æ·»åŠ å¤„ç†æç¤ºï¼ˆè®© AI çŸ¥é“æ–‡æ¡£ä¿¡æ¯ç»è¿‡å¤„ç†ï¼‰
    if summarized_docs:
        context_parts.append(f"\nğŸ“ æ³¨æ„ï¼šä»¥ä¸‹æ–‡æ¡£å·²é€šè¿‡AIç”Ÿæˆæ‘˜è¦ä»¥ä¿ç•™å…³é”®ä¿¡æ¯ï¼š{', '.join(summarized_docs)}")
    if truncated_docs:
        context_parts.append(f"\nâš ï¸ æ³¨æ„ï¼šä»¥ä¸‹æ–‡æ¡£å› é•¿åº¦é™åˆ¶å·²è¢«æˆªæ–­ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œæé—®ï¼š{', '.join(truncated_docs)}")

    # ========== æ™ºèƒ½è”ç½‘æœç´¢ï¼ˆè§„åˆ™é¢„åˆ¤ + AIå†³ç­–ï¼‰ ==========
    # è·å–æœ€è¿‘çš„é—®ç­”è®°å½•ç”¨äº AI åˆ¤æ–­
    recent_qa = interview_log[-3:] if interview_log else []
    will_search, search_query, search_reason = smart_search_decision(topic, dimension, session, recent_qa)

    if will_search and search_query:
        # æ›´æ–°æ€è€ƒçŠ¶æ€åˆ°"æœç´¢"é˜¶æ®µ
        if session_id:
            update_thinking_status(session_id, "searching", has_search=True)

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” æ‰§è¡Œæœç´¢: {search_query} (åŸå› : {search_reason})")

        search_results = web_search(search_query)

        if search_results:
            context_parts.append("\n## è¡Œä¸šçŸ¥è¯†å‚è€ƒï¼ˆè”ç½‘æœç´¢ï¼‰ï¼š")
            for idx, result in enumerate(search_results[:2], 1):
                if result["type"] == "intent":
                    context_parts.append(f"**{result['content'][:150]}**")
                else:
                    context_parts.append(f"{idx}. **{result.get('title', 'å‚è€ƒä¿¡æ¯')[:40]}**")
                    context_parts.append(f"   {result['content'][:150]}")

    # ========== æ»‘åŠ¨çª—å£ + æ‘˜è¦å‹ç¼© ==========
    if interview_log:
        context_parts.append("\n## å·²æ”¶é›†çš„ä¿¡æ¯ï¼š")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨æ‘˜è¦
        if len(interview_log) > CONTEXT_WINDOW_SIZE:
            # è·å–æˆ–ç”Ÿæˆå†å²æ‘˜è¦
            history_summary = generate_history_summary(session, exclude_recent=CONTEXT_WINDOW_SIZE)
            if history_summary:
                context_parts.append(f"\n### å†å²è®¿è°ˆæ‘˜è¦ï¼ˆå…±{len(interview_log) - CONTEXT_WINDOW_SIZE}æ¡ï¼‰ï¼š")
                context_parts.append(history_summary)
                context_parts.append("\n### æœ€è¿‘é—®ç­”è®°å½•ï¼š")

            # åªä¿ç•™æœ€è¿‘çš„å®Œæ•´è®°å½•
            recent_logs = interview_log[-CONTEXT_WINDOW_SIZE:]
        else:
            recent_logs = interview_log

        # æ·»åŠ å®Œæ•´çš„æœ€è¿‘é—®ç­”è®°å½•
        for log in recent_logs:
            follow_up_mark = " [è¿½é—®]" if log.get("is_follow_up") else ""
            context_parts.append(f"- Q: {log['question']}{follow_up_mark}")
            context_parts.append(f"  A: {log['answer']}")
            dim_name = session_dim_info.get(log.get("dimension", ""), {}).get("name", "")
            if dim_name:
                context_parts.append(f"  (ç»´åº¦: {dim_name})")

    # è®¡ç®—æ­£å¼é—®é¢˜æ•°é‡ï¼ˆæ’é™¤è¿½é—®ï¼‰
    formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])

    # ========== æ™ºèƒ½è¿½é—®åˆ¤æ–­ï¼ˆç»¼åˆé¢„ç®—+é¥±å’Œåº¦+ç–²åŠ³åº¦+è§„åˆ™è¯„ä¼°ï¼‰ ==========
    last_log = None
    should_follow_up = False
    suggest_ai_eval = False
    follow_up_reason = ""
    eval_signals = []
    comprehensive_decision = None

    if all_dim_logs:
        last_log = all_dim_logs[-1]
        last_answer = last_log.get("answer", "")
        last_question = last_log.get("question", "")
        last_options = last_log.get("options", [])
        last_is_follow_up = last_log.get("is_follow_up", False)

        # ä½¿ç”¨å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°ï¼ˆè§„åˆ™å±‚ï¼‰
        eval_result = evaluate_answer_depth(
            question=last_question,
            answer=last_answer,
            dimension=dimension,
            options=last_options,
            is_follow_up=last_is_follow_up
        )

        eval_signals = eval_result["signals"]

        # ä½¿ç”¨ç»¼åˆå†³ç­–å‡½æ•°ï¼ˆæ•´åˆé¢„ç®—ã€é¥±å’Œåº¦ã€ç–²åŠ³åº¦ï¼‰
        comprehensive_decision = should_follow_up_comprehensive(
            session=session,
            dimension=dimension,
            rule_based_result=eval_result
        )

        should_follow_up = comprehensive_decision["should_follow_up"]
        follow_up_reason = comprehensive_decision["reason"] or ""

        # åªæœ‰åœ¨è§„åˆ™å±‚å»ºè®® AI è¯„ä¼°ä¸”ç»¼åˆå†³ç­–å…è®¸è¿½é—®æ—¶ï¼Œæ‰å»ºè®® AI è¯„ä¼°
        suggest_ai_eval = eval_result["suggest_ai_eval"] and comprehensive_decision["should_follow_up"]

        if ENABLE_DEBUG_LOG:
            budget = comprehensive_decision.get("budget_status", {})
            saturation = comprehensive_decision.get("saturation", {})
            fatigue = comprehensive_decision.get("fatigue", {})
            print(f"ğŸ” è¿½é—®å†³ç­–: should_follow_up={should_follow_up}, reason={follow_up_reason}")
            print(f"   é¢„ç®—: {budget.get('total_used', 0)}/{budget.get('total_budget', 0)} (ç»´åº¦: {budget.get('dimension_used', 0)}/{budget.get('dimension_budget', 0)})")
            if saturation:
                print(f"   é¥±å’Œåº¦: {saturation.get('saturation_score', 0):.0%} ({saturation.get('level', 'unknown')})")
            if fatigue:
                print(f"   ç–²åŠ³åº¦: {fatigue.get('fatigue_score', 0):.0%}, ä¿¡å·: {fatigue.get('detected_signals', [])}")

    # æ„å»º AI è¯„ä¼°æç¤ºï¼ˆå½“è§„åˆ™æœªæ˜ç¡®è§¦å‘ä½†å»ºè®®AIåˆ¤æ–­æ—¶ï¼‰
    ai_eval_guidance = ""
    if suggest_ai_eval and last_log:
        ai_eval_guidance = f"""
## å›ç­”æ·±åº¦è¯„ä¼°

è¯·å…ˆè¯„ä¼°ç”¨æˆ·çš„ä¸Šä¸€ä¸ªå›ç­”æ˜¯å¦éœ€è¦è¿½é—®ï¼š

**ä¸Šä¸€ä¸ªé—®é¢˜**: {last_log.get('question', '')[:100]}
**ç”¨æˆ·å›ç­”**: {last_log.get('answer', '')}
**æ£€æµ‹ä¿¡å·**: {', '.join(eval_signals) if eval_signals else 'æ— æ˜æ˜¾é—®é¢˜'}

åˆ¤æ–­æ ‡å‡†ï¼ˆæ»¡è¶³ä»»ä¸€æ¡å³åº”è¿½é—®ï¼‰ï¼š
1. å›ç­”åªæ˜¯é€‰æ‹©äº†é€‰é¡¹ï¼Œæ²¡æœ‰è¯´æ˜å…·ä½“åœºæ™¯æˆ–åŸå› 
2. ç¼ºå°‘é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚æ—¶é—´ã€æ•°é‡ã€é¢‘ç‡ç­‰ï¼‰
3. å›ç­”æ¯”è¾ƒç¬¼ç»Ÿï¼Œæ²¡æœ‰é’ˆå¯¹æ€§ç»†èŠ‚
4. å¯èƒ½éšè—äº†æ›´æ·±å±‚çš„éœ€æ±‚æˆ–é¡¾è™‘

å¦‚æœåˆ¤æ–­éœ€è¦è¿½é—®ï¼Œè¯·ï¼š
- è®¾ç½® is_follow_up: true
- é’ˆå¯¹ä¸Šä¸€ä¸ªå›ç­”è¿›è¡Œæ·±å…¥æé—®
- é—®é¢˜è¦æ›´å…·ä½“ï¼Œå¼•å¯¼ç”¨æˆ·ç»™å‡ºæ˜ç¡®ç­”æ¡ˆ

å¦‚æœåˆ¤æ–­ä¸éœ€è¦è¿½é—®ï¼Œè¯·ç”Ÿæˆæ–°é—®é¢˜ç»§ç»­è®¿è°ˆã€‚
"""

    # æ„å»ºè¿½é—®æ¨¡å¼çš„æç¤º
    follow_up_section = ""
    if should_follow_up:
        follow_up_section = f"""## è¿½é—®æ¨¡å¼ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰

ä¸Šä¸€ä¸ªç”¨æˆ·å›ç­”éœ€è¦è¿½é—®ã€‚åŸå› ï¼š{follow_up_reason}

**ä¸Šä¸€ä¸ªé—®é¢˜**: {last_log.get('question', '')[:100] if last_log else ''}
**ç”¨æˆ·å›ç­”**: {last_log.get('answer', '') if last_log else ''}

è¿½é—®è¦æ±‚ï¼š
1. å¿…é¡»è®¾ç½® is_follow_up: true
2. é’ˆå¯¹ä¸Šä¸€ä¸ªå›ç­”è¿›è¡Œæ·±å…¥æé—®ï¼Œä¸è¦è·³åˆ°æ–°è¯é¢˜
3. è¿½é—®é—®é¢˜è¦æ›´å…·ä½“ã€æ›´æœ‰é’ˆå¯¹æ€§
4. å¼•å¯¼ç”¨æˆ·ç»™å‡ºå…·ä½“çš„åœºæ™¯ã€æ•°æ®ã€æˆ–æ˜ç¡®çš„é€‰æ‹©
5. å¯ä»¥ä½¿ç”¨"æ‚¨æåˆ°çš„XXXï¼Œèƒ½å¦å…·ä½“è¯´æ˜..."è¿™æ ·çš„å¥å¼
"""
    else:
        follow_up_section = """## é—®é¢˜ç”Ÿæˆè¦æ±‚

1. ç”Ÿæˆ 1 ä¸ªé’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œç”¨äºæ”¶é›†è¯¥ç»´åº¦çš„å…³é”®ä¿¡æ¯
2. ä¸ºè¿™ä¸ªé—®é¢˜æä¾› 3-4 ä¸ªå…·ä½“çš„é€‰é¡¹
3. é€‰é¡¹è¦åŸºäºï¼š
   - è®¿è°ˆä¸»é¢˜çš„è¡Œä¸šç‰¹ç‚¹
   - å‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
   - è”ç½‘æœç´¢çš„è¡Œä¸šçŸ¥è¯†ï¼ˆå¦‚æœ‰ï¼‰
   - å·²æ”¶é›†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
4. æ ¹æ®é—®é¢˜æ€§è´¨åˆ¤æ–­æ˜¯å•é€‰è¿˜æ˜¯å¤šé€‰ï¼š
   - å•é€‰åœºæ™¯ï¼šäº’æ–¥é€‰é¡¹ï¼ˆæ˜¯/å¦ï¼‰ã€ä¼˜å…ˆçº§é€‰æ‹©ã€å”¯ä¸€é€‰æ‹©
   - å¤šé€‰åœºæ™¯ï¼šå¯å¹¶å­˜çš„åŠŸèƒ½éœ€æ±‚ã€å¤šä¸ªç—›ç‚¹ã€å¤šç§ç”¨æˆ·è§’è‰²
5. å¦‚æœç”¨æˆ·çš„å›ç­”ä¸å‚è€ƒæ–‡æ¡£å†…å®¹æœ‰å†²çªï¼Œè¦åœ¨é—®é¢˜ä¸­æŒ‡å‡ºå¹¶è¯·æ±‚æ¾„æ¸…
"""

    prompt = f"""**ä¸¥æ ¼è¾“å‡ºè¦æ±‚ï¼šä½ çš„å›å¤å¿…é¡»æ˜¯çº¯ JSON å¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€markdown ä»£ç å—æˆ–å…¶ä»–æ–‡æœ¬ã€‚ç¬¬ä¸€ä¸ªå­—ç¬¦å¿…é¡»æ˜¯ {{ï¼Œæœ€åä¸€ä¸ªå­—ç¬¦å¿…é¡»æ˜¯ }}**

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¿è°ˆå¸ˆï¼Œæ­£åœ¨è¿›è¡Œ"{topic}"çš„è®¿è°ˆã€‚
ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯**æ·±åº¦æŒ–æ˜ç”¨æˆ·çš„çœŸå®éœ€æ±‚**ï¼Œä¸æ»¡è¶³äºè¡¨é¢å›ç­”ã€‚

{chr(10).join(context_parts)}

## å½“å‰ä»»åŠ¡

ä½ ç°åœ¨éœ€è¦é’ˆå¯¹ã€Œ{dim_info.get('name', dimension)}ã€ç»´åº¦æ”¶é›†ä¿¡æ¯ã€‚
è¿™ä¸ªç»´åº¦å…³æ³¨ï¼š{dim_info.get('description', '')}

è¯¥ç»´åº¦å·²æ”¶é›†äº† {formal_questions_count} ä¸ªæ­£å¼é—®é¢˜çš„å›ç­”ï¼Œå…³é”®æ–¹é¢åŒ…æ‹¬ï¼š{', '.join(dim_info.get('key_aspects', []))}
{ai_eval_guidance}
{follow_up_section}

## è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

{{
    "question": "ä½ çš„é—®é¢˜",
    "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"],
    "multi_select": false,
    "is_follow_up": {'true' if should_follow_up else 'false'},
    "follow_up_reason": {json.dumps(follow_up_reason, ensure_ascii=False) if should_follow_up else 'null'},
    "conflict_detected": false,
    "conflict_description": null
}}

å­—æ®µè¯´æ˜ï¼š
- question: å­—ç¬¦ä¸²ï¼Œä½ è¦é—®çš„é—®é¢˜
- options: å­—ç¬¦ä¸²æ•°ç»„ï¼Œ3-4 ä¸ªé€‰é¡¹
- multi_select: å¸ƒå°”å€¼ï¼Œtrue=å¯å¤šé€‰ï¼Œfalse=å•é€‰
- is_follow_up: å¸ƒå°”å€¼ï¼Œtrue=è¿½é—®ï¼ˆé’ˆå¯¹ä¸Šä¸€å›ç­”æ·±å…¥ï¼‰ï¼Œfalse=æ–°é—®é¢˜
- follow_up_reason: å­—ç¬¦ä¸²æˆ– nullï¼Œè¿½é—®æ—¶è¯´æ˜åŸå› 
- conflict_detected: å¸ƒå°”å€¼
- conflict_description: å­—ç¬¦ä¸²æˆ– null

**å…³é”®æé†’ï¼š**
- ä¸è¦ä½¿ç”¨ ```json ä»£ç å—æ ‡è®°
- ä¸è¦åœ¨ JSON å‰åæ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—
- ç¡®ä¿ JSON è¯­æ³•å®Œå…¨æ­£ç¡®ï¼ˆæ‰€æœ‰å­—ç¬¦ä¸²ç”¨åŒå¼•å·ï¼Œå¸ƒå°”å€¼ç”¨ true/falseï¼Œç©ºå€¼ç”¨ nullï¼‰
- ä½ çš„æ•´ä¸ªå›å¤å°±æ˜¯è¿™ä¸ª JSON å¯¹è±¡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹
- **é‡è¦**ï¼šis_follow_up çš„å€¼å·²ç”±ç³»ç»Ÿæ ¹æ®é¢„ç®—å’Œé¥±å’Œåº¦é¢„å…ˆå†³å®šï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿è®¾ç½®"""

    return prompt, truncated_docs


def build_assessment_report_prompt(session: dict) -> str:
    """æ„å»ºé¢è¯•è¯„ä¼°æŠ¥å‘Š prompt"""
    topic = session.get("topic", "å€™é€‰äººè¯„ä¼°")
    description = session.get("description", "")
    interview_log = session.get("interview_log", [])
    dimensions = session.get("dimensions", {})
    scenario_config = session.get("scenario_config", {})
    assessment_config = scenario_config.get("assessment", {})

    # è·å–ç»´åº¦é…ç½®
    dim_configs = {d["id"]: d for d in scenario_config.get("dimensions", [])}

    # è®¡ç®—ç»¼åˆè¯„åˆ†
    total_score = 0.0
    total_weight = 0.0
    dim_scores_info = []
    for dim_id, dim_data in dimensions.items():
        dim_config = dim_configs.get(dim_id, {})
        weight = dim_config.get("weight", 0.25)
        score = dim_data.get("score")
        if score is not None:
            total_score += score * weight
            total_weight += weight
            dim_scores_info.append({
                "id": dim_id,
                "name": dim_config.get("name", dim_id),
                "score": score,
                "weight": weight,
                "criteria": dim_config.get("scoring_criteria", {})
            })

    final_score = round(total_score / total_weight, 2) if total_weight > 0 else 0

    # ç¡®å®šæ¨èç­‰çº§
    recommendation_levels = assessment_config.get("recommendation_levels", [])
    recommendation = {"level": "D", "name": "ä¸æ¨è", "color": "#ef4444"}
    for level in sorted(recommendation_levels, key=lambda x: x.get("threshold", 0), reverse=True):
        if final_score >= level.get("threshold", 0):
            recommendation = level
            break

    # æ„å»ºè¯„åˆ†è¡¨æ ¼æ–‡æœ¬
    score_table = "| ç»´åº¦ | å¾—åˆ† | æƒé‡ | åŠ æƒå¾—åˆ† |\n|:---|:---:|:---:|:---:|\n"
    for info in dim_scores_info:
        weighted = round(info["score"] * info["weight"], 2)
        score_table += f"| {info['name']} | {info['score']:.1f} | {info['weight']*100:.0f}% | {weighted:.2f} |\n"
    score_table += f"| **ç»¼åˆå¾—åˆ†** | **{final_score:.2f}** | 100% | **{final_score:.2f}** |"

    # æŒ‰ç»´åº¦æ•´ç†é—®ç­”å’Œè¯„åˆ†
    qa_sections = ""
    for dim_info in dim_scores_info:
        dim_id = dim_info["id"]
        qa_list = [log for log in interview_log if log.get("dimension") == dim_id]
        qa_sections += f"\n### {dim_info['name']}ï¼ˆå¾—åˆ†: {dim_info['score']:.1f}/5.0ï¼‰\n"
        for qa in qa_list:
            qa_sections += f"**Q**: {qa['question']}\n"
            qa_sections += f"**A**: {qa['answer']}\n"
            if qa.get("score"):
                qa_sections += f"*å•é¢˜è¯„åˆ†: {qa['score']:.1f}*\n"
            qa_sections += "\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é¢è¯•å®˜å’Œäººæ‰è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦åŸºäºä»¥ä¸‹è®¿è°ˆè®°å½•ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„é¢è¯•è¯„ä¼°æŠ¥å‘Šã€‚

## è¯„ä¼°ä¸»é¢˜
{topic}
"""

    if description:
        prompt += f"""
## èƒŒæ™¯è¯´æ˜
{description}
"""

    prompt += f"""
## å„ç»´åº¦å¾—åˆ†

{score_table}

## è®¿è°ˆè®°å½•ä¸è¯„åˆ†
{qa_sections}

## æŠ¥å‘Šè¦æ±‚

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„é¢è¯•è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

### 1. å€™é€‰äººæ¦‚è§ˆ
- è¯„ä¼°ä¸»é¢˜
- è¯„ä¼°æ—¶é—´
- ç»¼åˆå¾—åˆ†ï¼š**{final_score:.2f}/5.0**
- æ¨èç­‰çº§ï¼š**{recommendation.get('name', 'å¾…å®š')}** ({recommendation.get('level', 'C')})

### 2. èƒ½åŠ›é›·è¾¾å›¾
ä½¿ç”¨ Mermaid é›·è¾¾å›¾å±•ç¤ºå„ç»´åº¦å¾—åˆ†ï¼ˆå¦‚æœ Mermaid ä¸æ”¯æŒé›·è¾¾å›¾ï¼Œå¯ç”¨å…¶ä»–å¯è§†åŒ–æ–¹å¼æ›¿ä»£ï¼‰ï¼š

**æ³¨æ„**ï¼šç”±äº Mermaid ä¸åŸç”Ÿæ”¯æŒé›·è¾¾å›¾ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆï¼š

```mermaid
xychart-beta
    title "èƒ½åŠ›è¯„ä¼°é›·è¾¾"
    x-axis [{', '.join([f'"{d["name"]}"' for d in dim_scores_info])}]
    y-axis "å¾—åˆ†" 0 --> 5
    bar [{', '.join([str(d["score"]) for d in dim_scores_info])}]
```

### 3. å„ç»´åº¦è¯¦ç»†åˆ†æ
å¯¹æ¯ä¸ªè¯„ä¼°ç»´åº¦è¿›è¡Œè¯¦ç»†åˆ†æï¼š
- è¯¥ç»´åº¦çš„å¾—åˆ†å’Œè¡¨ç°
- å…·ä½“çš„ä¼˜åŠ¿ä½“ç°
- å­˜åœ¨çš„ä¸è¶³æˆ–å¾…æå‡ç‚¹
- å…³é”®è¯æ®ï¼ˆå¼•ç”¨è®¿è°ˆå†…å®¹ï¼‰

### 4. æ ¸å¿ƒä¼˜åŠ¿
æ€»ç»“å€™é€‰äººçš„ 2-3 ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼Œç”¨å…·ä½“äº‹ä¾‹æ”¯æ’‘

### 5. å¾…æå‡é¢†åŸŸ
æŒ‡å‡º 1-2 ä¸ªéœ€è¦æå‡çš„æ–¹é¢ï¼Œç»™å‡ºå…·ä½“å»ºè®®

### 6. æ¨èæ„è§
åŸºäºç»¼åˆè¯„åˆ† **{final_score:.2f}** ç»™å‡ºï¼š
- æ¨èç­‰çº§ï¼š**{recommendation.get('name', 'å¾…å®š')}**
- ç­‰çº§è¯´æ˜ï¼š{recommendation.get('description', '')}
- å½•ç”¨å»ºè®®ï¼ˆè¯¦ç»†è¯´æ˜å½•ç”¨/ä¸å½•ç”¨çš„ç†ç”±ï¼Œä»¥åŠå¦‚æœå½•ç”¨çš„æ³¨æ„äº‹é¡¹ï¼‰

### 7. åç»­å»ºè®®
- å¦‚éœ€è¿›ä¸€æ­¥è¯„ä¼°çš„é—®é¢˜
- å…¥èŒåçš„åŸ¹å…»å»ºè®®ï¼ˆå¦‚æœæ¨èå½•ç”¨ï¼‰

## é‡è¦æé†’
- æ‰€æœ‰åˆ†æå¿…é¡»ä¸¥æ ¼åŸºäºè®¿è°ˆè®°å½•ä¸­çš„å®é™…å†…å®¹
- è¯„åˆ†å·²ç”± AI åœ¨è®¿è°ˆè¿‡ç¨‹ä¸­é€é¢˜æ‰“åˆ†ï¼Œè¯·åŸºäºè¿™äº›è¯„åˆ†è¿›è¡Œåˆ†æ
- å®¢è§‚å…¬æ­£ï¼Œæ—¢è¦æŒ‡å‡ºä¼˜åŠ¿ä¹Ÿè¦æŒ‡å‡ºä¸è¶³
- æŠ¥å‘Šè¦ä¸“ä¸šã€ç»“æ„æ¸…æ™°ã€æœ‰ç†æœ‰æ®
- ä½¿ç”¨ Markdown æ ¼å¼
- æŠ¥å‘Šæœ«å°¾ä½¿ç”¨ç½²åï¼š*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³-æ™ºèƒ½è®¿è°ˆåŠ©æ‰‹ç”Ÿæˆ*

è¯·ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šï¼š"""

    return prompt


def build_report_prompt(session: dict) -> str:
    """æ„å»ºæŠ¥å‘Šç”Ÿæˆ prompt"""
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¯„ä¼°ç±»å‹æŠ¥å‘Š
    report_type = session.get("scenario_config", {}).get("report", {}).get("type", "standard")
    if report_type == "assessment":
        return build_assessment_report_prompt(session)

    topic = session.get("topic", "æœªçŸ¥é¡¹ç›®")
    description = session.get("description")  # è·å–ä¸»é¢˜æè¿°
    interview_log = session.get("interview_log", [])
    dimensions = session.get("dimensions", {})
    # å…¼å®¹æ—§æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ reference_materialsï¼Œå¦åˆ™åˆå¹¶æ—§å­—æ®µ
    reference_materials = session.get("reference_materials", [])
    if not reference_materials:
        reference_materials = session.get("reference_docs", []) + session.get("research_docs", [])

    # è·å–ä¼šè¯çš„åŠ¨æ€ç»´åº¦ä¿¡æ¯
    report_dim_info = get_dimension_info_for_session(session)

    # æŒ‰ç»´åº¦æ•´ç†é—®ç­”
    qa_by_dim = {}
    for dim_key in report_dim_info:
        qa_by_dim[dim_key] = [log for log in interview_log if log.get("dimension") == dim_key]

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆï¼Œéœ€è¦åŸºäºä»¥ä¸‹è®¿è°ˆè®°å½•ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„è®¿è°ˆæŠ¥å‘Šã€‚

## è®¿è°ˆä¸»é¢˜
{topic}
"""

    # å¦‚æœæœ‰ä¸»é¢˜æè¿°ï¼Œæ·»åŠ åˆ° prompt ä¸­
    if description:
        prompt += f"""
## ä¸»é¢˜æè¿°
{description}
"""

    prompt += """
## å‚è€ƒèµ„æ–™
"""

    if reference_materials:
        prompt += "ä»¥ä¸‹æ˜¯ç”¨æˆ·æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œè¯·åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶å‚è€ƒè¿™äº›å†…å®¹ï¼š\n\n"
        for doc in reference_materials:
            doc_name = doc.get('name', 'æ–‡æ¡£')
            # æ ¹æ® source æ·»åŠ æ ‡è®°
            source_marker = "ğŸ”„ " if doc.get("source") == "auto" else ""
            prompt += f"### {source_marker}{doc_name}\n"
            if doc.get("content"):
                content = doc["content"]
                original_length = len(content)

                # ä½¿ç”¨æ™ºèƒ½æ‘˜è¦å¤„ç†é•¿æ–‡æ¡£
                if original_length > SMART_SUMMARY_THRESHOLD and ENABLE_SMART_SUMMARY:
                    processed_content, is_summarized = summarize_document(content, doc_name, topic)
                    if is_summarized:
                        prompt += f"{processed_content}\n"
                        prompt += f"*[åŸæ–‡æ¡£ {original_length} å­—ç¬¦ï¼Œå·²é€šè¿‡AIç”Ÿæˆæ‘˜è¦ä¿ç•™å…³é”®ä¿¡æ¯]*\n\n"
                    elif len(processed_content) > MAX_DOC_LENGTH:
                        prompt += f"{processed_content[:MAX_DOC_LENGTH]}\n"
                        prompt += f"*[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ {MAX_DOC_LENGTH} å­—ç¬¦]*\n\n"
                    else:
                        prompt += f"{processed_content}\n\n"
                elif original_length > MAX_DOC_LENGTH:
                    prompt += f"{content[:MAX_DOC_LENGTH]}\n"
                    prompt += f"*[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ {MAX_DOC_LENGTH} å­—ç¬¦]*\n\n"
                else:
                    prompt += f"{content}\n\n"
            else:
                prompt += "*[æ–‡æ¡£å†…å®¹ä¸ºç©º]*\n\n"
    else:
        prompt += "æ— å‚è€ƒèµ„æ–™\n"

    prompt += "\n## è®¿è°ˆè®°å½•\n"

    for dim_key, dim_info in report_dim_info.items():
        prompt += f"\n### {dim_info['name']}\n"
        qa_list = qa_by_dim.get(dim_key, [])
        if qa_list:
            for qa in qa_list:
                prompt += f"**Q**: {qa['question']}\n"
                prompt += f"**A**: {qa['answer']}\n\n"
        else:
            prompt += "*è¯¥ç»´åº¦æš‚æ— æ”¶é›†æ•°æ®*\n"

    prompt += """
## æŠ¥å‘Šè¦æ±‚

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„è®¿è°ˆæŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

1. **è®¿è°ˆæ¦‚è¿°** - åŸºæœ¬ä¿¡æ¯ã€è®¿è°ˆèƒŒæ™¯
2. **éœ€æ±‚æ‘˜è¦** - æ ¸å¿ƒéœ€æ±‚åˆ—è¡¨ã€ä¼˜å…ˆçº§çŸ©é˜µ
3. **è¯¦ç»†éœ€æ±‚åˆ†æ**
   - å®¢æˆ·/ç”¨æˆ·éœ€æ±‚ï¼ˆç—›ç‚¹ã€æœŸæœ›ã€åœºæ™¯ã€è§’è‰²ï¼‰
   - ä¸šåŠ¡æµç¨‹ï¼ˆå…³é”®æµç¨‹ã€å†³ç­–èŠ‚ç‚¹ï¼‰
   - æŠ€æœ¯çº¦æŸï¼ˆéƒ¨ç½²ã€é›†æˆã€å®‰å…¨ï¼‰
   - é¡¹ç›®çº¦æŸï¼ˆé¢„ç®—ã€æ—¶é—´ã€èµ„æºï¼‰
4. **å¯è§†åŒ–åˆ†æ** - ä½¿ç”¨ Mermaid å›¾è¡¨å±•ç¤ºå…³é”®ä¿¡æ¯
5. **æ–¹æ¡ˆå»ºè®®** - åŸºäºéœ€æ±‚çš„å¯è¡Œå»ºè®®
6. **é£é™©è¯„ä¼°** - æ½œåœ¨é£é™©å’Œåº”å¯¹ç­–ç•¥
7. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨** - å…·ä½“çš„è¡ŒåŠ¨é¡¹

**æ³¨æ„**ï¼šä¸éœ€è¦åŒ…å«"é™„å½•"ç« èŠ‚ï¼Œå®Œæ•´çš„è®¿è°ˆè®°å½•ä¼šåœ¨æŠ¥å‘Šç”Ÿæˆåè‡ªåŠ¨è¿½åŠ ã€‚

## Mermaid å›¾è¡¨è§„èŒƒ

è¯·åœ¨æŠ¥å‘Šä¸­åŒ…å«ä»¥ä¸‹ç±»å‹çš„ Mermaid å›¾è¡¨ã€‚**é™¤ quadrantChart å¤–ï¼Œæ‰€æœ‰å›¾è¡¨éƒ½åº”ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ã€‚

### 1. ä¼˜å…ˆçº§çŸ©é˜µï¼ˆå¿…é¡»ï¼Œä¸¤ç§å½¢å¼éƒ½è¦ï¼‰

#### 1.1 è±¡é™å›¾ï¼ˆMermaidï¼‰
ä½¿ç”¨ quadrantChart å±•ç¤ºéœ€æ±‚åœ¨é‡è¦æ€§-ç´§æ€¥æ€§åæ ‡ä¸­çš„ä½ç½®ï¼š

```mermaid
quadrantChart
    title Priority Matrix
    x-axis Low Urgency --> High Urgency
    y-axis Low Importance --> High Importance
    quadrant-1 Do First
    quadrant-2 Schedule
    quadrant-3 Delegate
    quadrant-4 Eliminate

    Requirement1: [0.8, 0.9]
    Requirement2: [0.3, 0.7]
```

**è±¡é™å›¾ä¸­æ–‡å›¾ä¾‹è¯´æ˜**ï¼ˆå¿…é¡»åœ¨è±¡é™å›¾ä¸‹æ–¹æ·»åŠ ï¼‰ï¼š
- **æ¨ªè½´**ï¼šç´§æ€¥ç¨‹åº¦ï¼ˆå·¦ä½å³é«˜ï¼‰
- **çºµè½´**ï¼šé‡è¦ç¨‹åº¦ï¼ˆä¸‹ä½ä¸Šé«˜ï¼‰
- **Do Firstï¼ˆç«‹å³æ‰§è¡Œï¼‰**ï¼šå³ä¸Šè±¡é™ï¼Œé‡è¦ä¸”ç´§æ€¥
- **Scheduleï¼ˆè®¡åˆ’æ‰§è¡Œï¼‰**ï¼šå·¦ä¸Šè±¡é™ï¼Œé‡è¦ä½†ä¸ç´§æ€¥
- **Delegateï¼ˆå¯å§”æ´¾ï¼‰**ï¼šå³ä¸‹è±¡é™ï¼Œç´§æ€¥ä½†ä¸é‡è¦
- **Eliminateï¼ˆä½ä¼˜å…ˆçº§ï¼‰**ï¼šå·¦ä¸‹è±¡é™ï¼Œä¸é‡è¦ä¸ç´§æ€¥
- ç„¶ååˆ—å‡ºæ¯ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„ä¸­æ–‡éœ€æ±‚åç§°ï¼Œå¦‚ï¼š`Requirement1 = éœ€æ±‚åç§°1`

**quadrantChart è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
- titleã€x-axisã€y-axisã€quadrant æ ‡ç­¾**å¿…é¡»ç”¨è‹±æ–‡**ï¼ˆæŠ€æœ¯é™åˆ¶ï¼‰
- æ•°æ®ç‚¹åç§°ç”¨è‹±æ–‡æˆ–æ‹¼éŸ³ï¼Œæ ¼å¼ï¼š`Name: [x, y]`ï¼Œxå’ŒyèŒƒå›´0-1
- ä¸è¦ä½¿ç”¨ç‰¹æ®Šç¬¦å·
- **å¿…é¡»åœ¨å›¾è¡¨ä¸‹æ–¹æ·»åŠ ä¸­æ–‡å›¾ä¾‹è¯´æ˜**

#### 1.2 ä¼˜å…ˆçº§æ¸…å•ï¼ˆMarkdownè¡¨æ ¼ï¼‰
ç´§æ¥ç€å›¾ä¾‹è¯´æ˜ï¼Œç”¨ä¸­æ–‡è¡¨æ ¼è¯¦ç»†è¯´æ˜æ¯ä¸ªéœ€æ±‚çš„ä¼˜å…ˆçº§ï¼š

| ä¼˜å…ˆçº§ | éœ€æ±‚é¡¹ | è¯´æ˜ |
|:---:|:---|:---|
| ğŸ”´ P0 ç«‹å³æ‰§è¡Œ | éœ€æ±‚1ã€éœ€æ±‚2 | é‡è¦ä¸”ç´§æ€¥ï¼Œå¿…é¡»ä¼˜å…ˆå¤„ç† |
| ğŸŸ¡ P1 è®¡åˆ’æ‰§è¡Œ | éœ€æ±‚3 | é‡è¦ä½†ä¸ç´§æ€¥ï¼Œéœ€è¦è§„åˆ’ |
| ğŸŸ¢ P2 å¯å§”æ´¾ | éœ€æ±‚4 | ç´§æ€¥ä½†ä¸é‡è¦ï¼Œå¯åˆ†é…ä»–äºº |
| âšª P3 ä½ä¼˜å…ˆçº§ | éœ€æ±‚5 | ä¸é‡è¦ä¸ç´§æ€¥ï¼Œå¯å»¶å |

**ä¸¤ç§å½¢å¼é…åˆä½¿ç”¨**ï¼šè±¡é™å›¾ç›´è§‚å±•ç¤ºä½ç½®åˆ†å¸ƒï¼Œä¸­æ–‡å›¾ä¾‹è§£é‡Šè‹±æ–‡æ ‡ç­¾ï¼Œè¡¨æ ¼è¯¦ç»†è¯´æ˜ä¼˜å…ˆçº§å’Œç†ç”±ã€‚

### 2. ä¸šåŠ¡æµç¨‹å›¾ï¼ˆæ¨èï¼‰
ä½¿ç”¨ flowchart å±•ç¤ºå…³é”®ä¸šåŠ¡æµç¨‹ï¼Œ**ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ï¼š

```mermaid
flowchart TD
    A[å¼€å§‹] --> B{åˆ¤æ–­æ¡ä»¶}
    B -->|æ¡ä»¶æ»¡è¶³| C[å¤„ç†æµç¨‹1]
    B -->|æ¡ä»¶ä¸æ»¡è¶³| D[å¤„ç†æµç¨‹2]
    C --> E[ç»“æŸ]
    D --> E
```

**æ³¨æ„**ï¼šå¸¦æ ‡ç­¾çš„è¿æ¥çº¿æ ¼å¼ä¸º `-->|æ ‡ç­¾|`ï¼Œæ ‡ç­¾å†™åœ¨ç®­å¤´åé¢çš„ç«–çº¿ä¹‹é—´ã€‚

**flowchart è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
- èŠ‚ç‚¹IDä½¿ç”¨è‹±æ–‡å­—æ¯ï¼ˆå¦‚ Aã€Bã€Cï¼‰ï¼ŒèŠ‚ç‚¹æ ‡ç­¾ä½¿ç”¨ä¸­æ–‡ï¼ˆå¦‚ `A[ä¸­æ–‡æ ‡ç­¾]`ï¼‰
- subgraph æ ‡é¢˜ä½¿ç”¨ä¸­æ–‡ï¼ˆå¦‚ `subgraph å­æµç¨‹åç§°`ï¼‰
- **æ¯ä¸ª subgraph å¿…é¡»æœ‰å¯¹åº”çš„ end å…³é—­**
- èŠ‚ç‚¹æ ‡ç­¾ä¸­**ä¸¥ç¦ä½¿ç”¨ä»¥ä¸‹ç‰¹æ®Šå­—ç¬¦**ï¼š
  - åŠè§’å†’å· `:` - ç”¨çŸ­æ¨ªçº¿ `-` æˆ–ç©ºæ ¼æ›¿ä»£
  - åŠè§’å¼•å· `"` - ç”¨å…¨è§’å¼•å· "" æˆ–ä¹¦åå· ã€Šã€‹ æ›¿ä»£
  - åŠè§’æ‹¬å· `()` - ç”¨å…¨è§’æ‹¬å· ï¼ˆï¼‰ æ›¿ä»£
  - HTML æ ‡ç­¾å¦‚ `<br>` - ç”¨ç©ºæ ¼æˆ–æ¢è¡Œæ›¿ä»£
- è±å½¢åˆ¤æ–­èŠ‚ç‚¹ä½¿ç”¨ `{ä¸­æ–‡}` æ ¼å¼
- **ä¸è¦åœ¨åŒä¸€ä¸ª flowchart ä¸­åµŒå¥—è¿‡å¤šå±‚çº§ï¼ˆæœ€å¤š2å±‚ subgraphï¼‰**
- **è¿æ¥çº¿è¯­æ³•è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰**ï¼š
  - æ— æ ‡ç­¾è¿æ¥ï¼š`A --> B`
  - å¸¦æ ‡ç­¾è¿æ¥ï¼š`A -->|æ ‡ç­¾æ–‡å­—| B`ï¼ˆæ ‡ç­¾åœ¨ç®­å¤´åé¢ï¼Œç«–çº¿åŒ…å›´ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A --|æ ‡ç­¾|--> B`ï¼ˆè¿™æ˜¯é”™è¯¯è¯­æ³•ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A -->|æ ‡ç­¾|--> B`ï¼ˆä¸èƒ½æœ‰åŒç®­å¤´ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A --- B`ï¼ˆè™šçº¿æ— ç®­å¤´ï¼‰

### 3. éœ€æ±‚åˆ†ç±»é¥¼å›¾ï¼ˆå¯é€‰ï¼‰
ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾ï¼š
```mermaid
pie title éœ€æ±‚åˆ†å¸ƒ
    "åŠŸèƒ½éœ€æ±‚" : 45
    "æ€§èƒ½éœ€æ±‚" : 25
    "å®‰å…¨éœ€æ±‚" : 20
    "æ˜“ç”¨æ€§" : 10
```

### 4. éƒ¨ç½²æ¶æ„å›¾ï¼ˆå¦‚æ¶‰åŠæŠ€æœ¯çº¦æŸï¼‰
å¦‚æœè®¿è°ˆä¸­æ¶‰åŠéƒ¨ç½²æ¨¡å¼ã€ç³»ç»Ÿæ¶æ„ç­‰æŠ€æœ¯è¯é¢˜ï¼Œå¯ä½¿ç”¨ flowchart å±•ç¤ºéƒ¨ç½²æ¶æ„ï¼š

```mermaid
flowchart LR
    subgraph å‰ç«¯
        A[å®¢æˆ·ç«¯]
    end
    subgraph åç«¯
        B[è´Ÿè½½å‡è¡¡]
        C[åº”ç”¨æœåŠ¡å™¨]
    end
    subgraph å­˜å‚¨
        D[(æ•°æ®åº“)]
    end
    A -->|è¯·æ±‚| B
    B --> C
    C -->|è¯»å†™æ•°æ®| D
```

**éƒ¨ç½²æ¶æ„å›¾è§„åˆ™ï¼š**
- ä½¿ç”¨ flowchart LRï¼ˆä»å·¦åˆ°å³ï¼‰æˆ– flowchart TDï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰
- èŠ‚ç‚¹IDä½¿ç”¨è‹±æ–‡å­—æ¯ï¼Œæ ‡ç­¾ä½¿ç”¨ä¸­æ–‡
- ä¿æŒç»“æ„ç®€æ´ï¼Œé¿å…è¿‡åº¦å¤æ‚çš„åµŒå¥—
- å¸¦æ ‡ç­¾çš„è¿æ¥çº¿ä½¿ç”¨ `-->|æ ‡ç­¾æ–‡å­—|` æ ¼å¼ï¼ˆæ ‡ç­¾åœ¨ç®­å¤´åé¢ï¼‰

## é‡è¦æé†’
- æ‰€æœ‰å†…å®¹å¿…é¡»ä¸¥æ ¼åŸºäºè®¿è°ˆè®°å½•ï¼Œä¸å¾—ç¼–é€ 
- ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒMermaid ä»£ç å—ä½¿ç”¨ ```mermaid æ ‡è®°
- **ä¼˜å…ˆçº§çŸ©é˜µå¿…é¡»åŒæ—¶åŒ…å«ï¼šquadrantChartè±¡é™å›¾ + Markdownè¡¨æ ¼**
- **flowchartã€pie ç­‰å›¾è¡¨ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ï¼ŒquadrantChart å› æŠ€æœ¯é™åˆ¶å¿…é¡»ç”¨è‹±æ–‡
- æŠ¥å‘Šè¦ä¸“ä¸šã€ç»“æ„æ¸…æ™°ã€å¯æ“ä½œ
- **Mermaid è¯­æ³•è¦æ±‚ä¸¥æ ¼ï¼Œè¯·ä»”ç»†æ£€æŸ¥æ¯ä¸ªå›¾è¡¨çš„è¯­æ³•æ­£ç¡®æ€§**
- **flowchart è¿æ¥çº¿å¸¦æ ‡ç­¾è¯­æ³•å¿…é¡»æ˜¯ `A -->|æ ‡ç­¾| B`ï¼Œç¦æ­¢ä½¿ç”¨ `A --|æ ‡ç­¾|--> B`**
- æŠ¥å‘Šæœ«å°¾ä½¿ç”¨ç½²åï¼š*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³-æ™ºèƒ½è®¿è°ˆåŠ©æ‰‹ç”Ÿæˆ*

è¯·ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Šï¼š"""

    return prompt


async def call_claude_async(prompt: str, max_tokens: int = None) -> Optional[str]:
    """å¼‚æ­¥è°ƒç”¨ Claude APIï¼Œå¸¦è¶…æ—¶æ§åˆ¶"""
    if not claude_client:
        return None

    if max_tokens is None:
        max_tokens = MAX_TOKENS_DEFAULT

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– å¼‚æ­¥è°ƒç”¨ Claude APIï¼Œmax_tokens={max_tokens}ï¼Œtimeout={API_TIMEOUT}s")

        # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
        message = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            timeout=API_TIMEOUT
        )

        response_text = message.content[0].text

        if ENABLE_DEBUG_LOG:
            print(f"âœ… API å¼‚æ­¥å“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")

        return response_text
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Claude API å¼‚æ­¥è°ƒç”¨å¤±è´¥: {error_msg}")

        if "timeout" in error_msg.lower():
            print(f"   åŸå› : API è°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{API_TIMEOUT}ç§’ï¼‰")
        elif "rate" in error_msg.lower():
            print(f"   åŸå› : API è¯·æ±‚é¢‘ç‡é™åˆ¶")
        elif "authentication" in error_msg.lower() or "api key" in error_msg.lower():
            print(f"   åŸå› : API Key è®¤è¯å¤±è´¥")

        return None


def describe_image_with_vision(image_path: Path, filename: str) -> str:
    """
    ä½¿ç”¨æ™ºè°±è§†è§‰æ¨¡å‹æè¿°å›¾ç‰‡å†…å®¹

    Args:
        image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        filename: åŸå§‹æ–‡ä»¶å

    Returns:
        str: å›¾ç‰‡æè¿°æ–‡æœ¬
    """
    if not ENABLE_VISION:
        return f"[å›¾ç‰‡: {filename}] (è§†è§‰åŠŸèƒ½å·²ç¦ç”¨)"

    if not ZHIPU_API_KEY or ZHIPU_API_KEY == "your-zhipu-api-key-here":
        return f"[å›¾ç‰‡: {filename}] (è§†è§‰ API æœªé…ç½®)"

    try:
        # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
        with open(image_path, "rb") as f:
            image_data = f.read()

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size_mb = len(image_data) / (1024 * 1024)
        if size_mb > MAX_IMAGE_SIZE_MB:
            return f"[å›¾ç‰‡: {filename}] (æ–‡ä»¶è¿‡å¤§: {size_mb:.1f}MB > {MAX_IMAGE_SIZE_MB}MB)"

        # ç¡®å®š MIME ç±»å‹
        ext = Path(filename).suffix.lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        mime_type = mime_types.get(ext, 'image/jpeg')

        base64_image = base64.b64encode(image_data).decode('utf-8')

        # æ„å»ºè¯·æ±‚
        headers = {
            "Authorization": f"Bearer {ZHIPU_API_KEY}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": VISION_MODEL_NAME,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. å›¾ç‰‡ä¸­çš„å…³é”®å…ƒç´ ï¼ˆäººç‰©ã€ç‰©ä½“ã€æ–‡å­—ç­‰ï¼‰
3. å¦‚æœæ˜¯æµç¨‹å›¾/æ¶æ„å›¾/å›¾è¡¨ï¼Œè¯·è§£è¯»å…¶å«ä¹‰
4. å¦‚æœæœ‰æ–‡å­—ï¼Œè¯·æå–ä¸»è¦æ–‡å­—å†…å®¹

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹å°½é‡å®Œæ•´å‡†ç¡®ã€‚"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 1000
        }

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ–¼ï¸ è°ƒç”¨è§†è§‰ API æè¿°å›¾ç‰‡: {filename}")

        response = requests.post(
            VISION_API_URL,
            headers=headers,
            json=payload,
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            description = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            if description:
                if ENABLE_DEBUG_LOG:
                    print(f"âœ… å›¾ç‰‡æè¿°ç”ŸæˆæˆåŠŸ: {len(description)} å­—ç¬¦")
                return f"[å›¾ç‰‡: {filename}]\n\n**AI å›¾ç‰‡æè¿°:**\n{description}"
            else:
                return f"[å›¾ç‰‡: {filename}] (æè¿°ç”Ÿæˆå¤±è´¥: ç©ºå“åº”)"
        else:
            error_msg = response.json().get("error", {}).get("message", response.text[:200])
            if ENABLE_DEBUG_LOG:
                print(f"âŒ è§†è§‰ API è°ƒç”¨å¤±è´¥: {error_msg}")
            return f"[å›¾ç‰‡: {filename}] (API é”™è¯¯: {error_msg[:100]})"

    except requests.exceptions.Timeout:
        return f"[å›¾ç‰‡: {filename}] (API è¶…æ—¶)"
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âŒ å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {e}")
        return f"[å›¾ç‰‡: {filename}] (å¤„ç†å¤±è´¥: {str(e)[:100]})"


def call_claude(prompt: str, max_tokens: int = None, retry_on_timeout: bool = True,
                call_type: str = "unknown", truncated_docs: list = None) -> Optional[str]:
    """åŒæ­¥è°ƒç”¨ Claude APIï¼Œå¸¦è¶…æ—¶æ§åˆ¶å’Œå®¹é”™æœºåˆ¶"""
    import time

    if not claude_client:
        return None

    if max_tokens is None:
        max_tokens = MAX_TOKENS_DEFAULT

    start_time = time.time()
    success = False
    timeout_occurred = False
    error_message = None
    response_text = None

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– è°ƒç”¨ Claude APIï¼Œmax_tokens={max_tokens}ï¼Œtimeout={API_TIMEOUT}s")

        # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
        message = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            timeout=API_TIMEOUT
        )

        response_text = message.content[0].text
        success = True

        if ENABLE_DEBUG_LOG:
            print(f"âœ… API å“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")

    except Exception as e:
        error_message = str(e)
        print(f"âŒ Claude API è°ƒç”¨å¤±è´¥: {error_message}")

        # è¯¦ç»†çš„é”™è¯¯åˆ†ç±»å’Œå®¹é”™å¤„ç†
        if "timeout" in error_message.lower():
            timeout_occurred = True
            print(f"   åŸå› : API è°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{API_TIMEOUT}ç§’ï¼‰")

            # è¶…æ—¶å®¹é”™ï¼šå¦‚æœå…è®¸é‡è¯•ï¼Œå°è¯•å‡å°‘ prompt é•¿åº¦
            if retry_on_timeout and len(prompt) > 5000:
                print(f"   ğŸ”„ å°è¯•å®¹é”™é‡è¯•ï¼šæˆªæ–­ prompt åé‡è¯•...")
                # æˆªæ–­ prompt åˆ°åŸæ¥çš„ 70%
                truncated_prompt = prompt[:int(len(prompt) * 0.7)]
                truncated_prompt += "\n\n[æ³¨æ„ï¼šç”±äºå†…å®¹è¿‡é•¿ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡å·²è¢«æˆªæ–­ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œå›ç­”]"

                # é€’å½’é‡è¯•ï¼ˆç¦æ­¢å†æ¬¡é‡è¯•ï¼‰
                response_text = call_claude(
                    truncated_prompt, max_tokens,
                    retry_on_timeout=False,
                    call_type=call_type + "_retry",
                    truncated_docs=truncated_docs
                )

                if response_text:
                    success = True

        elif "rate" in error_message.lower():
            print(f"   åŸå› : API è¯·æ±‚é¢‘ç‡é™åˆ¶")
        elif "authentication" in error_message.lower() or "api key" in error_message.lower():
            print(f"   åŸå› : API Key è®¤è¯å¤±è´¥")

    finally:
        # è®°å½•æŒ‡æ ‡
        response_time = time.time() - start_time
        metrics_collector.record_api_call(
            call_type=call_type,
            prompt_length=len(prompt),
            response_time=response_time,
            success=success,
            timeout=timeout_occurred,
            error_msg=error_message if not success else None,
            truncated_docs=truncated_docs,
            max_tokens=max_tokens
        )

    return response_text


# ============ é™æ€æ–‡ä»¶ ============

@app.route('/')
def index():
    return send_from_directory('.', 'index.html')


@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory('.', filename)


# ============ åœºæ™¯ API ============

@app.route('/api/scenarios', methods=['GET'])
def list_scenarios():
    """è·å–æ‰€æœ‰åœºæ™¯åˆ—è¡¨"""
    scenarios = scenario_loader.get_all_scenarios()
    # è¿”å›ç®€åŒ–çš„åœºæ™¯ä¿¡æ¯
    return jsonify([
        {
            "id": s.get("id"),
            "name": s.get("name"),
            "name_en": s.get("name_en"),
            "description": s.get("description"),
            "icon": s.get("icon"),
            "keywords": s.get("keywords", []),
            "builtin": s.get("builtin", True),
            "custom": s.get("custom", False),
            "dimensions": [
                {
                    "id": d.get("id"),
                    "name": d.get("name"),
                    "description": d.get("description")
                }
                for d in s.get("dimensions", [])
            ],
            "report_type": s.get("report", {}).get("type", "standard")
        }
        for s in scenarios
    ])


@app.route('/api/scenarios/<scenario_id>', methods=['GET'])
def get_scenario(scenario_id):
    """è·å–åœºæ™¯è¯¦æƒ…"""
    scenario = scenario_loader.get_scenario(scenario_id)
    if not scenario:
        return jsonify({"error": "åœºæ™¯ä¸å­˜åœ¨"}), 404
    return jsonify(scenario)


@app.route('/api/scenarios/generate', methods=['POST'])
def generate_scenario_with_ai():
    """AI è‡ªåŠ¨ç”Ÿæˆåœºæ™¯é…ç½®"""
    if not claude_client:
        return jsonify({"error": "AI æœåŠ¡ä¸å¯ç”¨"}), 503

    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    user_description = data.get("user_description", "").strip()
    if not user_description:
        return jsonify({"error": "è¯·è¾“å…¥åœºæ™¯æè¿°"}), 400

    if len(user_description) < 10:
        return jsonify({"error": "æè¿°å¤ªçŸ­ï¼Œè¯·è‡³å°‘è¾“å…¥10ä¸ªå­—"}), 400

    if len(user_description) > 500:
        return jsonify({"error": "æè¿°ä¸èƒ½è¶…è¿‡500å­—"}), 400

    # æ„å»º Prompt
    prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¿è°ˆåœºæ™¯è®¾è®¡å¸ˆã€‚ç”¨æˆ·å°†æè¿°ä»–ä»¬æƒ³è¦è¿›è¡Œçš„è®¿è°ˆæˆ–è°ƒç ”ç›®æ ‡ï¼Œä½ éœ€è¦è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„è®¿è°ˆåœºæ™¯é…ç½®ã€‚

## ç”¨æˆ·æè¿°
{user_description}

## è®¾è®¡è¦æ±‚
1. åœºæ™¯åç§°ï¼šç®€æ´æ˜äº†ï¼Œ4-10ä¸ªå­—
2. åœºæ™¯æè¿°ï¼šè¯´æ˜åœºæ™¯é€‚ç”¨èŒƒå›´ï¼Œ20-50å­—
3. å…³é”®è¯ï¼š5-10ä¸ªç”¨äºè‡ªåŠ¨åŒ¹é…çš„å…³é”®è¯
4. ç»´åº¦è®¾è®¡ï¼š3-5ä¸ªç»´åº¦ï¼ˆæ ¹æ®è®¿è°ˆå¤æ‚åº¦è°ƒæ•´ï¼‰
   - æ¯ä¸ªç»´åº¦éœ€è¦æœ‰æ¸…æ™°çš„åç§°ï¼ˆ2-6å­—ï¼‰
   - ç»´åº¦æè¿°è¯´æ˜è¯¥ç»´åº¦å…³æ³¨çš„å†…å®¹ï¼ˆ10-30å­—ï¼‰
   - æ¯ä¸ªç»´åº¦åŒ…å«3-5ä¸ªå…³é”®ç‚¹ï¼ˆkey_aspectsï¼‰
   - min_questions å›ºå®šä¸º 2ï¼Œmax_questions å›ºå®šä¸º 4

## è®¾è®¡åŸåˆ™
- ç»´åº¦ä¹‹é—´åº”è¯¥äº’è¡¥ï¼Œå…±åŒè¦†ç›–ç”¨æˆ·å…³å¿ƒçš„æ‰€æœ‰æ–¹é¢
- ç»´åº¦é¡ºåºåº”è¯¥ç¬¦åˆè®¤çŸ¥é€»è¾‘ï¼ˆå¦‚ä»å…·ä½“åˆ°æŠ½è±¡ï¼Œä»æ ¸å¿ƒåˆ°å¤–å›´ï¼‰
- å…³é”®ç‚¹åº”è¯¥å…·ä½“å¯é—®ï¼Œä¾¿äºAIç”Ÿæˆè®¿è°ˆé—®é¢˜
- å¦‚æœç”¨æˆ·æè¿°æ¶‰åŠè¯„ä¼°/è¯„åˆ†ç±»åœºæ™¯ï¼Œå¯è€ƒè™‘åœ¨ç»´åº¦ä¸­åŠ å…¥è¯„åˆ†ç›¸å…³çš„å…³é”®ç‚¹

## å‚è€ƒï¼šç°æœ‰åœºæ™¯ç¤ºä¾‹
- äº§å“éœ€æ±‚ï¼šå®¢æˆ·éœ€æ±‚ã€ä¸šåŠ¡æµç¨‹ã€æŠ€æœ¯çº¦æŸã€é¡¹ç›®çº¦æŸ
- ç”¨æˆ·ç ”ç©¶ï¼šç”¨æˆ·èƒŒæ™¯ã€ä½¿ç”¨åœºæ™¯ã€ç—›ç‚¹æœŸæœ›ã€è¡Œä¸ºæ¨¡å¼
- ç«å“åˆ†æï¼šå¸‚åœºå®šä½ã€åŠŸèƒ½å¯¹æ¯”ã€ç”¨æˆ·è¯„ä»·ã€å·®å¼‚åŒ–æœºä¼š

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼š
```json
{{
  "name": "åœºæ™¯åç§°",
  "description": "åœºæ™¯æè¿°",
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
  "dimensions": [
    {{
      "id": "dim_1",
      "name": "ç»´åº¦åç§°",
      "description": "ç»´åº¦æè¿°",
      "key_aspects": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3", "å…³é”®ç‚¹4"],
      "min_questions": 2,
      "max_questions": 4
    }}
  ],
  "explanation": "è®¾è®¡æ€è·¯è¯´æ˜ï¼ˆ1-2å¥è¯ï¼Œè§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ç»´åº¦ï¼‰"
}}
```'''

    try:
        response = claude_client.messages.create(
            model=MODEL_NAME,
            max_tokens=1500,
            timeout=30.0,
            messages=[{"role": "user", "content": prompt}]
        )

        raw_text = response.content[0].text.strip()

        # æå– JSONï¼ˆå…¼å®¹æ¨¡å‹è¿”å› markdown ä»£ç å—ï¼‰
        if "```json" in raw_text:
            raw_text = raw_text.split("```json")[1].split("```")[0].strip()
        elif "```" in raw_text:
            raw_text = raw_text.split("```")[1].split("```")[0].strip()

        generated = json.loads(raw_text)

        # éªŒè¯å¿…è¦å­—æ®µ
        if not generated.get("name"):
            return jsonify({"error": "ç”Ÿæˆçš„åœºæ™¯ç¼ºå°‘åç§°"}), 500

        if not generated.get("dimensions") or len(generated["dimensions"]) < 1:
            return jsonify({"error": "ç”Ÿæˆçš„åœºæ™¯ç¼ºå°‘ç»´åº¦"}), 500

        # æå– explanation å¹¶ç§»é™¤ï¼ˆä¸å­˜å…¥åœºæ™¯é…ç½®ï¼‰
        ai_explanation = generated.pop("explanation", "")

        # ç¡®ä¿ç»´åº¦æ ¼å¼æ­£ç¡®
        for i, dim in enumerate(generated["dimensions"]):
            dim["id"] = f"dim_{i + 1}"
            dim.setdefault("min_questions", 2)
            dim.setdefault("max_questions", 4)
            if not isinstance(dim.get("key_aspects"), list):
                dim["key_aspects"] = []

        # æ·»åŠ é»˜è®¤çš„ report é…ç½®
        generated["report"] = {"type": "standard"}

        return jsonify({
            "success": True,
            "generated_scenario": generated,
            "ai_explanation": ai_explanation
        })

    except json.JSONDecodeError as e:
        print(f"âš ï¸ AI ç”Ÿæˆåœºæ™¯ JSON è§£æå¤±è´¥: {e}")
        return jsonify({"error": "AI è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè¯·é‡è¯•"}), 500
    except Exception as e:
        print(f"âš ï¸ AI ç”Ÿæˆåœºæ™¯å¤±è´¥: {e}")
        return jsonify({"error": f"ç”Ÿæˆå¤±è´¥: {str(e)[:100]}"}), 500


@app.route('/api/scenarios/custom', methods=['POST'])
def create_custom_scenario():
    """åˆ›å»ºè‡ªå®šä¹‰åœºæ™¯"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    name = data.get("name", "").strip()
    if not name:
        return jsonify({"error": "åœºæ™¯åç§°ä¸èƒ½ä¸ºç©º"}), 400

    dimensions = data.get("dimensions", [])
    if not dimensions or len(dimensions) < 1:
        return jsonify({"error": "è‡³å°‘éœ€è¦ 1 ä¸ªç»´åº¦"}), 400
    if len(dimensions) > 8:
        return jsonify({"error": "æœ€å¤šæ”¯æŒ 8 ä¸ªç»´åº¦"}), 400

    # éªŒè¯ç»´åº¦æ•°æ®
    for i, dim in enumerate(dimensions):
        if not dim.get("name", "").strip():
            return jsonify({"error": f"ç¬¬ {i+1} ä¸ªç»´åº¦åç§°ä¸èƒ½ä¸ºç©º"}), 400
        # è‡ªåŠ¨ç”Ÿæˆç»´åº¦ ID
        if not dim.get("id"):
            dim["id"] = f"dim_{i+1}"
        # ç¡®ä¿æœ‰å¿…è¦å­—æ®µ
        dim.setdefault("description", "")
        dim.setdefault("key_aspects", [])
        dim.setdefault("min_questions", 2)
        dim.setdefault("max_questions", 4)

    scenario = {
        "name": name,
        "description": data.get("description", "").strip(),
        "icon": data.get("icon", "clipboard-list"),
        "keywords": data.get("keywords", []),
        "dimensions": dimensions,
        "report": data.get("report", {"type": "standard"}),
    }

    scenario_id = scenario_loader.save_custom_scenario(scenario)

    return jsonify({
        "success": True,
        "scenario_id": scenario_id,
        "scenario": scenario_loader.get_scenario(scenario_id)
    })


@app.route('/api/scenarios/custom/<scenario_id>', methods=['DELETE'])
def delete_custom_scenario(scenario_id):
    """åˆ é™¤è‡ªå®šä¹‰åœºæ™¯"""
    if not scenario_id.startswith("custom-"):
        return jsonify({"error": "åªèƒ½åˆ é™¤è‡ªå®šä¹‰åœºæ™¯"}), 400

    success = scenario_loader.delete_custom_scenario(scenario_id)
    if not success:
        return jsonify({"error": "åœºæ™¯ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤"}), 404

    return jsonify({"success": True})


@app.route('/api/scenarios/recognize', methods=['POST'])
def recognize_scenario():
    """æ ¹æ®ä¸»é¢˜å’Œæè¿°æ™ºèƒ½è¯†åˆ«æœ€åŒ¹é…çš„è®¿è°ˆåœºæ™¯"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    topic = data.get("topic", "")
    description = data.get("description", "")
    if not topic:
        return jsonify({"error": "ä¸»é¢˜ä¸èƒ½ä¸ºç©º"}), 400

    # æ„å»ºåœºæ™¯æ‘˜è¦ä¾› AI åˆ¤æ–­
    all_scenarios = scenario_loader.get_all_scenarios()
    scenario_list_text = "\n".join(
        f"- id: {s['id']}, åç§°: {s['name']}, è¯´æ˜: {s.get('description', '')}"
        for s in all_scenarios
    )

    user_input = f"è®¿è°ˆä¸»é¢˜ï¼š{topic}"
    if description:
        user_input += f"\nä¸»é¢˜æè¿°ï¼š{description}"

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®¿è°ˆåœºæ™¯åˆ†ç±»å™¨ã€‚æ ¹æ®ç”¨æˆ·çš„è®¿è°ˆä¸»é¢˜å’Œæè¿°ï¼Œä»ä»¥ä¸‹åœºæ™¯ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªã€‚

å¯é€‰åœºæ™¯ï¼š
{scenario_list_text}

{user_input}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{"scenario_id": "æœ€åŒ¹é…çš„åœºæ™¯id", "confidence": 0.0åˆ°1.0çš„ç½®ä¿¡åº¦, "reason": "ä¸€å¥è¯ç†ç”±"}}"""

    # ä¼˜å…ˆä½¿ç”¨ AI è¯†åˆ«ï¼Œå¤±è´¥æ—¶å›é€€åˆ°å…³é”®è¯åŒ¹é…
    ai_result = None
    if claude_client:
        try:
            response = claude_client.messages.create(
                model=MODEL_NAME,
                max_tokens=200,
                timeout=10.0,
                messages=[{"role": "user", "content": prompt}]
            )
            raw = response.content[0].text.strip()
            # æå– JSONï¼ˆå…¼å®¹æ¨¡å‹è¿”å› markdown ä»£ç å—ï¼‰
            if "```" in raw:
                raw = raw.split("```")[1]
                if raw.startswith("json"):
                    raw = raw[4:]
                raw = raw.strip()
            ai_result = json.loads(raw)
        except Exception as e:
            print(f"âš ï¸  AI åœºæ™¯è¯†åˆ«å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {e}")

    if ai_result and ai_result.get("scenario_id") in [s["id"] for s in all_scenarios]:
        best_id = ai_result["scenario_id"]
        confidence = min(1.0, max(0.0, float(ai_result.get("confidence", 0.8))))
        reason = ai_result.get("reason", "")
    else:
        # å›é€€ï¼šå…³é”®è¯åŒ¹é…
        kw_result = scenario_loader.match_by_keywords(topic)
        best_id = kw_result["scenario_id"]
        confidence = kw_result["confidence"]
        reason = ""

    recommended_scenario = scenario_loader.get_scenario(best_id)

    return jsonify({
        "recommended": {
            "id": best_id,
            "name": recommended_scenario.get("name") if recommended_scenario else best_id,
            "description": recommended_scenario.get("description") if recommended_scenario else "",
            "icon": recommended_scenario.get("icon") if recommended_scenario else "clipboard-list",
            "dimensions_count": len(recommended_scenario.get("dimensions", [])) if recommended_scenario else 4
        },
        "confidence": confidence,
        "reason": reason
    })


# ============ ä¼šè¯ API ============

@app.route('/api/sessions', methods=['GET'])
def list_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    sessions = []
    for f in SESSIONS_DIR.glob("*.json"):
        try:
            data = json.loads(f.read_text(encoding="utf-8"))
            sessions.append({
                "session_id": data.get("session_id"),
                "topic": data.get("topic"),
                "status": data.get("status"),
                "created_at": data.get("created_at"),
                "updated_at": data.get("updated_at"),
                "dimensions": data.get("dimensions", {}),
                "interview_count": len(data.get("interview_log", [])),
                "scenario_id": data.get("scenario_id"),
                "scenario_config": data.get("scenario_config")
            })
        except Exception as e:
            print(f"è¯»å–ä¼šè¯å¤±è´¥ {f}: {e}")

    sessions.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
    return jsonify(sessions)


@app.route('/api/sessions', methods=['POST'])
def create_session():
    """åˆ›å»ºæ–°ä¼šè¯"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    topic = data.get("topic", "æœªå‘½åè®¿è°ˆ")
    description = data.get("description")  # è·å–å¯é€‰çš„ä¸»é¢˜æè¿°
    interview_mode = data.get("interview_mode", DEFAULT_INTERVIEW_MODE)  # è·å–è®¿è°ˆæ¨¡å¼
    scenario_id = data.get("scenario_id")  # è·å–åœºæ™¯ID

    # éªŒè¯ topic
    if not isinstance(topic, str) or not topic.strip():
        return jsonify({"error": "ä¸»é¢˜ä¸èƒ½ä¸ºç©º"}), 400
    if len(topic) > 200:
        return jsonify({"error": "ä¸»é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡200å­—ç¬¦"}), 400

    # éªŒè¯ description
    if description and (not isinstance(description, str) or len(description) > 2000):
        return jsonify({"error": "æè¿°é•¿åº¦ä¸èƒ½è¶…è¿‡2000å­—ç¬¦"}), 400

    # éªŒè¯è®¿è°ˆæ¨¡å¼
    if interview_mode not in INTERVIEW_MODES:
        interview_mode = DEFAULT_INTERVIEW_MODE

    # åŠ è½½åœºæ™¯é…ç½®ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯ï¼‰
    if not scenario_id:
        scenario_id = "product-requirement"
    scenario_config = scenario_loader.get_scenario(scenario_id)
    if not scenario_config:
        scenario_config = scenario_loader.get_default_scenario()
        scenario_id = scenario_config.get("id", "product-requirement")

    # æ ¹æ®åœºæ™¯é…ç½®åˆ›å»ºåŠ¨æ€ç»´åº¦
    dimensions = {}
    for dim in scenario_config.get("dimensions", []):
        dimensions[dim["id"]] = {
            "coverage": 0,
            "items": [],
            "score": None  # ç”¨äºè¯„ä¼°å‹åœºæ™¯
        }

    session_id = generate_session_id()
    now = get_utc_now()

    session = {
        "session_id": session_id,
        "topic": topic,
        "description": description,  # å­˜å‚¨ä¸»é¢˜æè¿°
        "interview_mode": interview_mode,  # å­˜å‚¨è®¿è°ˆæ¨¡å¼
        "created_at": now,
        "updated_at": now,
        "status": "in_progress",
        "scenario_id": scenario_id,  # å­˜å‚¨åœºæ™¯ID
        "scenario_config": scenario_config,  # å­˜å‚¨åœºæ™¯å®Œæ•´é…ç½®
        "dimensions": dimensions,  # åŠ¨æ€ç»´åº¦
        "reference_materials": [],  # å‚è€ƒèµ„æ–™ï¼ˆåˆå¹¶åŸ reference_docs å’Œ research_docsï¼‰
        "interview_log": [],
        "requirements": [],
        "summary": None
    }

    session_file = SESSIONS_DIR / f"{session_id}.json"
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # ========== æ­¥éª¤6: é¢„ç”Ÿæˆé¦–é¢˜ ==========
    try:
        prefetch_first_question(session_id)
    except Exception as e:
        # é¢„ç”Ÿæˆå¤±è´¥ä¸å½±å“ä¼šè¯åˆ›å»º
        print(f"âš ï¸ é¢„ç”Ÿæˆé¦–é¢˜å¤±è´¥: {e}")

    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """è·å–ä¼šè¯è¯¦æƒ…"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = safe_load_session(session_file)
    if session is None:
        return jsonify({"error": "ä¼šè¯æ•°æ®æŸå"}), 500

    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯æ ¼å¼
    session = migrate_session_docs(session)
    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['PUT'])
def update_session(session_id):
    """æ›´æ–°ä¼šè¯"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    updates = request.get_json()
    session = safe_load_session(session_file)
    if session is None:
        return jsonify({"error": "ä¼šè¯æ•°æ®æŸå"}), 500

    # å®šä¹‰å…è®¸æ›´æ–°çš„å­—æ®µç™½åå•
    UPDATABLE_FIELDS = {"description", "topic", "status"}

    for key, value in updates.items():
        if key != "session_id" and key in UPDATABLE_FIELDS:
            session[key] = value

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if session_file.exists():
        session_file.unlink()

    # ========== æ­¥éª¤7: æ¸…ç†ç¼“å­˜å’ŒçŠ¶æ€ ==========
    invalidate_prefetch(session_id)
    clear_thinking_status(session_id)

    return jsonify({"success": True})


# ============ AI é©±åŠ¨çš„è®¿è°ˆ API ============

def parse_question_response(response: str, debug: bool = False) -> Optional[dict]:
    """è§£æ AI è¿”å›çš„é—®é¢˜ JSON å“åº”

    ä½¿ç”¨5ç§é€’è¿›å¼è§£æç­–ç•¥ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦æå–æœ‰æ•ˆJSONã€‚

    Args:
        response: AI è¿”å›çš„åŸå§‹å“åº”æ–‡æœ¬
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—

    Returns:
        è§£æåçš„ dictï¼ˆåŒ…å« question å’Œ optionsï¼‰ï¼Œå¤±è´¥è¿”å› None
    """
    import re

    result = None
    parse_error = None

    if debug:
        print(f"ğŸ“ AI åŸå§‹å“åº” (å‰500å­—): {response[:500]}")

    # æ–¹æ³•1: ç›´æ¥å°è¯•è§£æï¼ˆå¦‚æœAIä¸¥æ ¼éµå®ˆæŒ‡ä»¤ï¼‰
    try:
        cleaned = response.strip()
        if cleaned.startswith('{') and cleaned.endswith('}'):
            result = json.loads(cleaned)
            if debug:
                print(f"âœ… æ–¹æ³•1æˆåŠŸ: ç›´æ¥è§£æ")
    except json.JSONDecodeError as e:
        parse_error = e
        if debug:
            print(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")

    # æ–¹æ³•2: å°è¯•æå– ```json ä»£ç å—
    if result is None and "```json" in response:
        try:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end > json_start:
                json_str = response[json_start:json_end].strip()
                result = json.loads(json_str)
                if debug:
                    print(f"âœ… æ–¹æ³•2æˆåŠŸ: ä»ä»£ç å—æå–")
        except json.JSONDecodeError as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•2å¤±è´¥ (JSONé”™è¯¯): {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•2å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆèŠ±æ‹¬å·é…å¯¹ï¼‰
    if result is None:
        try:
            json_start = response.find('{')
            if json_start >= 0:
                brace_count = 0
                json_end = -1
                in_string = False
                escape_next = False

                for i in range(json_start, len(response)):
                    char = response[i]

                    if escape_next:
                        escape_next = False
                        continue

                    if char == '\\':
                        escape_next = True
                        continue

                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue

                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_end = i + 1
                                break

                if json_end > json_start:
                    try:
                        json_str = response[json_start:json_end]
                        result = json.loads(json_str)
                        if debug:
                            print(f"âœ… æ–¹æ³•3æˆåŠŸ: èŠ±æ‹¬å·é…å¯¹æå–")
                    except json.JSONDecodeError as e:
                        parse_error = e
                        if debug:
                            print(f"âš ï¸ æ–¹æ³•3å¤±è´¥ (JSONé”™è¯¯): {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•3å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•4: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– JSON å¯¹è±¡
    if result is None:
        try:
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.findall(json_pattern, response, re.DOTALL)
            for match in matches:
                try:
                    candidate = json.loads(match)
                    # éªŒè¯å¿…é¡»æœ‰ question å­—æ®µ
                    if isinstance(candidate, dict) and "question" in candidate:
                        result = candidate
                        if debug:
                            print(f"âœ… æ–¹æ³•4æˆåŠŸ: æ­£åˆ™è¡¨è¾¾å¼æå–")
                        break
                except json.JSONDecodeError:
                    continue
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•4å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•5: å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSONï¼ˆè¡¥å…¨ç¼ºå¤±å­—æ®µï¼‰
    if result is None and '{' in response and '"question"' in response:
        try:
            if debug:
                print(f"ğŸ”§ å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSON...")

            # æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹ä½ç½®
            json_start = response.find('{')
            json_content = response[json_start:]

            # å°è¯•è¡¥å…¨ç¼ºå¤±çš„ç»“å°¾éƒ¨åˆ†
            if '"options"' in json_content and '"question"' in json_content:
                # å¦‚æœæœ‰optionsæ•°ç»„ä½†æ²¡æœ‰æ­£ç¡®ç»“æŸï¼Œå°è¯•è¡¥å…¨
                if json_content.count('[') > json_content.count(']'):
                    json_content += ']'
                if json_content.count('{') > json_content.count('}'):
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    if '"multi_select"' not in json_content:
                        json_content += ', "multi_select": false'
                    if '"is_follow_up"' not in json_content:
                        json_content += ', "is_follow_up": false'
                    json_content += '}'

                # å°è¯•è§£æä¿®å¤åçš„JSON
                try:
                    result = json.loads(json_content)
                    if isinstance(result, dict) and "question" in result:
                        if debug:
                            print(f"âœ… æ–¹æ³•5æˆåŠŸ: JSONä¿®å¤å®Œæˆ")
                except json.JSONDecodeError as e:
                    if debug:
                        print(f"âš ï¸ æ–¹æ³•5å¤±è´¥: ä¿®å¤åä»æ— æ³•è§£æ - {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•5å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # éªŒè¯ç»“æœ
    if result is not None and isinstance(result, dict):
        if "question" in result and "options" in result:
            # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
            if "multi_select" not in result:
                result["multi_select"] = False
            if "is_follow_up" not in result:
                result["is_follow_up"] = False
            return result

    # æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥äº†
    if debug:
        print(f"âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥")
        print(f"ğŸ“„ AI å“åº”å‰500å­—ç¬¦:\n{response[:500] if response else 'None'}")
        print(f"ğŸ“„ æœ€åè§£æé”™è¯¯: {str(parse_error) if parse_error else 'æœªçŸ¥'}")

    return None

@app.route('/api/sessions/<session_id>/next-question', methods=['POST'])
def get_next_question(session_id):
    """è·å–ä¸‹ä¸€ä¸ªé—®é¢˜ï¼ˆAI ç”Ÿæˆï¼‰"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))
    data = request.get_json() or {}
    default_dim = get_dimension_order_for_session(session)[0] if get_dimension_order_for_session(session) else "customer_needs"
    dimension = data.get("dimension", default_dim)

    # ========== æ­¥éª¤5: æ£€æŸ¥é¢„ç”Ÿæˆç¼“å­˜ ==========
    prefetched = get_prefetch_result(session_id, dimension)
    if prefetched:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¯ é¢„ç”Ÿæˆç¼“å­˜å‘½ä¸­: session={session_id}, dimension={dimension}")

        # å…ˆæ£€æŸ¥ç»´åº¦æ˜¯å¦å·²å®Œæˆï¼ˆå³ä½¿æœ‰ç¼“å­˜ä¹Ÿè¦æ£€æŸ¥ï¼‰
        dim_data = session.get("dimensions", {}).get(dimension, {})
        dim_coverage = dim_data.get("coverage", 0)
        user_completed = dim_data.get("user_completed", False)
        if dim_coverage >= 100 or user_completed:
            # ç»´åº¦å·²å®Œæˆï¼Œå¿½ç•¥ç¼“å­˜ï¼Œè¿”å›å®ŒæˆçŠ¶æ€
            all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
            formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])
            dim_follow_ups = len([log for log in all_dim_logs if log.get("is_follow_up", False)])
            return jsonify({
                "dimension": dimension,
                "completed": True,
                "stats": {
                    "formal_questions": formal_questions_count,
                    "follow_ups": dim_follow_ups,
                    "saturation": 1.0
                }
            })

        # å¯¹é¢„ç”Ÿæˆç»“æœä¹Ÿåš is_follow_up æ ¡éªŒ
        all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
        if prefetched.get("is_follow_up", False) and all_dim_logs:
            last_log = all_dim_logs[-1]
            eval_result = evaluate_answer_depth(
                question=last_log.get("question", ""),
                answer=last_log.get("answer", ""),
                dimension=dimension,
                options=last_log.get("options", []),
                is_follow_up=last_log.get("is_follow_up", False)
            )
            comprehensive_check = should_follow_up_comprehensive(
                session=session,
                dimension=dimension,
                rule_based_result=eval_result
            )
            if not comprehensive_check["should_follow_up"]:
                if ENABLE_DEBUG_LOG:
                    print(f"âš ï¸ é¢„ç”Ÿæˆç¼“å­˜ is_follow_up=true ä½†åç«¯å†³ç­–ä¸å…è®¸ï¼Œå¼ºåˆ¶è¦†ç›–ä¸º false")
                prefetched["is_follow_up"] = False
                prefetched["follow_up_reason"] = None

        prefetched["prefetched"] = True
        return jsonify(prefetched)

    # æ£€æŸ¥æ˜¯å¦æœ‰ Claude API
    if not claude_client:
        return jsonify({
            "error": "AI æœåŠ¡æœªå¯ç”¨",
            "detail": "è¯·è”ç³»ç®¡ç†å‘˜é…ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡"
        }), 503

    # è·å–å½“å‰ç»´åº¦çš„æ‰€æœ‰è®°å½•
    all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]

    # è®¡ç®—æ­£å¼é—®é¢˜æ•°é‡ï¼ˆæ’é™¤è¿½é—®ï¼‰
    formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])

    # è·å–è®¿è°ˆæ¨¡å¼é…ç½®
    mode_config = get_interview_mode_config(session)
    required_formal_questions = mode_config["formal_questions_per_dim"]

    # è·å–å½“å‰ç»´åº¦çŠ¶æ€
    dim_data = session.get("dimensions", {}).get(dimension, {})
    dim_coverage = dim_data.get("coverage", 0)
    user_completed = dim_data.get("user_completed", False)

    # æ£€æŸ¥ç»´åº¦æ˜¯å¦å·²å®Œæˆï¼š
    # 1. æ­£å¼é—®é¢˜è¾¾åˆ°é…ç½®æ•°é‡
    # 2. æˆ–è€… coverage å·²ç» >= 100%ï¼ˆå¯èƒ½æ˜¯ç”¨æˆ·æ‰‹åŠ¨å®Œæˆï¼‰
    # 3. æˆ–è€…ç”¨æˆ·æ ‡è®°äº† user_completed
    if formal_questions_count >= required_formal_questions or dim_coverage >= 100 or user_completed:
        # ä½¿ç”¨ç»¼åˆå†³ç­–æ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦è¿½é—®
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„è§„åˆ™è¯„ä¼°ç»“æœæ¥è§¦å‘ç»¼åˆæ£€æŸ¥
        comprehensive_check = should_follow_up_comprehensive(
            session=session,
            dimension=dimension,
            rule_based_result={"needs_follow_up": False, "signals": []}
        )

        # å¦‚æœé¢„ç®—å·²ç”¨å®Œæˆ–é¥±å’Œåº¦è¶³å¤Ÿé«˜ï¼Œç›´æ¥å®Œæˆç»´åº¦
        budget_status = comprehensive_check.get("budget_status", {})
        saturation = comprehensive_check.get("saturation", {})

        should_complete = (
            not budget_status.get("can_follow_up", True) or
            saturation.get("level") == "high" or
            formal_questions_count >= required_formal_questions
        )

        if should_complete:
            # è®¡ç®—ç»´åº¦å®Œæˆçš„ç»Ÿè®¡ä¿¡æ¯
            dim_follow_ups = len([log for log in all_dim_logs if log.get("is_follow_up", False)])
            return jsonify({
                "dimension": dimension,
                "completed": True,
                "stats": {
                    "formal_questions": formal_questions_count,
                    "follow_ups": dim_follow_ups,
                    "saturation": saturation.get("saturation_score", 0) if saturation else 0
                }
            })

    # è°ƒç”¨ Claude ç”Ÿæˆé—®é¢˜
    # åˆ¤æ–­æ˜¯å¦ä¼šæœ‰æœç´¢ï¼ˆç”¨äºè®¾ç½®æ­£ç¡®çš„é˜¶æ®µæ•°ï¼‰
    has_search = should_search(session.get("topic", ""), dimension, session)

    try:
        # é˜¶æ®µ1: åˆ†æå›ç­”
        update_thinking_status(session_id, "analyzing", has_search)

        prompt, truncated_docs = build_interview_prompt(session, dimension, all_dim_logs, session_id=session_id)

        # æ—¥å¿—ï¼šè®°å½• prompt é•¿åº¦ï¼ˆä¾¿äºç›‘æ§å’Œè°ƒä¼˜ï¼‰
        if ENABLE_DEBUG_LOG:
            ref_docs_count = len(session.get("reference_materials", session.get("reference_docs", []) + session.get("research_docs", [])))
            print(f"ğŸ“Š è®¿è°ˆ Prompt ç»Ÿè®¡ï¼šæ€»é•¿åº¦={len(prompt)}å­—ç¬¦ï¼Œå‚è€ƒèµ„æ–™={ref_docs_count}ä¸ª")
            if truncated_docs:
                print(f"âš ï¸  æ–‡æ¡£æˆªæ–­ï¼š{len(truncated_docs)}ä¸ªæ–‡æ¡£è¢«æˆªæ–­")

        # é˜¶æ®µ3: ç”Ÿæˆé—®é¢˜
        update_thinking_status(session_id, "generating", has_search)

        response = call_claude(
            prompt,
            max_tokens=MAX_TOKENS_QUESTION,
            call_type="question",
            truncated_docs=truncated_docs
        )

        if not response:
            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            clear_thinking_status(session_id)
            return jsonify({
                "error": "AI å“åº”å¤±è´¥",
                "detail": "æœªèƒ½ä» AI æœåŠ¡è·å–å“åº”ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
            }), 503

        # ä½¿ç”¨æŠ½å–çš„è§£æå‡½æ•°è§£æ JSON å“åº”
        result = parse_question_response(response, debug=ENABLE_DEBUG_LOG)

        if result:
            result["dimension"] = dimension
            result["ai_generated"] = True

            # ========== åç«¯å¼ºåˆ¶æ ¡éªŒ is_follow_up ==========
            # é˜²æ­¢ AI ç»•è¿‡è¿½é—®é¢„ç®—æ§åˆ¶ï¼Œè‡ªè¡Œå°†é—®é¢˜æ ‡è®°ä¸ºè¿½é—®
            if result.get("is_follow_up", False):
                # é‡æ–°è®¡ç®—è¿½é—®å†³ç­–
                last_log = all_dim_logs[-1] if all_dim_logs else None
                if last_log:
                    eval_result = evaluate_answer_depth(
                        question=last_log.get("question", ""),
                        answer=last_log.get("answer", ""),
                        dimension=dimension,
                        options=last_log.get("options", []),
                        is_follow_up=last_log.get("is_follow_up", False)
                    )
                    comprehensive_check = should_follow_up_comprehensive(
                        session=session,
                        dimension=dimension,
                        rule_based_result=eval_result
                    )
                    if not comprehensive_check["should_follow_up"]:
                        if ENABLE_DEBUG_LOG:
                            print(f"âš ï¸ AI è¿”å› is_follow_up=true ä½†åç«¯å†³ç­–ä¸å…è®¸è¿½é—®ï¼Œå¼ºåˆ¶è¦†ç›–ä¸º false (åŸå› : {comprehensive_check['reason']})")
                        result["is_follow_up"] = False
                        result["follow_up_reason"] = None

            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            clear_thinking_status(session_id)
            # ========== æ­¥éª¤5: è§¦å‘é¢„ç”Ÿæˆï¼ˆå¦‚æœéœ€è¦ï¼‰==========
            trigger_prefetch_if_needed(session, dimension)
            return jsonify(result)

        # è§£æå¤±è´¥
        # æ¸…é™¤æ€è€ƒçŠ¶æ€
        clear_thinking_status(session_id)
        return jsonify({
            "error": "AI å“åº”æ ¼å¼é”™è¯¯",
            "detail": "AI è¿”å›çš„å†…å®¹æ— æ³•è§£æä¸ºæœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚è¯·ç‚¹å‡»ã€Œé‡è¯•ã€æŒ‰é’®é‡æ–°ç”Ÿæˆé—®é¢˜ã€‚"
        }), 503

    except Exception as e:
        # æ¸…é™¤æ€è€ƒçŠ¶æ€
        clear_thinking_status(session_id)
        print(f"ç”Ÿæˆé—®é¢˜æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        error_msg = str(e)

        # æ ¹æ®å¼‚å¸¸ç±»å‹æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        if "connection" in error_msg.lower() or "network" in error_msg.lower():
            return jsonify({
                "error": "ç½‘ç»œè¿æ¥å¤±è´¥",
                "detail": "æ— æ³•è¿æ¥åˆ° AI æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            }), 503
        elif "timeout" in error_msg.lower():
            return jsonify({
                "error": "è¯·æ±‚è¶…æ—¶",
                "detail": "AI æœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
            }), 503
        elif "authentication" in error_msg.lower() or "api key" in error_msg.lower():
            return jsonify({
                "error": "API è®¤è¯å¤±è´¥",
                "detail": "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }), 503
        elif "rate limit" in error_msg.lower():
            return jsonify({
                "error": "è¯·æ±‚é¢‘ç‡è¶…é™",
                "detail": "AI æœåŠ¡è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"
            }), 503
        else:
            return jsonify({
                "error": "ç”Ÿæˆé—®é¢˜å¤±è´¥",
                "detail": f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"
            }), 503


def get_fallback_question(session: dict, dimension: str) -> dict:
    """è·å–å¤‡ç”¨é—®é¢˜ï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    fallback_questions = {
        "customer_needs": [
            {"question": "æ‚¨å¸Œæœ›é€šè¿‡è¿™ä¸ªé¡¹ç›®è§£å†³å“ªäº›æ ¸å¿ƒé—®é¢˜ï¼Ÿ", "options": ["æå‡å·¥ä½œæ•ˆç‡", "é™ä½è¿è¥æˆæœ¬", "æ”¹å–„ç”¨æˆ·ä½“éªŒ", "å¢å¼ºæ•°æ®åˆ†æèƒ½åŠ›"], "multi_select": True},
            {"question": "ä¸»è¦çš„ç”¨æˆ·ç¾¤ä½“æœ‰å“ªäº›ï¼Ÿ", "options": ["å†…éƒ¨å‘˜å·¥", "å¤–éƒ¨å®¢æˆ·", "åˆä½œä¼™ä¼´", "ç®¡ç†å±‚"], "multi_select": True},
            {"question": "ç”¨æˆ·æœ€æœŸæœ›è·å¾—çš„æ ¸å¿ƒä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["èŠ‚çœæ—¶é—´", "å‡å°‘é”™è¯¯", "è·å–æ´å¯Ÿ", "æå‡åä½œ"], "multi_select": False},
        ],
        "business_process": [
            {"question": "å½“å‰ä¸šåŠ¡æµç¨‹ä¸­éœ€è¦ä¼˜åŒ–çš„ç¯èŠ‚æœ‰å“ªäº›ï¼Ÿ", "options": ["æ•°æ®å½•å…¥", "å®¡æ‰¹æµç¨‹", "æŠ¥è¡¨ç”Ÿæˆ", "è·¨éƒ¨é—¨åä½œ"], "multi_select": True},
            {"question": "å…³é”®ä¸šåŠ¡æµç¨‹æ¶‰åŠå“ªäº›éƒ¨é—¨ï¼Ÿ", "options": ["é”€å”®éƒ¨é—¨", "æŠ€æœ¯éƒ¨é—¨", "è´¢åŠ¡éƒ¨é—¨", "è¿è¥éƒ¨é—¨"], "multi_select": True},
            {"question": "æµç¨‹ä¸­æœ€å…³é”®çš„å†³ç­–èŠ‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["å®¡æ‰¹èŠ‚ç‚¹", "åˆ†é…èŠ‚ç‚¹", "éªŒæ”¶èŠ‚ç‚¹", "ç»“ç®—èŠ‚ç‚¹"], "multi_select": False},
        ],
        "tech_constraints": [
            {"question": "æœŸæœ›çš„ç³»ç»Ÿéƒ¨ç½²æ–¹å¼æ˜¯ï¼Ÿ", "options": ["å…¬æœ‰äº‘éƒ¨ç½²", "ç§æœ‰äº‘éƒ¨ç½²", "æ··åˆäº‘éƒ¨ç½²", "æœ¬åœ°éƒ¨ç½²"], "multi_select": False},
            {"question": "éœ€è¦ä¸å“ªäº›ç°æœ‰ç³»ç»Ÿé›†æˆï¼Ÿ", "options": ["ERPç³»ç»Ÿ", "CRMç³»ç»Ÿ", "OAåŠå…¬ç³»ç»Ÿ", "è´¢åŠ¡ç³»ç»Ÿ"], "multi_select": True},
            {"question": "å¯¹ç³»ç»Ÿå®‰å…¨æ€§çš„è¦æ±‚æ˜¯ï¼Ÿ", "options": ["ç­‰ä¿äºŒçº§", "ç­‰ä¿ä¸‰çº§", "åŸºç¡€å®‰å…¨å³å¯", "éœ€è¦è¯¦ç»†è¯„ä¼°"], "multi_select": False},
        ],
        "project_constraints": [
            {"question": "é¡¹ç›®çš„é¢„æœŸé¢„ç®—èŒƒå›´æ˜¯ï¼Ÿ", "options": ["10ä¸‡ä»¥å†…", "10-50ä¸‡", "50-100ä¸‡", "100ä¸‡ä»¥ä¸Š"], "multi_select": False},
            {"question": "æœŸæœ›çš„ä¸Šçº¿æ—¶é—´æ˜¯ï¼Ÿ", "options": ["1ä¸ªæœˆå†…", "1-3ä¸ªæœˆ", "3-6ä¸ªæœˆ", "6ä¸ªæœˆä»¥ä¸Š"], "multi_select": False},
            {"question": "é¡¹ç›®å›¢é˜Ÿçš„èµ„æºæƒ…å†µå¦‚ä½•ï¼Ÿ", "options": ["æœ‰ä¸“èŒå›¢é˜Ÿ", "å…¼èŒå‚ä¸", "å®Œå…¨å¤–åŒ…", "éœ€è¦è¯„ä¼°"], "multi_select": False},
        ]
    }

    # è·å–è¯¥ç»´åº¦å·²å›ç­”çš„é—®é¢˜æ•°
    answered = len([log for log in session.get("interview_log", []) if log.get("dimension") == dimension])
    questions = fallback_questions.get(dimension, [])

    if answered < len(questions):
        q = questions[answered]
        return {
            "question": q["question"],
            "options": q["options"],
            "multi_select": q.get("multi_select", False),
            "dimension": dimension,
            "ai_generated": False,
            "is_follow_up": False
        }

    # ç»´åº¦å·²å®Œæˆ
    return {
        "question": None,
        "dimension": dimension,
        "completed": True
    }


@app.route('/api/sessions/<session_id>/submit-answer', methods=['POST'])
def submit_answer(session_id):
    """æäº¤å›ç­”"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))

    # éªŒè¯è¯·æ±‚æ•°æ®
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    question = data.get("question")
    answer = data.get("answer")
    dimension = data.get("dimension")
    options = data.get("options", [])
    is_follow_up = data.get("is_follow_up", False)

    # éªŒè¯å¿…éœ€å‚æ•°
    if not question or not isinstance(question, str):
        return jsonify({"error": "é—®é¢˜ä¸èƒ½ä¸ºç©º"}), 400
    if not answer or not isinstance(answer, str):
        return jsonify({"error": "ç­”æ¡ˆä¸èƒ½ä¸ºç©º"}), 400
    if len(answer) > 5000:
        return jsonify({"error": "ç­”æ¡ˆé•¿åº¦ä¸èƒ½è¶…è¿‡5000å­—ç¬¦"}), 400
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400
    if not isinstance(options, list):
        return jsonify({"error": "é€‰é¡¹å¿…é¡»æ˜¯åˆ—è¡¨"}), 400
    if not isinstance(is_follow_up, bool):
        return jsonify({"error": "is_follow_upå¿…é¡»æ˜¯å¸ƒå°”å€¼"}), 400

    # ä½¿ç”¨å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°åˆ¤æ–­å›ç­”æ˜¯å¦éœ€è¦è¿½é—®
    eval_result = evaluate_answer_depth(
        question=question,
        answer=answer,
        dimension=dimension,
        options=options,
        is_follow_up=is_follow_up
    )
    needs_follow_up = eval_result["needs_follow_up"]
    follow_up_signals = eval_result["signals"]

    if ENABLE_DEBUG_LOG and (needs_follow_up or eval_result["suggest_ai_eval"]):
        print(f"ğŸ“ å›ç­”è¯„ä¼°: signals={follow_up_signals}, needs_follow_up={needs_follow_up}")

    # æ·»åŠ åˆ°è®¿è°ˆè®°å½•
    log_entry = {
        "timestamp": get_utc_now(),
        "question": question,
        "answer": answer,
        "dimension": dimension,
        "options": options,
        "is_follow_up": is_follow_up,
        "needs_follow_up": needs_follow_up,
        "follow_up_signals": follow_up_signals  # è®°å½•æ£€æµ‹åˆ°çš„ä¿¡å·
    }
    session["interview_log"].append(log_entry)

    # æ›´æ–°ç»´åº¦æ•°æ®ï¼ˆåªæœ‰æ­£å¼é—®é¢˜æ‰æ·»åŠ åˆ°ç»´åº¦éœ€æ±‚åˆ—è¡¨ï¼‰
    if dimension and dimension in session["dimensions"] and not is_follow_up:
        session["dimensions"][dimension]["items"].append({
            "name": answer,
            "description": question,
            "priority": "ä¸­"
        })

    # è®¡ç®—è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼Œè¿½é—®ä¸è®¡å…¥ï¼‰
    if dimension and dimension in session["dimensions"]:
        session["dimensions"][dimension]["coverage"] = calculate_dimension_coverage(session, dimension)

    # è¯„ä¼°åœºæ™¯ï¼šä¸ºæ¯æ¬¡å›ç­”è¿›è¡Œ AI è¯„åˆ†
    is_assessment = session.get("scenario_config", {}).get("report", {}).get("type") == "assessment"
    if is_assessment and dimension and dimension in session["dimensions"]:
        score = score_assessment_answer(session, dimension, question, answer)
        if score is not None:
            # è®°å½•æœ¬æ¬¡å›ç­”çš„è¯„åˆ†
            log_entry["score"] = score
            # æ›´æ–°ç»´åº¦çš„ç»¼åˆè¯„åˆ†ï¼ˆå–è¯¥ç»´åº¦æ‰€æœ‰è¯„åˆ†çš„å¹³å‡å€¼ï¼‰
            dim_scores = [
                log.get("score") for log in session["interview_log"]
                if log.get("dimension") == dimension and log.get("score") is not None
            ]
            if dim_scores:
                session["dimensions"][dimension]["score"] = round(sum(dim_scores) / len(dim_scores), 2)
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ“Š è¯„ä¼°è¯„åˆ†: {dimension} = {score}åˆ†ï¼Œç»´åº¦å‡åˆ† = {session['dimensions'][dimension].get('score')}")

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # å¼‚æ­¥æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆè¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
    import threading
    def async_update_summary():
        try:
            update_context_summary(session, session_file)
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æ›´æ–°æ‘˜è¦å¤±è´¥: {e}")
    threading.Thread(target=async_update_summary, daemon=True).start()

    return jsonify(session)


@app.route('/api/sessions/<session_id>/undo-answer', methods=['POST'])
def undo_answer(session_id):
    """æ’¤é”€æœ€åä¸€ä¸ªå›ç­”"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))

    # æ£€æŸ¥æ˜¯å¦æœ‰å›ç­”å¯ä»¥æ’¤é”€
    if not session.get("interview_log") or len(session["interview_log"]) == 0:
        return jsonify({"error": "æ²¡æœ‰å¯æ’¤é”€çš„å›ç­”"}), 400

    # åˆ é™¤æœ€åä¸€ä¸ªå›ç­”
    last_log = session["interview_log"].pop()
    dimension = last_log.get("dimension")
    was_follow_up = last_log.get("is_follow_up", False)

    # æ›´æ–°ç»´åº¦æ•°æ®ï¼ˆåªæœ‰æ­£å¼é—®é¢˜æ‰å½±å“ç»´åº¦ itemsï¼‰
    if dimension and dimension in session["dimensions"]:
        # åªæœ‰åˆ é™¤çš„æ˜¯æ­£å¼é—®é¢˜æ—¶ï¼Œæ‰ä» items ä¸­åˆ é™¤
        if not was_follow_up and session["dimensions"][dimension]["items"]:
            session["dimensions"][dimension]["items"].pop()

        # é‡æ–°è®¡ç®—è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼‰
        session["dimensions"][dimension]["coverage"] = calculate_dimension_coverage(session, dimension)

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify(session)


@app.route('/api/sessions/<session_id>/skip-follow-up', methods=['POST'])
def skip_follow_up(session_id):
    """
    ç”¨æˆ·ä¸»åŠ¨è·³è¿‡å½“å‰é—®é¢˜çš„è¿½é—®
    æ ‡è®°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜çš„å›ç­”ä¸º"ä¸éœ€è¦è¿½é—®"
    """
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))
    data = request.get_json() or {}
    dimension = data.get("dimension")

    # éªŒè¯ dimension
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400

    interview_log = session.get("interview_log", [])
    if not interview_log:
        return jsonify({"error": "æ²¡æœ‰å¯è·³è¿‡çš„é—®é¢˜"}), 400

    # æ‰¾åˆ°å½“å‰ç»´åº¦çš„æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]
    formal_logs = [log for log in dim_logs if not log.get("is_follow_up", False)]

    if not formal_logs:
        return jsonify({"error": "æ²¡æœ‰å¯è·³è¿‡çš„æ­£å¼é—®é¢˜"}), 400

    # æ ‡è®°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜ä¸éœ€è¦è¿½é—®
    last_formal = formal_logs[-1]
    last_formal["needs_follow_up"] = False
    last_formal["user_skip_follow_up"] = True  # æ ‡è®°ä¸ºç”¨æˆ·ä¸»åŠ¨è·³è¿‡

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    if ENABLE_DEBUG_LOG:
        print(f"â­ï¸ ç”¨æˆ·è·³è¿‡è¿½é—®: dimension={dimension}")

    return jsonify({"success": True, "message": "å·²è·³è¿‡è¿½é—®"})


@app.route('/api/sessions/<session_id>/complete-dimension', methods=['POST'])
def complete_dimension(session_id):
    """
    ç”¨æˆ·ä¸»åŠ¨å®Œæˆå½“å‰ç»´åº¦
    å°†å½“å‰ç»´åº¦æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆè¦†ç›–åº¦è®¾ä¸º100%ï¼‰
    """
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))
    data = request.get_json() or {}
    dimension = data.get("dimension")

    # éªŒè¯ç»´åº¦æœ‰æ•ˆæ€§
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400

    # æ£€æŸ¥è¦†ç›–åº¦æ˜¯å¦å·²è¾¾åˆ°è‡³å°‘ 50%
    current_coverage = session["dimensions"][dimension]["coverage"]
    if current_coverage < 50:
        return jsonify({
            "error": "æ— æ³•å®Œæˆç»´åº¦",
            "detail": "å½“å‰ç»´åº¦è¦†ç›–åº¦ä¸è¶³50%ï¼Œå»ºè®®è‡³å°‘å›ç­”ä¸€åŠé—®é¢˜åå†è·³è¿‡"
        }), 400

    # æ ‡è®°æ‰€æœ‰è¯¥ç»´åº¦çš„å›ç­”ä¸ºä¸éœ€è¦è¿½é—®
    for log in session.get("interview_log", []):
        if log.get("dimension") == dimension and not log.get("is_follow_up", False):
            log["needs_follow_up"] = False

    # å°†ç»´åº¦è¦†ç›–åº¦è®¾ä¸º 100%
    session["dimensions"][dimension]["coverage"] = 100
    session["dimensions"][dimension]["user_completed"] = True  # æ ‡è®°ä¸ºç”¨æˆ·ä¸»åŠ¨å®Œæˆ

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    if ENABLE_DEBUG_LOG:
        print(f"â­ï¸ ç”¨æˆ·å®Œæˆç»´åº¦: dimension={dimension}, coverage={current_coverage}%")

    session_dim_info = get_dimension_info_for_session(session)
    dim_name = session_dim_info.get(dimension, {}).get('name', dimension)
    return jsonify({"success": True, "message": f"{dim_name}ç»´åº¦å·²å®Œæˆ"})


# ============ æ–‡æ¡£ä¸Šä¼  API ============

@app.route('/api/sessions/<session_id>/documents', methods=['POST'])
def upload_document(session_id):
    """ä¸Šä¼ å‚è€ƒæ–‡æ¡£"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    if 'file' not in request.files:
        return jsonify({"error": "æœªæ‰¾åˆ°æ–‡ä»¶"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æ–‡ä»¶åä¸ºç©º"}), 400

    # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§10MBï¼‰
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
    file_size = file.tell()
    file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    if file_size > MAX_FILE_SIZE:
        return jsonify({"error": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§{MAX_FILE_SIZE // 1024 // 1024}MBï¼‰"}), 400

    filename = file.filename
    filepath = TEMP_DIR / filename
    file.save(filepath)

    # è¯»å–æ–‡ä»¶å†…å®¹
    ext = Path(filename).suffix.lower()
    content = ""

    try:
        # å›¾ç‰‡å¤„ç†
        if ext in SUPPORTED_IMAGE_TYPES:
            content = describe_image_with_vision(filepath, filename)
        elif ext in ['.md', '.txt']:
            content = filepath.read_text(encoding="utf-8")
            if not content or not content.strip():
                return jsonify({"error": "æ–‡ä»¶å†…å®¹ä¸ºç©º"}), 400
        elif ext in ['.pdf', '.docx', '.xlsx', '.pptx']:
            # è°ƒç”¨è½¬æ¢è„šæœ¬
            import subprocess
            convert_script = SKILL_DIR / "scripts" / "convert_doc.py"
            if convert_script.exists():
                try:
                    result = subprocess.run(
                        ["uv", "run", str(convert_script), "convert", str(filepath)],
                        capture_output=True, text=True, cwd=str(SKILL_DIR)
                    )
                    if result.returncode == 0:
                        converted_file = CONVERTED_DIR / f"{Path(filename).stem}.md"
                        if converted_file.exists():
                            content = converted_file.read_text(encoding="utf-8")
                        else:
                            content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: æœªæ‰¾åˆ°è½¬æ¢åçš„æ–‡ä»¶]"
                    else:
                        error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                        content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: {error_msg}]"
                except Exception as e:
                    print(f"è½¬æ¢æ–‡æ¡£å¤±è´¥: {e}")
                    content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: {str(e)[:200]}]"
            else:
                content = f"[{ext.upper()[1:]} æ–‡ä»¶: {filename}] (è½¬æ¢è„šæœ¬ä¸å­˜åœ¨)"
    except UnicodeDecodeError as e:
        return jsonify({"error": f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}"}), 400
    except Exception as e:
        return jsonify({"error": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"}), 500

    if not content or not content.strip():
        return jsonify({"error": "æ–‡ä»¶è§£æåå†…å®¹ä¸ºç©º"}), 400

    # æ›´æ–°ä¼šè¯
    session = json.loads(session_file.read_text(encoding="utf-8"))
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)
    session["reference_materials"].append({
        "name": filename,
        "type": ext,
        "content": content[:10000],  # é™åˆ¶é•¿åº¦
        "source": "upload",  # ç”¨æˆ·ä¸Šä¼ 
        "uploaded_at": get_utc_now()
    })
    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify({
        "success": True,
        "filename": filename,
        "content_length": len(content)
    })


@app.route('/api/sessions/<session_id>/documents/<path:doc_name>', methods=['DELETE'])
def delete_document(session_id, doc_name):
    """åˆ é™¤å‚è€ƒèµ„æ–™ï¼ˆè½¯åˆ é™¤ï¼‰"""
    # è·¯å¾„éå†é˜²æŠ¤
    if '..' in doc_name or doc_name.startswith('/'):
        return jsonify({"error": "æ— æ•ˆçš„æ–‡æ¡£å"}), 400

    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)

    # æŸ¥æ‰¾å¹¶åˆ é™¤æ–‡æ¡£
    original_count = len(session["reference_materials"])
    session["reference_materials"] = [
        doc for doc in session["reference_materials"]
        if doc["name"] != doc_name
    ]

    if len(session["reference_materials"]) == original_count:
        return jsonify({"error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # è½¯åˆ é™¤ï¼šè®°å½•åˆ°åˆ é™¤æ—¥å¿—ï¼Œæ–‡ä»¶ä¿ç•™åœ¨ temp/converted ç›®å½•
    mark_doc_as_deleted(session_id, doc_name)

    return jsonify({
        "success": True,
        "deleted": doc_name,
        "message": "æ–‡æ¡£å·²ä»åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆæ–‡ä»¶å·²å­˜æ¡£ï¼‰"
    })


# ============ é‡æ–°è®¿è°ˆ API ============

@app.route('/api/sessions/<session_id>/restart-interview', methods=['POST'])
def restart_interview(session_id):
    """é‡æ–°è®¿è°ˆï¼šå°†å½“å‰è®¿è°ˆè®°å½•ä¿å­˜ä¸ºå‚è€ƒèµ„æ–™ï¼Œç„¶åé‡ç½®è®¿è°ˆçŠ¶æ€"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)

    # æ•´ç†å½“å‰è®¿è°ˆè®°å½•ä¸º markdown æ ¼å¼
    interview_log = session.get("interview_log", [])
    if not interview_log:
        return jsonify({"error": "æ²¡æœ‰è®¿è°ˆè®°å½•å¯ä»¥ä¿å­˜"}), 400

    # ç”Ÿæˆè®¿è°ˆè®°å½•æ–‡æ¡£å†…å®¹
    research_content = f"""# è®¿è°ˆè®°å½• - {session.get('topic', 'æœªå‘½åè®¿è°ˆ')}

ç”Ÿæˆæ—¶é—´: {get_utc_now()}

"""

    if session.get("description"):
        # æ¸…ç†æè¿°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        desc = session['description'].replace('\n', ' ').replace('\r', '')
        research_content += f"ä¸»é¢˜æè¿°: {desc}\n\n"

    research_content += "## è®¿è°ˆè®°å½•\n\n"

    # æŒ‰ç»´åº¦æ•´ç†è®¿è°ˆè®°å½•
    restart_dim_info = get_dimension_info_for_session(session)
    for dim_key, dim_info in restart_dim_info.items():
        dim_logs = [log for log in interview_log if log.get("dimension") == dim_key]
        if dim_logs:
            research_content += f"### {dim_info['name']}\n\n"
            for log in dim_logs:
                # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…å½±å“ JSON è§£æ
                question = log.get('question', '').replace('**', '').replace('`', '')
                answer = log.get('answer', '').replace('**', '').replace('`', '')

                research_content += f"Q: {question}\n\n"
                research_content += f"A: {answer}\n\n"

                if log.get('follow_up_question'):
                    follow_q = log['follow_up_question'].replace('**', '').replace('`', '')
                    follow_a = log.get('follow_up_answer', '').replace('**', '').replace('`', '')
                    research_content += f"è¿½é—®: {follow_q}\n\n"
                    research_content += f"å›ç­”: {follow_a}\n\n"
                research_content += "---\n\n"

    # æ·»åŠ åˆ°å‚è€ƒèµ„æ–™åˆ—è¡¨
    doc_name = f"è®¿è°ˆè®°å½•-{get_utc_now().replace(':', '-').replace(' ', '_')}.md"

    # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿å¯¼è‡´ AI prompt é—®é¢˜
    max_length = 2000
    if len(research_content) > max_length:
        research_content = research_content[:max_length] + "\n\n...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"

    session["reference_materials"].append({
        "name": doc_name,
        "type": ".md",
        "content": research_content,
        "source": "auto",  # ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ
        "uploaded_at": get_utc_now()
    })

    # é‡ç½®è®¿è°ˆçŠ¶æ€ - ä»åœºæ™¯é…ç½®åŠ¨æ€åˆ›å»ºç»´åº¦
    session["interview_log"] = []
    reset_dim_info = get_dimension_info_for_session(session)
    session["dimensions"] = {
        dim_key: {"coverage": 0, "items": [], "score": None}
        for dim_key in reset_dim_info
    }
    session["status"] = "in_progress"  # é‡ç½®çŠ¶æ€ä¸ºè¿›è¡Œä¸­
    session["updated_at"] = get_utc_now()

    # ä¿å­˜ä¼šè¯
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify({
        "success": True,
        "message": "å·²ä¿å­˜å½“å‰è®¿è°ˆå†…å®¹å¹¶é‡ç½®è®¿è°ˆ",
        "research_doc_name": doc_name
    })


# ============ æŠ¥å‘Šç”Ÿæˆ API ============

@app.route('/api/sessions/<session_id>/generate-report', methods=['POST'])
def generate_report(session_id):
    """ç”Ÿæˆè®¿è°ˆæŠ¥å‘Šï¼ˆAI ç”Ÿæˆï¼‰"""
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session = json.loads(session_file.read_text(encoding="utf-8"))

    # æ£€æŸ¥æ˜¯å¦æœ‰ Claude API
    if claude_client:
        prompt = build_report_prompt(session)

        # æ—¥å¿—ï¼šè®°å½•æŠ¥å‘Šç”Ÿæˆ prompt ç»Ÿè®¡
        if ENABLE_DEBUG_LOG:
            ref_docs_count = len(session.get("reference_materials", session.get("reference_docs", []) + session.get("research_docs", [])))
            interview_count = len(session.get("interview_log", []))
            print(f"ğŸ“Š æŠ¥å‘Šç”Ÿæˆ Prompt ç»Ÿè®¡ï¼šæ€»é•¿åº¦={len(prompt)}å­—ç¬¦ï¼Œå‚è€ƒèµ„æ–™={ref_docs_count}ä¸ªï¼Œè®¿è°ˆè®°å½•={interview_count}æ¡")

        report_content = call_claude(
            prompt,
            max_tokens=MAX_TOKENS_REPORT,
            call_type="report"
        )

        if report_content:
            # è¿½åŠ å®Œæ•´çš„è®¿è°ˆè®°å½•é™„å½•ï¼ˆç¡®ä¿é™„å½•å®Œæ•´ï¼‰
            appendix = generate_interview_appendix(session)
            report_content = report_content + appendix

            # ä¿å­˜æŠ¥å‘Š
            topic_slug = session.get("topic", "report").replace(" ", "-")[:30]
            date_str = datetime.now().strftime("%Y%m%d")
            filename = f"deep-vision-{date_str}-{topic_slug}.md"
            report_file = REPORTS_DIR / filename
            report_file.write_text(report_content, encoding="utf-8")

            # æ›´æ–°ä¼šè¯çŠ¶æ€
            session["status"] = "completed"
            session["updated_at"] = get_utc_now()
            session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

            return jsonify({
                "success": True,
                "report_path": str(report_file),
                "report_name": filename,
                "ai_generated": True
            })

    # å›é€€åˆ°ç®€å•æŠ¥å‘Šç”Ÿæˆ
    report_content = generate_simple_report(session)
    topic_slug = session.get("topic", "report").replace(" ", "-")[:30]
    date_str = datetime.now().strftime("%Y%m%d")
    filename = f"deep-vision-{date_str}-{topic_slug}.md"
    report_file = REPORTS_DIR / filename
    report_file.write_text(report_content, encoding="utf-8")

    session["status"] = "completed"
    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify({
        "success": True,
        "report_path": str(report_file),
        "report_name": filename,
        "ai_generated": False
    })


def generate_interview_appendix(session: dict) -> str:
    """ç”Ÿæˆå®Œæ•´çš„è®¿è°ˆè®°å½•é™„å½•"""
    interview_log = session.get("interview_log", [])
    if not interview_log:
        return ""

    appendix = "\n\n---\n\n## é™„å½•ï¼šå®Œæ•´è®¿è°ˆè®°å½•\n\n"
    appendix += f"> æœ¬æ¬¡è®¿è°ˆå…±æ”¶é›†äº† {len(interview_log)} ä¸ªé—®é¢˜çš„å›ç­”\n\n"

    appendix_dim_info = get_dimension_info_for_session(session)
    for i, log in enumerate(interview_log, 1):
        dim_name = appendix_dim_info.get(log.get('dimension', ''), {}).get('name', 'æœªåˆ†ç±»')
        appendix += f"### Q{i}: {log['question']}\n\n"
        appendix += f"**å›ç­”**: {log['answer']}\n\n"
        appendix += f"**ç»´åº¦**: {dim_name}\n\n"
        if log.get('timestamp'):
            appendix += f"*è®°å½•æ—¶é—´: {log['timestamp']}*\n\n"
        appendix += "---\n\n"

    return appendix


def generate_simple_report(session: dict) -> str:
    """ç”Ÿæˆç®€å•æŠ¥å‘Šï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    topic = session.get("topic", "æœªå‘½åé¡¹ç›®")
    interview_log = session.get("interview_log", [])
    now = datetime.now()

    content = f"""# {topic} è®¿è°ˆæŠ¥å‘Š

**è®¿è°ˆæ—¥æœŸ**: {now.strftime('%Y-%m-%d')}
**æŠ¥å‘Šç¼–å·**: deep-vision-{now.strftime('%Y%m%d')}

---

## 1. è®¿è°ˆæ¦‚è¿°

æœ¬æ¬¡è®¿è°ˆä¸»é¢˜ä¸ºã€Œ{topic}ã€ï¼Œå…±æ”¶é›†äº† {len(interview_log)} ä¸ªé—®é¢˜çš„å›ç­”ã€‚

## 2. éœ€æ±‚æ‘˜è¦

"""

    simple_dim_info = get_dimension_info_for_session(session)
    for dim_key, dim_info in simple_dim_info.items():
        content += f"### {dim_info['name']}\n\n"
        logs = [log for log in interview_log if log.get("dimension") == dim_key]
        if logs:
            for log in logs:
                content += f"- **{log['answer']}** - {log['question']}\n"
        else:
            content += "*æš‚æ— æ•°æ®*\n"
        content += "\n"

    # ä½¿ç”¨ç»Ÿä¸€çš„é™„å½•ç”Ÿæˆå‡½æ•°ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
    content += generate_interview_appendix(session)

    content += """
*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³-æ™ºèƒ½è®¿è°ˆåŠ©æ‰‹ç”Ÿæˆ*
"""

    return content


# ============ æŠ¥å‘Š API ============

@app.route('/api/reports', methods=['GET'])
def list_reports():
    """è·å–æ‰€æœ‰æŠ¥å‘Šï¼ˆæ’é™¤å·²åˆ é™¤çš„ï¼‰"""
    deleted = get_deleted_reports()
    reports = []
    for f in REPORTS_DIR.glob("*.md"):
        # è·³è¿‡å·²æ ‡è®°ä¸ºåˆ é™¤çš„æŠ¥å‘Š
        if f.name in deleted:
            continue
        stat = f.stat()
        reports.append({
            "name": f.name,
            "path": str(f),
            "size": stat.st_size,
            "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat()
        })

    reports.sort(key=lambda x: x["created_at"], reverse=True)
    return jsonify(reports)


@app.route('/api/reports/<path:filename>', methods=['GET'])
def get_report(filename):
    """è·å–æŠ¥å‘Šå†…å®¹"""
    report_file = REPORTS_DIR / filename
    if not report_file.exists():
        return jsonify({"error": "æŠ¥å‘Šä¸å­˜åœ¨"}), 404

    content = report_file.read_text(encoding="utf-8")
    return jsonify({"name": filename, "content": content})


@app.route('/api/reports/<path:filename>', methods=['DELETE'])
def delete_report(filename):
    """åˆ é™¤æŠ¥å‘Šï¼ˆä»…æ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä¿ç•™æ–‡ä»¶å­˜æ¡£ï¼‰"""
    report_file = REPORTS_DIR / filename
    if not report_file.exists():
        return jsonify({"error": "æŠ¥å‘Šä¸å­˜åœ¨"}), 404

    try:
        # åªæ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä¸çœŸæ­£åˆ é™¤æ–‡ä»¶
        mark_report_as_deleted(filename)
        return jsonify({
            "message": "æŠ¥å‘Šå·²ä»åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆæ–‡ä»¶å·²å­˜æ¡£ï¼‰",
            "name": filename
        })
    except Exception as e:
        return jsonify({"error": f"æ ‡è®°åˆ é™¤å¤±è´¥: {str(e)}"}), 500


# ============ çŠ¶æ€ API ============

@app.route('/api/status', methods=['GET'])
def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return jsonify({
        "status": "running",
        "ai_available": claude_client is not None,
        "model": MODEL_NAME if claude_client else None,
        "sessions_dir": str(SESSIONS_DIR),
        "reports_dir": str(REPORTS_DIR)
    })


@app.route('/api/status/web-search', methods=['GET'])
def get_web_search_status():
    """è·å– Web Search API è°ƒç”¨çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯å‘¼å¸ç¯æ•ˆæœï¼‰"""
    return jsonify({
        "active": web_search_active
    })


@app.route('/api/status/thinking/<session_id>', methods=['GET'])
def get_thinking_status(session_id):
    """è·å– AI æ€è€ƒè¿›åº¦çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯åˆ†é˜¶æ®µè¿›åº¦å±•ç¤ºï¼‰"""
    with thinking_status_lock:
        status = thinking_status.get(session_id)

    if status:
        return jsonify({
            "active": True,
            "stage": status["stage"],
            "stage_index": status["stage_index"],
            "total_stages": status["total_stages"],
            "message": status["message"],
        })
    else:
        return jsonify({"active": False})


@app.route('/api/metrics', methods=['GET'])
def get_metrics():
    """è·å– API æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯"""
    last_n = request.args.get('last_n', type=int)
    stats = metrics_collector.get_statistics(last_n=last_n)
    return jsonify(stats)


@app.route('/api/metrics/reset', methods=['POST'])
def reset_metrics():
    """é‡ç½®æ€§èƒ½æŒ‡æ ‡ï¼ˆæ¸…ç©ºå†å²æ•°æ®ï¼‰"""
    try:
        metrics_collector.metrics_file.write_text(json.dumps({
            "calls": [],
            "summary": {
                "total_calls": 0,
                "total_timeouts": 0,
                "total_truncations": 0,
                "avg_response_time": 0,
                "avg_prompt_length": 0
            }
        }, ensure_ascii=False, indent=2), encoding="utf-8")
        return jsonify({"success": True, "message": "æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®"})
    except Exception as e:
        return jsonify({"error": f"é‡ç½®å¤±è´¥: {e}"}), 500


@app.route('/api/summaries', methods=['GET'])
def get_summaries_info():
    """è·å–æ™ºèƒ½æ‘˜è¦ç¼“å­˜ä¿¡æ¯"""
    try:
        cache_files = list(SUMMARIES_DIR.glob("*.txt"))
        total_size = sum(f.stat().st_size for f in cache_files)

        return jsonify({
            "enabled": ENABLE_SMART_SUMMARY,
            "cache_enabled": SUMMARY_CACHE_ENABLED,
            "threshold": SMART_SUMMARY_THRESHOLD,
            "target_length": SMART_SUMMARY_TARGET,
            "cached_count": len(cache_files),
            "cache_size_bytes": total_size,
            "cache_size_kb": round(total_size / 1024, 2),
            "cache_directory": str(SUMMARIES_DIR)
        })
    except Exception as e:
        return jsonify({"error": f"è·å–æ‘˜è¦ä¿¡æ¯å¤±è´¥: {e}"}), 500


@app.route('/api/summaries/clear', methods=['POST'])
def clear_summaries_cache():
    """æ¸…ç©ºæ™ºèƒ½æ‘˜è¦ç¼“å­˜"""
    try:
        cache_files = list(SUMMARIES_DIR.glob("*.txt"))
        deleted_count = 0
        for f in cache_files:
            try:
                f.unlink()
                deleted_count += 1
            except Exception:
                pass

        return jsonify({
            "success": True,
            "message": f"å·²æ¸…ç©º {deleted_count} ä¸ªæ‘˜è¦ç¼“å­˜",
            "deleted_count": deleted_count
        })
    except Exception as e:
        return jsonify({"error": f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}"}), 500


if __name__ == '__main__':
    print("=" * 60)
    print("Deep Vision Web Server - AI é©±åŠ¨ç‰ˆæœ¬")
    print("=" * 60)
    print(f"Sessions: {SESSIONS_DIR}")
    print(f"Reports: {REPORTS_DIR}")
    print(f"AI çŠ¶æ€: {'å·²å¯ç”¨' if claude_client else 'æœªå¯ç”¨'}")
    if claude_client:
        print(f"æ¨¡å‹: {MODEL_NAME}")

    # æœç´¢åŠŸèƒ½çŠ¶æ€
    search_enabled = ENABLE_WEB_SEARCH and ZHIPU_API_KEY and ZHIPU_API_KEY != "your-zhipu-api-key-here"
    print(f"è”ç½‘æœç´¢: {'âœ… å·²å¯ç”¨ (æ™ºè°±AI MCP)' if search_enabled else 'âš ï¸  æœªå¯ç”¨'}")
    if not search_enabled and ENABLE_WEB_SEARCH:
        print("   æç¤º: é…ç½® ZHIPU_API_KEY ä»¥å¯ç”¨è”ç½‘æœç´¢åŠŸèƒ½")

    print()
    print(f"è®¿é—®: http://localhost:{SERVER_PORT}")
    print("=" * 60)
    app.run(host=SERVER_HOST, port=SERVER_PORT, debug=DEBUG_MODE)
