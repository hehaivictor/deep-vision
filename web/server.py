#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = ["flask", "flask-cors", "anthropic", "requests"]
# ///
"""
Deep Vision Web Server - AI é©±åŠ¨ç‰ˆæœ¬

å®Œæ•´å®ç° deep-vision æŠ€èƒ½çš„æ‰€æœ‰åŠŸèƒ½ï¼š
- åŠ¨æ€ç”Ÿæˆé—®é¢˜å’Œé€‰é¡¹ï¼ˆåŸºäºä¸Šä¸‹æ–‡å’Œè¡Œä¸šçŸ¥è¯†ï¼‰
- æ™ºèƒ½è¿½é—®ï¼ˆè¯†åˆ«è¡¨é¢éœ€æ±‚ï¼ŒæŒ–æ˜æœ¬è´¨ï¼‰
- å†²çªæ£€æµ‹ï¼ˆæ£€æµ‹å›ç­”ä¸å‚è€ƒæ–‡æ¡£çš„å†²çªï¼‰
- çŸ¥è¯†å¢å¼ºï¼ˆä¸“ä¸šé¢†åŸŸä¿¡æ¯èå…¥é€‰é¡¹ï¼‰
- ç”Ÿæˆä¸“ä¸šè®¿è°ˆæŠ¥å‘Š
"""

import base64
import json
import os
import re
import secrets
import sqlite3
import threading
import time as _time
import requests
from functools import wraps
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional
from urllib.parse import quote, unquote, urlparse

from flask import Flask, jsonify, request, send_from_directory, redirect, session
from flask_cors import CORS
from werkzeug.security import check_password_hash, generate_password_hash

# åŠ è½½é…ç½®æ–‡ä»¶
try:
    from config import (
        ANTHROPIC_API_KEY,
        ANTHROPIC_BASE_URL,
        MODEL_NAME,
        MAX_TOKENS_DEFAULT,
        MAX_TOKENS_QUESTION,
        MAX_TOKENS_REPORT,
        SERVER_HOST,
        SERVER_PORT,
        DEBUG_MODE,
        ENABLE_AI,
        ENABLE_DEBUG_LOG,
        ENABLE_WEB_SEARCH,
        ZHIPU_API_KEY,
        ZHIPU_SEARCH_ENGINE,
        SEARCH_MAX_RESULTS,
        SEARCH_TIMEOUT,
        VISION_MODEL_NAME,
        VISION_API_URL,
        ENABLE_VISION,
        MAX_IMAGE_SIZE_MB,
        SUPPORTED_IMAGE_TYPES,
        REFLY_API_URL,
        REFLY_API_KEY,
        REFLY_WORKFLOW_ID,
        REFLY_INPUT_FIELD,
        REFLY_FILES_FIELD,
        REFLY_TIMEOUT,
    )
    print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸  æœªæ‰¾åˆ° config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    print("   è¯·å¤åˆ¶ config.example.py ä¸º config.py å¹¶å¡«å…¥å®é™…é…ç½®")
    # é»˜è®¤é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
    ANTHROPIC_BASE_URL = os.environ.get("ANTHROPIC_BASE_URL", "")
    ZHIPU_API_KEY = os.environ.get("ZHIPU_API_KEY", "")
    MODEL_NAME = os.environ.get("MODEL_NAME", "")
    MAX_TOKENS_DEFAULT = 5000
    MAX_TOKENS_QUESTION = 2000
    MAX_TOKENS_REPORT = 10000
    SERVER_HOST = "0.0.0.0"
    SERVER_PORT = 5001
    DEBUG_MODE = True
    ENABLE_AI = True
    ENABLE_DEBUG_LOG = True
    ENABLE_WEB_SEARCH = False
    ZHIPU_API_KEY = ""
    ZHIPU_SEARCH_ENGINE = "search_pro"
    SEARCH_MAX_RESULTS = 3
    SEARCH_TIMEOUT = 10
    # Vision é»˜è®¤é…ç½®
    VISION_MODEL_NAME = os.environ.get("VISION_MODEL_NAME", "")
    VISION_API_URL = os.environ.get("VISION_API_URL", "")
    ENABLE_VISION = True
    MAX_IMAGE_SIZE_MB = 10
    SUPPORTED_IMAGE_TYPES = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
    REFLY_API_URL = os.environ.get("REFLY_API_URL", "")
    REFLY_API_KEY = os.environ.get("REFLY_API_KEY", "")
    REFLY_WORKFLOW_ID = os.environ.get("REFLY_WORKFLOW_ID", "")
    REFLY_INPUT_FIELD = os.environ.get("REFLY_INPUT_FIELD", "report")
    REFLY_FILES_FIELD = os.environ.get("REFLY_FILES_FIELD", "files")
    REFLY_TIMEOUT = int(os.environ.get("REFLY_TIMEOUT", "30"))

try:
    REFLY_TIMEOUT = int(REFLY_TIMEOUT)
except Exception:
    REFLY_TIMEOUT = 30

if "REFLY_FILES_FIELD" not in globals():
    REFLY_FILES_FIELD = os.environ.get("REFLY_FILES_FIELD", "files")

try:
    REFLY_POLL_TIMEOUT = int(os.environ.get("REFLY_POLL_TIMEOUT", "600"))
except Exception:
    REFLY_POLL_TIMEOUT = 600
try:
    REFLY_POLL_INTERVAL = float(os.environ.get("REFLY_POLL_INTERVAL", "2"))
except Exception:
    REFLY_POLL_INTERVAL = 2.0

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False
    print("è­¦å‘Š: anthropic åº“æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨ AI åŠŸèƒ½")

try:
    import config as runtime_config
except Exception:
    runtime_config = None

# æ¨¡å‹è·¯ç”±é…ç½®ï¼šæ”¯æŒé—®é¢˜/æŠ¥å‘Šåˆ†ç¦»ï¼Œæœªé…ç½®æ—¶å‘åå…¼å®¹ MODEL_NAME
_base_model_name = str(MODEL_NAME or "").strip()
_cfg_question_model = str(getattr(runtime_config, "QUESTION_MODEL_NAME", "")).strip() if runtime_config else ""
if not _cfg_question_model:
    _cfg_question_model = str(os.environ.get("QUESTION_MODEL_NAME", "")).strip()

_cfg_report_model = str(getattr(runtime_config, "REPORT_MODEL_NAME", "")).strip() if runtime_config else ""
if not _cfg_report_model:
    _cfg_report_model = str(os.environ.get("REPORT_MODEL_NAME", "")).strip()

QUESTION_MODEL_NAME = _cfg_question_model or _base_model_name
REPORT_MODEL_NAME = _cfg_report_model or QUESTION_MODEL_NAME

# å…¼å®¹å†å²ä»£ç ä¸­ç›´æ¥å¼•ç”¨ MODEL_NAME çš„ä½ç½®ï¼šé»˜è®¤æŒ‡å‘é—®é¢˜æ¨¡å‹
MODEL_NAME = QUESTION_MODEL_NAME


def resolve_model_name(call_type: str = "", model_name: str = "") -> str:
    """æ ¹æ®è°ƒç”¨ç±»å‹é€‰æ‹©æ¨¡å‹ï¼›æ˜¾å¼ä¼ å…¥ model_name æ—¶ä¼˜å…ˆä½¿ç”¨ã€‚"""
    explicit = str(model_name or "").strip()
    if explicit:
        return explicit

    lowered = (call_type or "").lower()
    if "report" in lowered:
        return REPORT_MODEL_NAME
    return QUESTION_MODEL_NAME


# è®¿è°ˆæ·±åº¦å¢å¼º V2 å·²å‡çº§ä¸ºæ­£å¼ç‰ˆï¼ˆå…¨æ¨¡å¼å›ºå®šå¯ç”¨ï¼‰
DEEP_MODE_SKIP_FOLLOWUP_CONFIRM = bool(getattr(runtime_config, "DEEP_MODE_SKIP_FOLLOWUP_CONFIRM", True)) if runtime_config else True

app = Flask(__name__, static_folder='.')
CORS(app)

# Session é…ç½®
config_secret_key = getattr(runtime_config, "SECRET_KEY", "") if runtime_config else ""
env_secret_key = os.environ.get("DEEPVISION_SECRET_KEY", "") or os.environ.get("SECRET_KEY", "")
if config_secret_key or env_secret_key:
    app_secret_key = config_secret_key or env_secret_key
elif DEBUG_MODE:
    # å¼€å‘æ¨¡å¼ä½¿ç”¨å›ºå®šé»˜è®¤å¯†é’¥ï¼Œé¿å…æ¯æ¬¡é‡å¯åç™»å½•æ€å…¨éƒ¨å¤±æ•ˆ
    app_secret_key = os.environ.get("DEEPVISION_DEV_SECRET_KEY", "deepvision-dev-secret-key")
    print("âš ï¸  æœªé…ç½® SECRET_KEYï¼Œå¼€å‘æ¨¡å¼ä½¿ç”¨å›ºå®šé»˜è®¤ä¼šè¯å¯†é’¥")
else:
    app_secret_key = secrets.token_hex(32)
    print("âš ï¸  æœªé…ç½® SECRET_KEYï¼Œå½“å‰ä½¿ç”¨ä¸´æ—¶ä¼šè¯å¯†é’¥ï¼ˆé‡å¯åç™»å½•æ€ä¼šå¤±æ•ˆï¼‰")

app.config["SECRET_KEY"] = app_secret_key
app.config["SESSION_COOKIE_HTTPONLY"] = True
app.config["SESSION_COOKIE_SAMESITE"] = "Lax"
app.config["SESSION_COOKIE_SECURE"] = not DEBUG_MODE
app.config["PERMANENT_SESSION_LIFETIME"] = timedelta(days=30)

# è·¯å¾„é…ç½®
SKILL_DIR = Path(__file__).parent.parent.resolve()
DATA_DIR = SKILL_DIR / "data"
SESSIONS_DIR = DATA_DIR / "sessions"
REPORTS_DIR = DATA_DIR / "reports"
CONVERTED_DIR = DATA_DIR / "converted"
TEMP_DIR = DATA_DIR / "temp"
METRICS_DIR = DATA_DIR / "metrics"
SUMMARIES_DIR = DATA_DIR / "summaries"  # æ–‡æ¡£æ‘˜è¦ç¼“å­˜ç›®å½•
PRESENTATIONS_DIR = DATA_DIR / "presentations"
AUTH_DIR = DATA_DIR / "auth"
PRESENTATION_MAP_FILE = PRESENTATIONS_DIR / ".presentation_map.json"
PRESENTATION_MAP_LOCK = threading.Lock()
DELETED_REPORTS_FILE = REPORTS_DIR / ".deleted_reports.json"
DELETED_DOCS_FILE = DATA_DIR / ".deleted_docs.json"  # è½¯åˆ é™¤è®°å½•æ–‡ä»¶
REPORT_OWNERS_FILE = REPORTS_DIR / ".owners.json"
REPORT_OWNERS_LOCK = threading.Lock()

auth_db_from_config = getattr(runtime_config, "AUTH_DB_PATH", "") if runtime_config else ""
auth_db_from_env = os.environ.get("DEEPVISION_AUTH_DB_PATH", "")
raw_auth_db_path = auth_db_from_env or auth_db_from_config
if raw_auth_db_path:
    AUTH_DB_PATH = Path(raw_auth_db_path).expanduser()
    if not AUTH_DB_PATH.is_absolute():
        AUTH_DB_PATH = (SKILL_DIR / AUTH_DB_PATH).resolve()
else:
    AUTH_DB_PATH = AUTH_DIR / "users.db"

for d in [SESSIONS_DIR, REPORTS_DIR, CONVERTED_DIR, TEMP_DIR, METRICS_DIR, SUMMARIES_DIR, PRESENTATIONS_DIR, AUTH_DIR]:
    d.mkdir(parents=True, exist_ok=True)

AUTH_EMAIL_PATTERN = re.compile(r"^[^\s@]+@[^\s@]+\.[^\s@]+$")
AUTH_PHONE_PATTERN = re.compile(r"^1\d{10}$")

# ============ åœºæ™¯é…ç½®åŠ è½½å™¨ ============
import sys
sys.path.insert(0, str(SKILL_DIR))
from scripts.scenario_loader import get_scenario_loader
scenario_loader = get_scenario_loader(DATA_DIR / "scenarios")

# Web Search çŠ¶æ€è¿½è¸ªï¼ˆç”¨äºå‰ç«¯å‘¼å¸ç¯æ•ˆæœï¼‰
web_search_active = False

# ============ æ€è€ƒè¿›åº¦çŠ¶æ€è¿½è¸ªï¼ˆæ–¹æ¡ˆBï¼‰============
thinking_status = {}           # { session_id: { stage, stage_index, total_stages, message } }
thinking_status_lock = threading.Lock()

THINKING_STAGES = {
    "analyzing": {"index": 0, "message": "æ­£åœ¨åˆ†ææ‚¨çš„å›ç­”..."},
    "searching": {"index": 1, "message": "æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™..."},
    "generating": {"index": 2, "message": "æ­£åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜..."},
}

# ============ æŠ¥å‘Šç”Ÿæˆè¿›åº¦çŠ¶æ€è¿½è¸ª ============
report_generation_status = {}   # { session_id: { state, stage_index, total_stages, progress, message, updated_at, active } }
report_generation_status_lock = threading.Lock()
report_generation_workers = {}  # { session_id: threading.Thread }
report_generation_workers_lock = threading.Lock()

REPORT_GENERATION_STAGES = {
    "queued": {"index": 0, "progress": 5, "message": "å·²æäº¤è¯·æ±‚ï¼Œå‡†å¤‡ç”ŸæˆæŠ¥å‘Š..."},
    "building_prompt": {"index": 1, "progress": 20, "message": "æ­£åœ¨æ•´ç†è®¿è°ˆä¸èµ„æ–™ä¸Šä¸‹æ–‡..."},
    "generating": {"index": 2, "progress": 65, "message": "æ­£åœ¨è°ƒç”¨ AI ç”ŸæˆæŠ¥å‘Šæ­£æ–‡..."},
    "fallback": {"index": 3, "progress": 78, "message": "AI å“åº”è¾ƒæ…¢ï¼Œæ­£åœ¨åˆ‡æ¢æ¨¡æ¿ç”Ÿæˆ..."},
    "saving": {"index": 4, "progress": 90, "message": "æ­£åœ¨ä¿å­˜æŠ¥å‘Šå¹¶æ›´æ–°ä¼šè¯çŠ¶æ€..."},
    "completed": {"index": 5, "progress": 100, "message": "æŠ¥å‘Šç”Ÿæˆå®Œæˆ"},
    "failed": {"index": 5, "progress": 100, "message": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"},
}

# ============ é¢„ç”Ÿæˆç¼“å­˜ï¼ˆæ™ºèƒ½é¢„ç”Ÿæˆï¼‰============
prefetch_cache = {}            # { session_id: { dimension: { question_data, created_at, valid } } }
prefetch_cache_lock = threading.Lock()
PREFETCH_TTL = 300             # é¢„ç”Ÿæˆç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰


def safe_load_session(session_file: Path) -> dict:
    """å®‰å…¨åŠ è½½ä¼šè¯æ–‡ä»¶ï¼Œå¤„ç† JSON è§£æé”™è¯¯"""
    try:
        return json.loads(session_file.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        print(f"âš ï¸ ä¼šè¯æ–‡ä»¶æŸå: {session_file}, é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âš ï¸ è¯»å–ä¼šè¯æ–‡ä»¶å¤±è´¥: {session_file}, é”™è¯¯: {e}")
        return None


def get_auth_db_connection() -> sqlite3.Connection:
    conn = sqlite3.connect(AUTH_DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn


def init_auth_db() -> None:
    with get_auth_db_connection() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                email TEXT UNIQUE,
                phone TEXT UNIQUE,
                password_hash TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                CHECK (email IS NOT NULL OR phone IS NOT NULL)
            )
            """
        )
        conn.execute("CREATE INDEX IF NOT EXISTS idx_users_email ON users(email)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_users_phone ON users(phone)")
        conn.commit()


def normalize_phone_number(raw_phone: str) -> str:
    normalized = re.sub(r"[\s-]", "", raw_phone or "")
    if normalized.startswith("+86"):
        normalized = normalized[3:]
    elif normalized.startswith("86") and len(normalized) == 13:
        normalized = normalized[2:]
    return normalized


def normalize_account(account: str) -> tuple[Optional[str], Optional[str], str]:
    account_text = str(account or "").strip()
    if not account_text:
        return None, None, "è´¦å·ä¸èƒ½ä¸ºç©º"

    if "@" in account_text:
        email = account_text.lower()
        if not AUTH_EMAIL_PATTERN.match(email):
            return None, None, "è¯·è¾“å…¥æœ‰æ•ˆçš„é‚®ç®±åœ°å€"
        return email, None, ""

    phone = normalize_phone_number(account_text)
    if not AUTH_PHONE_PATTERN.match(phone):
        return None, None, "è¯·è¾“å…¥æœ‰æ•ˆçš„æ‰‹æœºå·ï¼ˆä¸­å›½å¤§é™† 11 ä½ï¼‰"
    return None, phone, ""


def build_user_payload(row: sqlite3.Row) -> dict:
    return {
        "id": int(row["id"]),
        "email": row["email"] or "",
        "phone": row["phone"] or "",
        "account": row["email"] or row["phone"] or "",
        "created_at": row["created_at"]
    }


def query_user_by_id(user_id: int) -> Optional[sqlite3.Row]:
    with get_auth_db_connection() as conn:
        return conn.execute(
            "SELECT id, email, phone, password_hash, created_at, updated_at FROM users WHERE id = ? LIMIT 1",
            (user_id,)
        ).fetchone()


def query_user_by_account(email: Optional[str], phone: Optional[str]) -> Optional[sqlite3.Row]:
    with get_auth_db_connection() as conn:
        if email:
            return conn.execute(
                "SELECT id, email, phone, password_hash, created_at, updated_at FROM users WHERE email = ? LIMIT 1",
                (email,)
            ).fetchone()
        if phone:
            return conn.execute(
                "SELECT id, email, phone, password_hash, created_at, updated_at FROM users WHERE phone = ? LIMIT 1",
                (phone,)
            ).fetchone()
    return None


def get_current_user() -> Optional[sqlite3.Row]:
    user_id = session.get("user_id")
    if not user_id:
        return None

    try:
        user_id_int = int(user_id)
    except (TypeError, ValueError):
        session.clear()
        return None

    user_row = query_user_by_id(user_id_int)
    if not user_row:
        session.clear()
        return None

    return user_row


def login_user(user_row: sqlite3.Row) -> None:
    session.clear()
    session.permanent = True
    session["user_id"] = int(user_row["id"])


def require_login(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        user = get_current_user()
        if not user:
            return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401
        return func(*args, **kwargs)

    return wrapper


@app.before_request
def enforce_auth_for_protected_routes():
    if request.method == "OPTIONS":
        return None

    path = request.path or ""
    if path.startswith("/api/sessions") or path.startswith("/api/reports"):
        if not get_current_user():
            return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    return None


init_auth_db()


def load_presentation_map() -> dict:
    if not PRESENTATION_MAP_FILE.exists():
        return {}
    try:
        payload = json.loads(PRESENTATION_MAP_FILE.read_text(encoding="utf-8"))
        if not isinstance(payload, dict):
            return {}
        normalized = {}
        mutated = False
        for raw_name, record in payload.items():
            name = normalize_presentation_report_filename(raw_name)
            if not name:
                mutated = True
                continue
            if not isinstance(record, dict):
                mutated = True
                continue
            if name in normalized:
                previous = normalized.get(name, {})
                previous_ts = datetime.fromisoformat(previous.get("created_at", "1970-01-01T00:00:00")) if previous.get("created_at") else datetime.min
                current_ts = datetime.fromisoformat(record.get("created_at", "1970-01-01T00:00:00")) if record.get("created_at") else datetime.min
                if current_ts >= previous_ts:
                    normalized[name] = record
                mutated = True
            else:
                normalized[name] = record
            if name != raw_name:
                mutated = True
        if mutated:
            save_presentation_map(normalized)
        return normalized
    except Exception:
        return {}


def save_presentation_map(data: dict) -> None:
    try:
        PRESENTATION_MAP_FILE.write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
    except Exception:
        pass


def normalize_presentation_report_filename(report_filename: Optional[str]) -> str:
    if not isinstance(report_filename, str):
        return ""
    cleaned = report_filename.strip()
    if not cleaned:
        return ""
    if cleaned.lower() in {"null", "undefined", "none"}:
        return ""
    return cleaned


def find_execution_owner_in_map(data: dict, execution_id: str) -> str:
    if not execution_id or not isinstance(data, dict):
        return ""
    for raw_name, record in data.items():
        if not isinstance(record, dict):
            continue
        if record.get("execution_id") != execution_id:
            continue
        normalized_name = normalize_presentation_report_filename(raw_name)
        if normalized_name:
            return normalized_name
    return ""


def get_execution_owner_report(execution_id: str) -> str:
    if not execution_id:
        return ""
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
    return find_execution_owner_in_map(data, execution_id)


def record_presentation_file(report_filename: str, download_info: Optional[dict] = None, pdf_url: Optional[str] = None) -> Optional[dict]:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return None
    record = {
        "created_at": datetime.now().isoformat()
    }
    if download_info and download_info.get("path"):
        record.update({
            "path": download_info.get("path"),
            "filename": download_info.get("filename")
        })
    if pdf_url:
        record["pdf_url"] = pdf_url
    if "path" not in record and "pdf_url" not in record:
        return None
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
        data[report_filename] = record
        save_presentation_map(data)
    return record


def record_presentation_execution(report_filename: str, execution_id: str) -> None:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not execution_id or not report_filename:
        return
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
        owner = find_execution_owner_in_map(data, execution_id)
        if owner and owner != report_filename:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸ å¿½ç•¥è·¨æŠ¥å‘Š execution_id ç»‘å®š: {execution_id} å·²å±äº {owner}, å½“å‰={report_filename}")
            return
        record = data.get(report_filename) if isinstance(data.get(report_filename), dict) else {}
        record = record or {}
        stopped_at = record.get("stopped_at")
        stopped_execution_id = str(record.get("stopped_execution_id") or "").strip()
        if stopped_at and (not stopped_execution_id or stopped_execution_id == execution_id):
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸ å¿½ç•¥å·²åœæ­¢ä»»åŠ¡çš„ execution_id å›å†™: execution_id={execution_id}, report={report_filename}")
            return
        record["execution_id"] = execution_id
        record.pop("stopped_at", None)
        record.pop("stopped_execution_id", None)
        if "created_at" not in record:
            record["created_at"] = datetime.now().isoformat()
        data[report_filename] = record
        save_presentation_map(data)


def clear_presentation_execution(report_filename: str) -> None:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
        record = data.get(report_filename) if isinstance(data.get(report_filename), dict) else {}
        record = record or {}
        if "execution_id" not in record:
            return
        record.pop("execution_id", None)
        data[report_filename] = record
        save_presentation_map(data)


def mark_presentation_stopped(report_filename: str, execution_id: str = "") -> None:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
        record = data.get(report_filename) if isinstance(data.get(report_filename), dict) else {}
        record = record or {}
        stopped_execution_id = str(execution_id or "").strip()
        if not stopped_execution_id:
            stopped_execution_id = str(record.get("execution_id") or "").strip()
        record.pop("execution_id", None)
        if stopped_execution_id:
            record["stopped_execution_id"] = stopped_execution_id
        else:
            record.pop("stopped_execution_id", None)
        record["stopped_at"] = datetime.now().isoformat()
        if "created_at" not in record:
            record["created_at"] = datetime.now().isoformat()
        data[report_filename] = record
        save_presentation_map(data)


def clear_presentation_stopped(report_filename: str) -> None:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
        record = data.get(report_filename) if isinstance(data.get(report_filename), dict) else {}
        record = record or {}
        if "stopped_at" in record or "stopped_execution_id" in record:
            record.pop("stopped_at", None)
            record.pop("stopped_execution_id", None)
            data[report_filename] = record
            save_presentation_map(data)


def get_presentation_record(report_filename: str) -> Optional[dict]:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return None
    with PRESENTATION_MAP_LOCK:
        data = load_presentation_map()
    record = data.get(report_filename)
    return record if isinstance(record, dict) else None


def is_presentation_execution_stopped(report_filename: str, execution_id: str = "") -> bool:
    report_filename = normalize_presentation_report_filename(report_filename)
    if not report_filename:
        return False
    record = get_presentation_record(report_filename) or {}
    stopped_at = record.get("stopped_at")
    if not stopped_at:
        return False
    stopped_execution_id = str(record.get("stopped_execution_id") or "").strip()
    current_execution_id = str(execution_id or "").strip()
    if stopped_execution_id and current_execution_id and stopped_execution_id != current_execution_id:
        return False
    return True


def is_path_under(path: Path, directory: Path) -> bool:
    try:
        resolved_path = path.resolve()
        resolved_dir = directory.resolve()
        return resolved_path == resolved_dir or resolved_dir in resolved_path.parents
    except Exception:
        return False


def update_thinking_status(session_id: str, stage: str, has_search: bool = True):
    """æ›´æ–°æ€è€ƒè¿›åº¦çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    stage_info = THINKING_STAGES.get(stage)
    if not stage_info:
        return

    # å§‹ç»ˆä½¿ç”¨åŸå§‹çš„ stage_indexï¼Œç¡®ä¿ index å’Œ message ä¸€è‡´
    # - æœ‰æœç´¢æ—¶ï¼šåˆ†æ(0) -> æ£€ç´¢(1) -> ç”Ÿæˆ(2)
    # - æ— æœç´¢æ—¶ï¼šåˆ†æ(0) -> ç”Ÿæˆ(2)ï¼Œæ£€ç´¢é˜¶æ®µè¢«è·³è¿‡
    index = stage_info["index"]

    with thinking_status_lock:
        thinking_status[session_id] = {
            "stage": stage,
            "stage_index": index,
            "total_stages": 3,  # æ€»æ˜¯3ä¸ªé˜¶æ®µï¼Œæ— æœç´¢æ—¶æ£€ç´¢ä¼šè¢«å¿«é€Ÿè·³è¿‡
            "message": stage_info["message"],
        }


def clear_thinking_status(session_id: str):
    """æ¸…é™¤æ€è€ƒè¿›åº¦çŠ¶æ€"""
    with thinking_status_lock:
        thinking_status.pop(session_id, None)


def update_report_generation_status(session_id: str, stage: str, message: Optional[str] = None, active: bool = True):
    """æ›´æ–°æŠ¥å‘Šç”Ÿæˆè¿›åº¦çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    stage_info = REPORT_GENERATION_STAGES.get(stage)
    if not stage_info:
        return

    with report_generation_status_lock:
        existing = report_generation_status.get(session_id)
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update({
            "active": active,
            "state": stage,
            "stage_index": stage_info["index"],
            "total_stages": 6,
            "progress": stage_info["progress"],
            "message": message or stage_info["message"],
            "updated_at": datetime.now(timezone.utc).isoformat(),
        })
        report_generation_status[session_id] = merged


def set_report_generation_metadata(session_id: str, updates: Optional[dict] = None):
    """å†™å…¥æŠ¥å‘Šç”Ÿæˆçš„æ‰©å±•å…ƒä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰ã€‚"""
    if not session_id or not isinstance(updates, dict):
        return

    with report_generation_status_lock:
        existing = report_generation_status.get(session_id)
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update(updates)
        merged["updated_at"] = datetime.now(timezone.utc).isoformat()
        report_generation_status[session_id] = merged


def get_report_generation_record(session_id: str) -> Optional[dict]:
    if not session_id:
        return None
    with report_generation_status_lock:
        status = report_generation_status.get(session_id)
        if not isinstance(status, dict):
            return None
        return dict(status)


def build_report_generation_payload(record: Optional[dict]) -> dict:
    if not isinstance(record, dict):
        return {"active": False}

    def _safe_int(value, default):
        try:
            return int(value)
        except Exception:
            return default

    payload = {
        "active": bool(record.get("active", False)),
        "processing": bool(record.get("active", False)),
        "state": record.get("state", "queued"),
        "stage_index": _safe_int(record.get("stage_index", 0), 0),
        "total_stages": _safe_int(record.get("total_stages", 6), 6),
        "progress": _safe_int(record.get("progress", 0), 0),
        "message": record.get("message", "æ­£åœ¨ç”ŸæˆæŠ¥å‘Š..."),
        "updated_at": record.get("updated_at"),
        "request_id": record.get("request_id", ""),
        "action": record.get("action", "generate"),
        "started_at": record.get("started_at", ""),
        "completed_at": record.get("completed_at", ""),
        "report_name": record.get("report_name", ""),
        "report_path": record.get("report_path", ""),
        "ai_generated": record.get("ai_generated"),
        "v3_enabled": record.get("v3_enabled"),
        "error": record.get("error", ""),
    }

    quality_meta = record.get("report_quality_meta")
    if isinstance(quality_meta, dict):
        payload["report_quality_meta"] = quality_meta

    return payload


def is_report_generation_worker_alive(session_id: str) -> bool:
    if not session_id:
        return False
    with report_generation_workers_lock:
        worker = report_generation_workers.get(session_id)
        if worker and worker.is_alive():
            return True
        if worker and not worker.is_alive():
            report_generation_workers.pop(session_id, None)
    return False


def register_report_generation_worker(session_id: str, worker: threading.Thread) -> None:
    if not session_id or worker is None:
        return
    with report_generation_workers_lock:
        report_generation_workers[session_id] = worker


def cleanup_report_generation_worker(session_id: str, worker: Optional[threading.Thread] = None) -> None:
    if not session_id:
        return
    with report_generation_workers_lock:
        current = report_generation_workers.get(session_id)
        if worker is not None and current is not worker:
            return
        report_generation_workers.pop(session_id, None)


def clear_report_generation_status(session_id: str):
    """æ¸…é™¤æŠ¥å‘Šç”Ÿæˆè¿›åº¦çŠ¶æ€"""
    with report_generation_status_lock:
        report_generation_status.pop(session_id, None)
    cleanup_report_generation_worker(session_id)


# ============ é¢„ç”Ÿæˆç¼“å­˜å‡½æ•° ============

def get_prefetch_result(session_id: str, dimension: str) -> Optional[dict]:
    """è·å–é¢„ç”Ÿæˆç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰ï¼Œå‘½ä¸­åˆ™æ¶ˆè´¹ï¼ˆåˆ é™¤ç¼“å­˜ï¼‰

    Args:
        session_id: ä¼šè¯ID
        dimension: ç»´åº¦åç§°

    Returns:
        å‘½ä¸­åˆ™è¿”å›é—®é¢˜æ•°æ®dictï¼Œå¦åˆ™è¿”å›None
    """
    with prefetch_cache_lock:
        session_cache = prefetch_cache.get(session_id, {})
        cached = session_cache.get(dimension)
        if cached and cached.get("valid"):
            # æ£€æŸ¥TTL
            if _time.time() - cached["created_at"] < PREFETCH_TTL:
                # æ¶ˆè´¹ç¼“å­˜ï¼ˆåˆ é™¤ï¼‰
                session_cache.pop(dimension, None)
                if ENABLE_DEBUG_LOG:
                    print(f"ğŸš€ é¢„ç”Ÿæˆç¼“å­˜å‘½ä¸­: session={session_id}, dim={dimension}")
                return cached["question_data"]
            else:
                # è¿‡æœŸï¼Œæ¸…é™¤
                session_cache.pop(dimension, None)
                if ENABLE_DEBUG_LOG:
                    print(f"â° é¢„ç”Ÿæˆç¼“å­˜è¿‡æœŸ: session={session_id}, dim={dimension}")
    return None


def invalidate_prefetch(session_id: str, dimension: str = None):
    """ä½¿é¢„ç”Ÿæˆç¼“å­˜å¤±æ•ˆ

    Args:
        session_id: ä¼šè¯ID
        dimension: ç»´åº¦åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ•´ä¸ªä¼šè¯çš„ç¼“å­˜
    """
    with prefetch_cache_lock:
        if dimension:
            prefetch_cache.get(session_id, {}).pop(dimension, None)
        else:
            prefetch_cache.pop(session_id, None)


def trigger_prefetch_if_needed(session: dict, current_dimension: str):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦é¢„ç”Ÿæˆä¸‹ä¸€ç»´åº¦é¦–é¢˜ï¼Œå¦‚æœéœ€è¦åˆ™å¯åŠ¨åå°çº¿ç¨‹

    é¢„ç”Ÿæˆè§¦å‘æ¡ä»¶ï¼šå½“å‰ç»´åº¦æ­£å¼é—®é¢˜æ•° >= 2

    Args:
        session: ä¼šè¯æ•°æ®
        current_dimension: å½“å‰ç»´åº¦
    """
    session_id = session.get("session_id")
    interview_log = session.get("interview_log", [])

    # è®¡ç®—å½“å‰ç»´åº¦çš„æ­£å¼é—®é¢˜æ•°
    dim_logs = [l for l in interview_log if l.get("dimension") == current_dimension]
    formal_count = len([l for l in dim_logs if not l.get("is_follow_up", False)])

    current_mode_config = get_interview_mode_config(session)
    current_min_formal = current_mode_config.get("formal_questions_per_dim", 2)

    # å½“å‰ç»´åº¦è¾¾åˆ°æœ€ä½æ­£å¼é¢˜åï¼Œé¢„ç”Ÿæˆä¸‹ä¸€ç»´åº¦é¦–é¢˜
    if formal_count < current_min_formal:
        return

    # ç»´åº¦é¡ºåº
    dimension_order = ['customer_needs', 'business_process', 'tech_constraints', 'project_constraints']
    current_idx = dimension_order.index(current_dimension) if current_dimension in dimension_order else -1

    # æ‰¾ä¸‹ä¸€ä¸ªæœªå®Œæˆçš„ç»´åº¦
    next_dimension = None
    for i in range(1, len(dimension_order)):
        candidate = dimension_order[(current_idx + i) % len(dimension_order)]
        cand_logs = [l for l in interview_log if l.get("dimension") == candidate]
        cand_formal = len([l for l in cand_logs if not l.get("is_follow_up", False)])

        # å…¼å®¹åŠ¨æ€åœºæ™¯ï¼šæ„é€ è½»é‡ session ä»¥è¯»å–è¯¥ä¼šè¯æ¨¡å¼é…ç½®
        candidate_min_formal = get_interview_mode_config(session).get("formal_questions_per_dim", 3)
        if cand_formal < candidate_min_formal:
            next_dimension = candidate
            break

    if not next_dimension:
        return

    # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰
    with prefetch_cache_lock:
        existing = prefetch_cache.get(session_id, {}).get(next_dimension)
        if existing and existing.get("valid"):
            return  # å·²æœ‰æœ‰æ•ˆç¼“å­˜ï¼Œä¸é‡å¤ç”Ÿæˆ

    # å¯åŠ¨åå°é¢„ç”Ÿæˆçº¿ç¨‹
    def do_prefetch():
        try:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ”® å¼€å§‹é¢„ç”Ÿæˆ: session={session_id}, next_dim={next_dimension}")

            # é‡æ–°è¯»å–ä¼šè¯æ•°æ®ï¼ˆå¯èƒ½å·²æ›´æ–°ï¼‰
            session_file = SESSIONS_DIR / f"{session_id}.json"
            if not session_file.exists():
                return

            session_data = json.loads(session_file.read_text(encoding="utf-8"))
            next_dim_logs = [l for l in session_data.get("interview_log", [])
                           if l.get("dimension") == next_dimension]

            # æ„å»ºé¢„ç”Ÿæˆçš„ prompt
            prompt, truncated_docs, _decision_meta = build_interview_prompt(
                session_data, next_dimension, next_dim_logs
            )

            # è°ƒç”¨ Claude API
            response = call_claude(
                prompt,
                max_tokens=MAX_TOKENS_QUESTION,
                call_type="prefetch",
                truncated_docs=truncated_docs
            )

            if response:
                # è§£æå“åº”
                result = parse_question_response(response, debug=False)
                if result:
                    result["dimension"] = next_dimension
                    result["ai_generated"] = True

                    with prefetch_cache_lock:
                        if session_id not in prefetch_cache:
                            prefetch_cache[session_id] = {}
                        prefetch_cache[session_id][next_dimension] = {
                            "question_data": result,
                            "created_at": _time.time(),
                            "topic": session_data.get("topic"),
                            "valid": True,
                        }
                    if ENABLE_DEBUG_LOG:
                        print(f"âœ… é¢„ç”Ÿæˆå®Œæˆ: session={session_id}, dim={next_dimension}")
                else:
                    if ENABLE_DEBUG_LOG:
                        print(f"âš ï¸ é¢„ç”Ÿæˆè§£æå¤±è´¥: session={session_id}, dim={next_dimension}")
        except Exception as e:
            print(f"âš ï¸ é¢„ç”Ÿæˆå¤±è´¥: {e}")

    threading.Thread(target=do_prefetch, daemon=True).start()


def prefetch_first_question(session_id: str):
    """åå°é¢„ç”Ÿæˆä¼šè¯çš„ç¬¬ä¸€ä¸ªé—®é¢˜

    åœ¨ä¼šè¯åˆ›å»ºåè°ƒç”¨ï¼Œå¼‚æ­¥ç”Ÿæˆ customer_needs ç»´åº¦çš„é¦–é¢˜ã€‚

    Args:
        session_id: ä¼šè¯ID
    """
    def do_prefetch():
        try:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ”® å¼€å§‹é¢„ç”Ÿæˆé¦–é¢˜: session={session_id}")

            session_file = SESSIONS_DIR / f"{session_id}.json"
            if not session_file.exists():
                return

            session_data = json.loads(session_file.read_text(encoding="utf-8"))

            # è·å–ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆåŠ¨æ€åœºæ™¯æ”¯æŒï¼‰
            first_dim = get_dimension_order_for_session(session_data)[0] if get_dimension_order_for_session(session_data) else "customer_needs"

            # é¦–é¢˜ä¸ä¾èµ–ä»»ä½•å†å²è®°å½•
            prompt, truncated_docs, _decision_meta = build_interview_prompt(
                session_data, first_dim, []
            )

            response = call_claude(
                prompt,
                max_tokens=MAX_TOKENS_QUESTION,
                call_type="prefetch_first",
                truncated_docs=truncated_docs
            )

            if response:
                result = parse_question_response(response, debug=False)
                if result:
                    result["dimension"] = first_dim
                    result["ai_generated"] = True

                    with prefetch_cache_lock:
                        if session_id not in prefetch_cache:
                            prefetch_cache[session_id] = {}
                        prefetch_cache[session_id][first_dim] = {
                            "question_data": result,
                            "created_at": _time.time(),
                            "topic": session_data.get("topic"),
                            "valid": True,
                        }
                    if ENABLE_DEBUG_LOG:
                        print(f"âœ… é¦–é¢˜é¢„ç”Ÿæˆå®Œæˆ: session={session_id}")
        except Exception as e:
            print(f"âš ï¸ é¦–é¢˜é¢„ç”Ÿæˆå¤±è´¥: {e}")

    threading.Thread(target=do_prefetch, daemon=True).start()


# ============ æ€§èƒ½ç›‘æ§ç³»ç»Ÿ ============

class MetricsCollector:
    """API æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, metrics_file: Path):
        self.metrics_file = metrics_file
        self._ensure_metrics_file()

    def _ensure_metrics_file(self):
        """ç¡®ä¿æŒ‡æ ‡æ–‡ä»¶å­˜åœ¨"""
        if not self.metrics_file.exists():
            self.metrics_file.write_text(json.dumps({
                "calls": [],
                "summary": {
                    "total_calls": 0,
                    "total_timeouts": 0,
                    "total_truncations": 0,
                    "avg_response_time": 0,
                    "avg_prompt_length": 0
                }
            }, ensure_ascii=False, indent=2), encoding="utf-8")

    def record_api_call(self, call_type: str, prompt_length: int, response_time: float,
                       success: bool, timeout: bool = False, error_msg: str = None,
                       truncated_docs: list = None, max_tokens: int = None):
        """è®°å½• API è°ƒç”¨æŒ‡æ ‡"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            data = json.loads(self.metrics_file.read_text(encoding="utf-8"))

            # æ·»åŠ æ–°è®°å½•
            call_record = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "type": call_type,  # "question" or "report"
                "prompt_length": prompt_length,
                "response_time_ms": round(response_time * 1000, 2),
                "max_tokens": max_tokens,
                "success": success,
                "timeout": timeout,
                "error": error_msg,
                "truncated_docs": truncated_docs or []
            }

            data["calls"].append(call_record)

            # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
            summary = data["summary"]
            summary["total_calls"] = summary.get("total_calls", 0) + 1
            if timeout:
                summary["total_timeouts"] = summary.get("total_timeouts", 0) + 1
            if truncated_docs:
                summary["total_truncations"] = summary.get("total_truncations", 0) + len(truncated_docs)

            # è®¡ç®—å¹³å‡å€¼
            all_calls = data["calls"]
            if all_calls:
                summary["avg_response_time"] = round(
                    sum(c["response_time_ms"] for c in all_calls) / len(all_calls), 2
                )
                summary["avg_prompt_length"] = round(
                    sum(c["prompt_length"] for c in all_calls) / len(all_calls), 2
                )

            # ä¿å­˜ï¼ˆåªä¿ç•™æœ€è¿‘ 1000 æ¡è®°å½•ï¼‰
            if len(data["calls"]) > 1000:
                data["calls"] = data["calls"][-1000:]

            self.metrics_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )

        except Exception as e:
            print(f"âš ï¸  è®°å½•æŒ‡æ ‡å¤±è´¥: {e}")

    def get_statistics(self, last_n: int = None) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            data = json.loads(self.metrics_file.read_text(encoding="utf-8"))
            calls = data["calls"]

            if last_n:
                calls = calls[-last_n:]

            if not calls:
                return {
                    "total_calls": 0,
                    "message": "æš‚æ— æ•°æ®"
                }

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_calls = len(calls)
            successful_calls = sum(1 for c in calls if c["success"])
            timeout_calls = sum(1 for c in calls if c.get("timeout", False))
            truncation_events = sum(len(c.get("truncated_docs", [])) for c in calls)

            response_times = [c["response_time_ms"] for c in calls if c["success"]]
            prompt_lengths = [c["prompt_length"] for c in calls]

            stats = {
                "period": f"æœ€è¿‘ {last_n} æ¬¡è°ƒç”¨" if last_n else "å…¨éƒ¨è°ƒç”¨",
                "total_calls": total_calls,
                "successful_calls": successful_calls,
                "failed_calls": total_calls - successful_calls,
                "timeout_calls": timeout_calls,
                "timeout_rate": round(timeout_calls / total_calls * 100, 2) if total_calls > 0 else 0,
                "truncation_events": truncation_events,
                "truncation_rate": round(truncation_events / total_calls * 100, 2) if total_calls > 0 else 0,
                "avg_response_time_ms": round(sum(response_times) / len(response_times), 2) if response_times else 0,
                "max_response_time_ms": round(max(response_times), 2) if response_times else 0,
                "min_response_time_ms": round(min(response_times), 2) if response_times else 0,
                "avg_prompt_length": round(sum(prompt_lengths) / len(prompt_lengths), 2) if prompt_lengths else 0,
                "max_prompt_length": max(prompt_lengths) if prompt_lengths else 0,
            }

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            stats["recommendations"] = self._generate_recommendations(stats)

            return stats

        except Exception as e:
            return {"error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}"}

    def _generate_recommendations(self, stats: dict) -> list:
        """åŸºäºç»Ÿè®¡æ•°æ®ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # è¶…æ—¶ç‡è¿‡é«˜
        if stats["timeout_rate"] > 10:
            recommendations.append({
                "level": "critical",
                "message": f"è¶…æ—¶ç‡è¿‡é«˜ ({stats['timeout_rate']}%)ï¼Œå»ºè®®å‡å°‘æ–‡æ¡£é•¿åº¦é™åˆ¶æˆ–å®æ–½æ™ºèƒ½æ‘˜è¦"
            })
        elif stats["timeout_rate"] > 5:
            recommendations.append({
                "level": "warning",
                "message": f"è¶…æ—¶ç‡åé«˜ ({stats['timeout_rate']}%)ï¼Œéœ€è¦å…³æ³¨"
            })

        # æˆªæ–­ç‡è¿‡é«˜
        if stats["truncation_rate"] > 50:
            recommendations.append({
                "level": "warning",
                "message": f"æ–‡æ¡£æˆªæ–­é¢‘ç¹ ({stats['truncation_rate']}%)ï¼Œå»ºè®®å®æ–½æ™ºèƒ½æ‘˜è¦åŠŸèƒ½"
            })

        # Prompt è¿‡é•¿
        if stats["avg_prompt_length"] > 8000:
            recommendations.append({
                "level": "warning",
                "message": f"å¹³å‡ Prompt é•¿åº¦è¾ƒå¤§ ({stats['avg_prompt_length']} å­—ç¬¦)ï¼Œå¯èƒ½å½±å“å“åº”é€Ÿåº¦"
            })

        # å“åº”æ—¶é—´è¿‡é•¿
        if stats["avg_response_time_ms"] > 60000:
            recommendations.append({
                "level": "warning",
                "message": f"å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ ({stats['avg_response_time_ms']/1000:.1f} ç§’)ï¼Œå»ºè®®ä¼˜åŒ– Prompt é•¿åº¦"
            })

        # ä¸€åˆ‡æ­£å¸¸
        if not recommendations:
            if stats["timeout_rate"] < 5 and stats["truncation_rate"] < 30:
                recommendations.append({
                    "level": "info",
                    "message": "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œå¯è€ƒè™‘é€‚åº¦å¢åŠ æ–‡æ¡£é•¿åº¦é™åˆ¶ä»¥æå‡è´¨é‡"
                })

        return recommendations


# åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = MetricsCollector(METRICS_DIR / "api_metrics.json")

# Claude å®¢æˆ·ç«¯åˆå§‹åŒ–
claude_client = None

# æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆ
def is_valid_api_key(api_key: str) -> bool:
    """æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯é»˜è®¤å ä½ç¬¦ï¼‰"""
    if not api_key:
        return False
    placeholder_patterns = [
        "your-", "your_", "example", "test", "placeholder",
        "api-key", "apikey", "YOUR-", "YOUR_"
    ]
    api_key_lower = api_key.lower()
    for pattern in placeholder_patterns:
        if pattern in api_key_lower:
            return False
    return True

# æ£€æŸ¥é…ç½®
api_key_valid = is_valid_api_key(ANTHROPIC_API_KEY)
base_url_valid = ANTHROPIC_BASE_URL and ANTHROPIC_BASE_URL != "https://api.anthropic.com" or api_key_valid

if not api_key_valid:
    print("âš ï¸  ANTHROPIC_API_KEY æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
    print("   è¯·åœ¨ config.py ä¸­å¡«å…¥æœ‰æ•ˆçš„ API Key")
    ENABLE_AI = False

if not base_url_valid and not ANTHROPIC_BASE_URL:
    print("âš ï¸  ANTHROPIC_BASE_URL æœªé…ç½®")
    print("   è¯·åœ¨ config.py ä¸­å¡«å…¥æœ‰æ•ˆçš„ Base URL")

if ENABLE_AI and HAS_ANTHROPIC and api_key_valid:
    try:
        claude_client = anthropic.Anthropic(
            api_key=ANTHROPIC_API_KEY,
            base_url=ANTHROPIC_BASE_URL
        )
        print(f"âœ… Claude å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        if QUESTION_MODEL_NAME == REPORT_MODEL_NAME:
            print(f"   æ¨¡å‹: {QUESTION_MODEL_NAME}")
        else:
            print(f"   é—®é¢˜æ¨¡å‹: {QUESTION_MODEL_NAME}")
            print(f"   æŠ¥å‘Šæ¨¡å‹: {REPORT_MODEL_NAME}")
        print(f"   Base URL: {ANTHROPIC_BASE_URL}")

        # æµ‹è¯• API è¿æ¥
        try:
            test_response = claude_client.messages.create(
                model=QUESTION_MODEL_NAME,
                max_tokens=5,
                messages=[{"role": "user", "content": "Hi"}]
            )
            print(f"âœ… API è¿æ¥æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            print("   è¯·æ£€æŸ¥ API Key å’Œ Base URLï¼›å®¢æˆ·ç«¯å°†ä¿ç•™ï¼Œåç»­è¯·æ±‚ä¼šç»§ç»­å°è¯•")
    except Exception as e:
        print(f"âŒ Claude å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        claude_client = None
    except Exception as e:
        print(f"âŒ Claude å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
else:
    if not ENABLE_AI:
        print("â„¹ï¸  AI åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_AI=Falseï¼‰")
    elif not HAS_ANTHROPIC:
        print("âŒ anthropic åº“æœªå®‰è£…")
    elif not ANTHROPIC_API_KEY:
        print("âŒ æœªé…ç½® ANTHROPIC_API_KEY")


def _content_block_field(block, field: str):
    """å…¼å®¹å¯¹è±¡/å­—å…¸ä¸¤ç§å†…å®¹å—ç»“æ„è¯»å–å­—æ®µã€‚"""
    if isinstance(block, dict):
        return block.get(field)
    return getattr(block, field, None)


def extract_message_text(message, allow_non_text_fallback: bool = False) -> str:
    """ä»æ¨¡å‹å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œä¼˜å…ˆæå– type=textã€‚"""
    content = getattr(message, "content", None) or []
    if not content:
        return ""

    # ä¼˜å…ˆæå– type=text çš„å†…å®¹å—ï¼Œé¿å…æ‹¿åˆ° thinking å—å¯¼è‡´ç©ºæ–‡æœ¬
    text_parts = []
    for block in content:
        if _content_block_field(block, "type") != "text":
            continue
        block_text = _content_block_field(block, "text")
        if isinstance(block_text, str) and block_text.strip():
            text_parts.append(block_text.strip())

    if text_parts:
        return "\n".join(text_parts).strip()

    # é»˜è®¤ä¸å›é€€åˆ°é text å—ï¼Œé¿å…æŠŠ thinking å½“ä½œæœ€ç»ˆè¾“å‡º
    if not allow_non_text_fallback:
        return ""

    # å…œåº•ï¼šæå°‘æ•°å…¼å®¹å®ç°å¯èƒ½æœªæ ‡è®° typeï¼Œä½†æœ‰ text å­—æ®µ
    fallback_parts = []
    for block in content:
        block_text = _content_block_field(block, "text")
        if isinstance(block_text, str) and block_text.strip():
            fallback_parts.append(block_text.strip())

    return "\n".join(fallback_parts).strip()


def _collect_json_candidates(raw_text: str) -> list:
    """æå–å¯èƒ½çš„ JSON å€™é€‰å­—ç¬¦ä¸²ï¼ˆç›´å‡ºã€ä»£ç å—ã€èŠ±æ‹¬å·é…å¯¹ï¼‰ã€‚"""
    candidates = []
    text = (raw_text or "").strip()
    if not text:
        return candidates

    candidates.append(text)

    # ä»£ç å—å€™é€‰
    for match in re.finditer(r"```(?:json)?\s*([\s\S]*?)```", text, re.IGNORECASE):
        block = (match.group(1) or "").strip()
        if block:
            candidates.append(block)

    # èŠ±æ‹¬å·é…å¯¹æå–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡
    json_start = text.find("{")
    if json_start >= 0:
        brace_count = 0
        in_string = False
        escape_next = False
        json_end = -1

        for idx in range(json_start, len(text)):
            char = text[idx]
            if escape_next:
                escape_next = False
                continue
            if char == "\\":
                escape_next = True
                continue
            if char == "\"":
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == "{":
                brace_count += 1
            elif char == "}":
                brace_count -= 1
                if brace_count == 0:
                    json_end = idx + 1
                    break

        if json_end > json_start:
            candidates.append(text[json_start:json_end].strip())

    # å»é‡ï¼ˆä¿åºï¼‰+ å¤„ç† ```json æå–åæ®‹ç•™çš„ json å‰ç¼€
    normalized = []
    seen = set()
    for candidate in candidates:
        item = candidate.strip()
        if item.lower().startswith("json"):
            item = item[4:].strip()
        if not item or item in seen:
            continue
        seen.add(item)
        normalized.append(item)
    return normalized


def parse_json_object_response(raw_text: str, required_keys: list = None) -> Optional[dict]:
    """ä»æ¨¡å‹æ–‡æœ¬ä¸­å®¹é”™è§£æ JSON å¯¹è±¡ã€‚"""
    required_keys = required_keys or []
    last_error = None

    for candidate in _collect_json_candidates(raw_text):
        try:
            parsed = json.loads(candidate)
        except Exception as exc:
            last_error = exc
            continue

        if not isinstance(parsed, dict):
            last_error = ValueError("JSON ä¸æ˜¯å¯¹è±¡")
            continue

        if any(key not in parsed for key in required_keys):
            last_error = ValueError(f"JSON ç¼ºå°‘å¿…è¦å­—æ®µ: {required_keys}")
            continue

        return parsed

    if last_error:
        raise ValueError(f"JSONè§£æå¤±è´¥: {last_error}")
    raise ValueError("æœªæ‰¾åˆ°å¯è§£æçš„ JSON å¯¹è±¡")


def parse_scenario_recognition_response(raw_text: str, valid_scenario_ids: set) -> Optional[dict]:
    """è§£æåœºæ™¯è¯†åˆ«ç»“æœï¼Œæ”¯æŒ JSON ä¸åŠç»“æ„åŒ–å…œåº•æå–ã€‚"""
    try:
        parsed = parse_json_object_response(raw_text, required_keys=["scenario_id"])
        scenario_id = str(parsed.get("scenario_id", "")).strip()
        if scenario_id not in valid_scenario_ids:
            return None
        confidence = parsed.get("confidence", 0.8)
        try:
            confidence = float(confidence)
        except Exception:
            confidence = 0.8
        confidence = min(1.0, max(0.0, confidence))
        reason = str(parsed.get("reason", "") or "").strip()
        return {
            "scenario_id": scenario_id,
            "confidence": confidence,
            "reason": reason
        }
    except Exception:
        pass

    # å…œåº•ï¼šå¸¸è§æˆªæ–­åœºæ™¯ï¼ˆå¦‚ reason æœªé—­åˆï¼‰æ—¶ï¼Œå°½é‡æå– scenario_id/confidence
    sid_match = re.search(r'"scenario_id"\s*:\s*"([^"]+)"', raw_text or "")
    if not sid_match:
        return None

    scenario_id = sid_match.group(1).strip()
    if scenario_id not in valid_scenario_ids:
        return None

    confidence = 0.8
    conf_match = re.search(r'"confidence"\s*:\s*([0-9]+(?:\.[0-9]+)?)', raw_text or "")
    if conf_match:
        try:
            confidence = float(conf_match.group(1))
        except Exception:
            confidence = 0.8
    confidence = min(1.0, max(0.0, confidence))

    reason = ""
    reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', raw_text or "")
    if reason_match:
        reason = reason_match.group(1).strip()

    return {
        "scenario_id": scenario_id,
        "confidence": confidence,
        "reason": reason
    }


def get_utc_now() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def generate_session_id() -> str:
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    random_suffix = secrets.token_hex(4)
    return f"dv-{timestamp}-{random_suffix}"


def get_deleted_reports() -> set:
    """è·å–å·²åˆ é™¤æŠ¥å‘Šçš„åˆ—è¡¨"""
    if not DELETED_REPORTS_FILE.exists():
        return set()
    try:
        data = json.loads(DELETED_REPORTS_FILE.read_text(encoding="utf-8"))
        return set(data.get("deleted", []))
    except Exception:
        return set()


def load_report_owners() -> dict:
    if not REPORT_OWNERS_FILE.exists():
        return {}
    try:
        payload = json.loads(REPORT_OWNERS_FILE.read_text(encoding="utf-8"))
        if not isinstance(payload, dict):
            return {}
        normalized = {}
        for name, owner in payload.items():
            if not isinstance(name, str):
                continue
            try:
                owner_id = int(owner)
            except (TypeError, ValueError):
                continue
            if owner_id <= 0:
                continue
            normalized[name] = owner_id
        return normalized
    except Exception:
        return {}


def save_report_owners(data: dict) -> None:
    REPORT_OWNERS_FILE.write_text(
        json.dumps(data, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def get_report_owner_id(filename: str) -> int:
    with REPORT_OWNERS_LOCK:
        owners = load_report_owners()
    try:
        return int(owners.get(filename, 0))
    except (TypeError, ValueError):
        return 0


def set_report_owner_id(filename: str, owner_user_id: int) -> None:
    owner_id = int(owner_user_id)
    with REPORT_OWNERS_LOCK:
        owners = load_report_owners()
        owners[filename] = owner_id
        save_report_owners(owners)


def ensure_report_owner(filename: str, owner_user_id: int) -> bool:
    owner = get_report_owner_id(filename)
    owner_id = int(owner_user_id)
    # ç¦æ­¢è‡ªåŠ¨è®¤é¢†æ— å½’å±å†å²æ•°æ®ï¼Œéœ€é€šè¿‡è¿ç§»è„šæœ¬æ˜¾å¼è¿ç§»
    if owner <= 0:
        return False
    return owner == owner_id


def is_session_owned_by_user(session: dict, user_id: int) -> bool:
    if not isinstance(session, dict):
        return False
    owner = session.get("owner_user_id")
    try:
        owner_id = int(owner)
    except (TypeError, ValueError):
        return False
    return owner_id == int(user_id)


def ensure_session_owner(session: dict, user_id: int) -> bool:
    if not isinstance(session, dict):
        return False

    current_owner = session.get("owner_user_id")
    try:
        owner_id = int(current_owner)
    except (TypeError, ValueError):
        owner_id = 0

    # ç¦æ­¢è‡ªåŠ¨è®¤é¢†æ— å½’å±å†å²æ•°æ®ï¼Œéœ€é€šè¿‡è¿ç§»è„šæœ¬æ˜¾å¼è¿ç§»
    if owner_id <= 0:
        return False

    return owner_id == int(user_id)


def get_current_user_id_or_none() -> Optional[int]:
    user = get_current_user()
    if not user:
        return None
    try:
        return int(user["id"])
    except (TypeError, ValueError, KeyError):
        return None


def load_session_for_user(session_id: str, user_id: int, include_missing: bool = False):
    session_file = SESSIONS_DIR / f"{session_id}.json"
    if not session_file.exists():
        if include_missing:
            return None, None, "not_found"
        return None, "ä¼šè¯ä¸å­˜åœ¨", 404

    session_data = safe_load_session(session_file)
    if session_data is None:
        return None, "ä¼šè¯æ•°æ®æŸå", 500

    if not ensure_session_owner(session_data, user_id):
        return None, "ä¼šè¯ä¸å­˜åœ¨", 404

    if include_missing:
        return session_file, session_data, "ok"
    return session_file, session_data


def enforce_report_owner_or_404(filename: str, user_id: int) -> tuple[Optional[Path], Optional[tuple]]:
    report_file = REPORTS_DIR / filename
    if not report_file.exists():
        return None, (jsonify({"error": "æŠ¥å‘Šä¸å­˜åœ¨"}), 404)

    if not ensure_report_owner(filename, user_id):
        return None, (jsonify({"error": "æŠ¥å‘Šä¸å­˜åœ¨"}), 404)

    return report_file, None


def mark_report_as_deleted(filename: str):
    """æ ‡è®°æŠ¥å‘Šä¸ºå·²åˆ é™¤ï¼ˆä¸çœŸæ­£åˆ é™¤æ–‡ä»¶ï¼‰"""
    deleted = get_deleted_reports()
    deleted.add(filename)
    DELETED_REPORTS_FILE.write_text(
        json.dumps({"deleted": list(deleted)}, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def normalize_topic_slug(topic: str) -> str:
    """æŒ‰æŠ¥å‘Šå‘½åè§„åˆ™ç”Ÿæˆä¸»é¢˜ slugã€‚"""
    if not isinstance(topic, str):
        return ""
    topic = topic.strip()
    if not topic:
        return ""
    return re.sub(r"\s+", "-", topic)[:30]


def get_session_total_progress(session: dict) -> int:
    """è®¡ç®—ä¼šè¯æ€»è¿›åº¦ï¼ˆå„ç»´åº¦è¦†ç›–ç‡å¹³å‡å€¼ï¼‰ã€‚"""
    dimensions = session.get("dimensions")
    if not isinstance(dimensions, dict) or not dimensions:
        return 0

    values = []
    for value in dimensions.values():
        if not isinstance(value, dict):
            continue
        coverage = value.get("coverage", 0)
        try:
            coverage_value = int(coverage)
        except Exception:
            coverage_value = 0
        values.append(max(0, min(100, coverage_value)))

    if not values:
        return 0
    return round(sum(values) / len(values))


def get_effective_session_status(session: dict) -> str:
    """ç»Ÿä¸€ä¼šè¯çŠ¶æ€å£å¾„ï¼ˆä¸å‰ç«¯ä¿æŒä¸€è‡´ï¼‰ã€‚"""
    raw = session.get("status") or "in_progress"
    progress = get_session_total_progress(session)
    if raw == "in_progress" and progress >= 100:
        return "pending_review"
    return raw


def find_reports_by_session_topic(session: dict) -> list:
    """æ ¹æ®ä¼šè¯ä¸»é¢˜åŒ¹é…å…³è”æŠ¥å‘Šæ–‡ä»¶ã€‚"""
    topic_slug = normalize_topic_slug(session.get("topic", ""))
    if not topic_slug:
        return []

    suffix = f"-{topic_slug}.md"
    matched = []
    for report_file in REPORTS_DIR.glob("deep-vision-*.md"):
        if report_file.name.endswith(suffix):
            matched.append(report_file.name)
    return matched


def unique_non_empty_strings(items) -> list:
    """å»é‡å¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ã€‚"""
    if not isinstance(items, list):
        return []

    result = []
    seen = set()
    for item in items:
        if not isinstance(item, str):
            continue
        value = item.strip()
        if not value or value in seen:
            continue
        seen.add(value)
        result.append(value)
    return result


def get_deleted_docs() -> dict:
    """è·å–å·²åˆ é™¤æ–‡æ¡£çš„è®°å½•"""
    if not DELETED_DOCS_FILE.exists():
        return {"reference_materials": []}
    try:
        data = json.loads(DELETED_DOCS_FILE.read_text(encoding="utf-8"))
        # å…¼å®¹æ—§æ ¼å¼
        materials = data.get("reference_materials", [])
        materials.extend(data.get("reference_docs", []))
        materials.extend(data.get("research_docs", []))
        return {"reference_materials": materials}
    except Exception:
        return {"reference_materials": []}


def mark_doc_as_deleted(session_id: str, doc_name: str, doc_type: str = "reference_materials"):
    """æ ‡è®°æ–‡æ¡£ä¸ºå·²åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰

    Args:
        session_id: ä¼šè¯ ID
        doc_name: æ–‡æ¡£åç§°
        doc_type: æ–‡æ¡£ç±»å‹ï¼ˆé»˜è®¤ 'reference_materials'ï¼‰
    """
    deleted = get_deleted_docs()
    record = {
        "session_id": session_id,
        "doc_name": doc_name,
        "deleted_at": get_utc_now()
    }
    if "reference_materials" not in deleted:
        deleted["reference_materials"] = []
    deleted["reference_materials"].append(record)
    DELETED_DOCS_FILE.write_text(
        json.dumps(deleted, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def migrate_session_docs(session: dict) -> dict:
    """è¿ç§»æ—§ä¼šè¯æ•°æ®ï¼šå°† reference_docs + research_docs åˆå¹¶ä¸º reference_materials

    Args:
        session: ä¼šè¯æ•°æ®å­—å…¸

    Returns:
        è¿ç§»åçš„ä¼šè¯æ•°æ®
    """
    # å¦‚æœå·²ç»æœ‰ reference_materialsï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ—§å­—æ®µéœ€è¦è¿ç§»
    if "reference_materials" not in session:
        session["reference_materials"] = []

    # è¿ç§» reference_docs
    if "reference_docs" in session:
        for doc in session["reference_docs"]:
            if "source" not in doc:
                doc["source"] = "upload"
            session["reference_materials"].append(doc)
        del session["reference_docs"]

    # è¿ç§» research_docs
    if "research_docs" in session:
        for doc in session["research_docs"]:
            if "source" not in doc:
                doc["source"] = "auto"
            session["reference_materials"].append(doc)
        del session["research_docs"]

    return session


# ============ è”ç½‘æœç´¢åŠŸèƒ½ ============

class MCPClient:
    """æ™ºè°±AI MCPå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
        self.session_id = None
        self.message_id = 0

    def _get_next_id(self):
        """è·å–ä¸‹ä¸€ä¸ªæ¶ˆæ¯ID"""
        self.message_id += 1
        return self.message_id

    def _make_request(self, method: str, params: dict = None):
        """å‘é€MCP JSON-RPCè¯·æ±‚"""
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }

        # å¦‚æœæœ‰session_idï¼Œæ·»åŠ åˆ°header
        if self.session_id:
            headers["Mcp-Session-Id"] = self.session_id

        # åœ¨URLä¸­æ·»åŠ Authorizationå‚æ•°
        url = f"{self.base_url}?Authorization={self.api_key}"

        request_data = {
            "jsonrpc": "2.0",
            "id": self._get_next_id(),
            "method": method,
            "params": params or {}
        }

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“¤ MCPè¯·æ±‚: {method}")
            print(f"   å‚æ•°: {params}")

        response = requests.post(url, json=request_data, headers=headers, timeout=SEARCH_TIMEOUT)
        response.raise_for_status()

        # æ£€æŸ¥å“åº”å¤´ä¸­çš„Session ID
        if "Mcp-Session-Id" in response.headers:
            self.session_id = response.headers["Mcp-Session-Id"]
            if ENABLE_DEBUG_LOG:
                print(f"   ğŸ“ è·å¾—Session ID: {self.session_id}")

        # è§£æSSEæ ¼å¼çš„å“åº”
        response_text = response.text.strip()

        # SSEæ ¼å¼: id:1\nevent:message\ndata:{json}\n\n
        result_data = None
        for line in response_text.split('\n'):
            line = line.strip()
            if line.startswith('data:'):
                json_str = line[5:].strip()  # å»æ‰ "data:" å‰ç¼€
                try:
                    result_data = json.loads(json_str)
                    break
                except:
                    continue

        if not result_data:
            raise Exception(f"æ— æ³•è§£æSSEå“åº”: {response_text[:200]}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in result_data:
            raise Exception(f"MCPé”™è¯¯: {result_data['error']}")

        return result_data.get("result", {})

    def initialize(self):
        """åˆå§‹åŒ–MCPè¿æ¥"""
        try:
            result = self._make_request("initialize", {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {
                    "name": "deep-vision",
                    "version": "1.0.0"
                }
            })
            if ENABLE_DEBUG_LOG:
                print(f"âœ… MCPåˆå§‹åŒ–æˆåŠŸ")
            return result
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âŒ MCPåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def call_tool(self, tool_name: str, arguments: dict):
        """è°ƒç”¨MCPå·¥å…·"""
        try:
            # ç¡®ä¿å·²åˆå§‹åŒ–
            if not self.session_id:
                self.initialize()

            result = self._make_request("tools/call", {
                "name": tool_name,
                "arguments": arguments
            })

            return result
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            raise


def web_search(query: str) -> list:
    """ä½¿ç”¨æ™ºè°±AI MCP web_search_prime è¿›è¡Œè”ç½‘æœç´¢"""
    global web_search_active

    if not ENABLE_WEB_SEARCH or not ZHIPU_API_KEY or ZHIPU_API_KEY == "your-zhipu-api-key-here":
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  æœç´¢åŠŸèƒ½æœªå¯ç”¨æˆ– API Key æœªé…ç½®ï¼Œè·³è¿‡æœç´¢: {query}")
        return []

    try:
        # è®¾ç½®æœç´¢çŠ¶æ€ä¸ºæ´»åŠ¨
        web_search_active = True

        mcp_url = "https://open.bigmodel.cn/api/mcp/web_search_prime/mcp"

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” å¼€å§‹MCPæœç´¢: {query}")

        # åˆ›å»ºMCPå®¢æˆ·ç«¯
        client = MCPClient(ZHIPU_API_KEY, mcp_url)

        # è°ƒç”¨webSearchPrimeå·¥å…·ï¼ˆæ³¨æ„ï¼šå·¥å…·åæ˜¯é©¼å³°å‘½åï¼‰
        result = client.call_tool("webSearchPrime", {
            "search_query": query,
            "search_recency_filter": "noLimit",
            "content_size": "medium"
        })

        # è§£æç»“æœ
        results = []

        # MCPè¿”å›çš„contentæ˜¯ä¸€ä¸ªåˆ—è¡¨
        content_list = result.get("content", [])

        for item in content_list:
            if item.get("type") == "text":
                # æ–‡æœ¬å†…å®¹
                text = item.get("text", "")

                # å°è¯•è§£æJSONæ ¼å¼çš„æœç´¢ç»“æœ
                try:
                    import json as json_module

                    # ç¬¬ä¸€æ¬¡è§£æï¼šå»æ‰å¤–å±‚å¼•å·å’Œè½¬ä¹‰
                    if text.startswith('"') and text.endswith('"'):
                        text = json_module.loads(text)

                    # ç¬¬äºŒæ¬¡è§£æï¼šè·å–å®é™…çš„æœç´¢ç»“æœæ•°ç»„
                    search_data = json_module.loads(text)

                    # å¦‚æœæ˜¯åˆ—è¡¨å½¢å¼çš„æœç´¢ç»“æœ
                    if isinstance(search_data, list):
                        for entry in search_data[:SEARCH_MAX_RESULTS]:
                            title = entry.get("title", "")
                            content = entry.get("content", "")
                            url = entry.get("link", entry.get("url", ""))

                            if title or content:  # ç¡®ä¿æœ‰å®é™…å†…å®¹
                                results.append({
                                    "type": "result",
                                    "title": title[:100] if title else "æœç´¢ç»“æœ",
                                    "content": content[:300],
                                    "url": url
                                })
                    # å¦‚æœæ˜¯å•ä¸ªç»“æœ
                    elif isinstance(search_data, dict):
                        title = search_data.get("title", "")
                        content = search_data.get("content", text[:300])
                        url = search_data.get("link", search_data.get("url", ""))

                        results.append({
                            "type": "result",
                            "title": title[:100] if title else "æœç´¢ç»“æœ",
                            "content": content[:300],
                            "url": url
                        })
                except Exception as parse_error:
                    if ENABLE_DEBUG_LOG:
                        print(f"âš ï¸  è§£ææœç´¢ç»“æœå¤±è´¥: {parse_error}")
                        print(f"   åŸå§‹æ–‡æœ¬å‰200å­—ç¬¦: {text[:200]}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥ä½œä¸ºæ–‡æœ¬ç»“æœ
                    results.append({
                        "type": "result",
                        "title": "æœç´¢ç»“æœ",
                        "content": text[:300],
                        "url": ""
                    })

        if ENABLE_DEBUG_LOG:
            print(f"âœ… MCPæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")

        # æœç´¢å®Œæˆï¼Œé‡ç½®çŠ¶æ€
        web_search_active = False
        return results

    except requests.exceptions.Timeout:
        print(f"â±ï¸  æœç´¢è¶…æ—¶: {query}")
        web_search_active = False
        return []
    except Exception as e:
        print(f"âŒ MCPæœç´¢å¤±è´¥: {e}")
        if ENABLE_DEBUG_LOG:
            import traceback
            traceback.print_exc()
        web_search_active = False
        return []


def should_search(topic: str, dimension: str, context: dict) -> bool:
    """
    è§„åˆ™é¢„åˆ¤ï¼šå¿«é€Ÿåˆ¤æ–­æ˜¯å¦å¯èƒ½éœ€è¦è”ç½‘æœç´¢ï¼ˆå…œåº•è§„åˆ™ï¼‰
    è¿”å› True è¡¨ç¤º"å¯èƒ½éœ€è¦"ï¼Œåç»­ä¼šäº¤ç»™ AI åšæœ€ç»ˆåˆ¤æ–­
    """
    if not ENABLE_WEB_SEARCH:
        return False

    # ========== æ‰©å±•çš„å…³é”®è¯åº“ ==========

    # æŠ€æœ¯å…³é”®è¯
    tech_keywords = [
        "æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æ¡†æ¶", "å·¥å…·", "è½¯ä»¶", "åº”ç”¨", "æ¶æ„",
        "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¤§æ¨¡å‹", "LLM", "GPT",
        "äº‘", "SaaS", "PaaS", "IaaS", "å¾®æœåŠ¡", "å®¹å™¨", "Docker", "K8s", "Kubernetes",
        "æ•°æ®åº“", "ä¸­é—´ä»¶", "API", "é›†æˆ", "éƒ¨ç½²", "è¿ç»´", "DevOps",
        "å‰ç«¯", "åç«¯", "å…¨æ ˆ", "ç§»åŠ¨ç«¯", "App", "å°ç¨‹åº"
    ]

    # å‚ç›´è¡Œä¸šå…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    industry_keywords = [
        # åŒ»ç–—å¥åº·
        "åŒ»é™¢", "åŒ»ç–—", "HIS", "LIS", "PACS", "EMR", "ç”µå­ç—…å†", "DRG", "åŒ»ä¿",
        "è¯Šæ‰€", "è¯æˆ¿", "å¤„æ–¹", "æŒ‚å·", "é—¨è¯Š", "ä½é™¢", "æŠ¤ç†", "CDSS",
        # é‡‘è
        "é“¶è¡Œ", "ä¿é™©", "è¯åˆ¸", "åŸºé‡‘", "ä¿¡æ‰˜", "æ”¯ä»˜", "æ¸…ç®—", "é£æ§",
        "åæ´—é’±", "å¾ä¿¡", "èµ„ç®¡", "ç†è´¢", "è´·æ¬¾", "ä¿¡ç”¨å¡",
        # æ•™è‚²
        "å­¦æ ¡", "æ•™è‚²", "åŸ¹è®­", "è¯¾ç¨‹", "æ•™å­¦", "å­¦ç”Ÿ", "è€ƒè¯•", "æ‹›ç”Ÿ",
        "åœ¨çº¿æ•™è‚²", "ç½‘è¯¾", "åŒå‡", "æ–°è¯¾æ ‡",
        # åˆ¶é€ 
        "å·¥å‚", "åˆ¶é€ ", "ç”Ÿäº§", "è½¦é—´", "MES", "ERP", "PLM", "SCM", "WMS",
        "å·¥ä¸šäº’è”ç½‘", "æ™ºèƒ½åˆ¶é€ ", "æ•°å­—å­ªç”Ÿ", "è´¨æ£€", "è®¾å¤‡", "äº§çº¿",
        # é›¶å”®ç”µå•†
        "é›¶å”®", "ç”µå•†", "é—¨åº—", "å•†åŸ", "è®¢å•", "åº“å­˜", "ç‰©æµ", "é…é€",
        "ä¼šå‘˜", "è¥é”€", "ä¿ƒé”€", "CRM", "POS",
        # æ”¿åŠ¡
        "æ”¿åºœ", "æ”¿åŠ¡", "å®¡æ‰¹", "åŠäº‹", "å…¬å…±æœåŠ¡", "æ™ºæ…§åŸå¸‚", "æ•°å­—æ”¿åºœ",
        # èƒ½æº
        "ç”µåŠ›", "èƒ½æº", "ç”µç½‘", "æ–°èƒ½æº", "å…‰ä¼", "é£ç”µ", "å‚¨èƒ½", "å……ç”µæ¡©",
        # äº¤é€šç‰©æµ
        "äº¤é€š", "ç‰©æµ", "è¿è¾“", "ä»“å‚¨", "TMS", "è°ƒåº¦", "è½¦é˜Ÿ"
    ]

    # åˆè§„æ”¿ç­–å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    compliance_keywords = [
        "åˆè§„", "æ ‡å‡†", "è§„èŒƒ", "è®¤è¯", "ç­‰ä¿", "ISO", "GDPR", "éšç§",
        "å®‰å…¨", "å®¡è®¡", "æ³•è§„", "æ”¿ç­–", "ç›‘ç®¡", "èµ„è´¨", "è®¸å¯è¯"
    ]

    # æ—¶æ•ˆæ€§å…³é”®è¯
    time_sensitive_keywords = [
        "æœ€æ–°", "å½“å‰", "ç°åœ¨", "è¿‘æœŸ", "ä»Šå¹´", "æ˜å¹´",
        "2024", "2025", "2026", "2027",
        "è¶‹åŠ¿", "æœªæ¥", "å‘å±•", "åŠ¨æ€", "å˜åŒ–", "æ›´æ–°",
        "å¸‚åœº", "è¡Œæƒ…", "ç«å“", "å¯¹æ‰‹", "ç°çŠ¶"
    ]

    # ä¸ç¡®å®šæ€§/ä¸“ä¸šæ€§å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    uncertainty_keywords = [
        "æ€ä¹ˆé€‰", "å¦‚ä½•é€‰æ‹©", "å“ªä¸ªå¥½", "æ¨è", "å»ºè®®", "æ¯”è¾ƒ",
        "æœ€ä½³å®è·µ", "ä¸šç•Œ", "å¤´éƒ¨", "é¢†å…ˆ", "ä¸»æµ", "æ ‡æ†"
    ]

    all_keywords = (tech_keywords + industry_keywords + compliance_keywords +
                   time_sensitive_keywords + uncertainty_keywords)

    # å¦‚æœä¸»é¢˜åŒ…å«ä»»ä½•å…³é”®è¯ï¼Œæ ‡è®°ä¸º"å¯èƒ½éœ€è¦æœç´¢"
    for keyword in all_keywords:
        if keyword.lower() in topic.lower():
            return True

    # æŠ€æœ¯çº¦æŸç»´åº¦é€šå¸¸éœ€è¦æœç´¢
    if dimension == "tech_constraints":
        return True

    return False


def ai_evaluate_search_need(topic: str, dimension: str, context: dict, recent_qa: list) -> dict:
    """
    AI è‡ªä¸»åˆ¤æ–­ï¼šè®© AI è¯„ä¼°æ˜¯å¦éœ€è¦è”ç½‘æœç´¢
    è¿”å›: { "need_search": bool, "reason": str, "search_query": str }
    """
    global claude_client

    if not ENABLE_WEB_SEARCH or not claude_client:
        return {"need_search": False, "reason": "æœç´¢åŠŸèƒ½æœªå¯ç”¨", "search_query": ""}

    search_dim_info = get_dimension_info_for_session(context) if context else DIMENSION_INFO
    dim_info = search_dim_info.get(dimension, {})
    dim_name = dim_info.get("name", dimension)

    # æ„å»ºæœ€è¿‘çš„é—®ç­”ä¸Šä¸‹æ–‡
    recent_context = ""
    if recent_qa:
        recent_context = "\n".join([
            f"Q: {qa.get('question', '')}\nA: {qa.get('answer', '')}"
            for qa in recent_qa[-3:]  # åªå–æœ€è¿‘3æ¡
        ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢å†³ç­–åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­åœ¨å½“å‰è®¿è°ˆåœºæ™¯ä¸‹ï¼Œæ˜¯å¦éœ€è¦è”ç½‘æœç´¢æ¥è·å–æ›´å‡†ç¡®ã€æ›´ä¸“ä¸šçš„ä¿¡æ¯ã€‚

## å½“å‰è®¿è°ˆä¿¡æ¯
- è®¿è°ˆä¸»é¢˜ï¼š{topic}
- å½“å‰ç»´åº¦ï¼š{dim_name}
- æœ€è¿‘é—®ç­”ï¼š
{recent_context if recent_context else "ï¼ˆå°šæœªå¼€å§‹é—®ç­”ï¼‰"}

## åˆ¤æ–­æ ‡å‡†
è¯·è¯„ä¼°ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ï¼š

1. **çŸ¥è¯†æ—¶æ•ˆæ€§**ï¼šæ˜¯å¦æ¶‰åŠè¿‘1-2å¹´çš„æ”¿ç­–ã€å¸‚åœºã€æŠ€æœ¯å˜åŒ–ï¼Ÿ
2. **ä¸“ä¸šé¢†åŸŸæ·±åº¦**ï¼šæ˜¯å¦æ¶‰åŠä½ å¯èƒ½ä¸å¤Ÿç†Ÿæ‚‰çš„å‚ç›´è¡Œä¸šç»†èŠ‚ï¼ˆå¦‚åŒ»ç–—ç¼–ç è§„åˆ™ã€é‡‘èç›‘ç®¡è¦æ±‚ã€è¡Œä¸šæ ‡å‡†å‚æ•°ç­‰ï¼‰ï¼Ÿ
3. **ç«å“/å¸‚åœºä¿¡æ¯**ï¼šæ˜¯å¦éœ€è¦äº†è§£å¸‚åœºç°çŠ¶ã€ç«äº‰å¯¹æ‰‹ã€è¡Œä¸šå¤´éƒ¨äº§å“ï¼Ÿ
4. **æœ€ä½³å®è·µå‚è€ƒ**ï¼šæ˜¯å¦éœ€è¦äº†è§£ä¸šç•Œçš„æœ€æ–°åšæ³•ã€æˆåŠŸæ¡ˆä¾‹ï¼Ÿ
5. **æ•°æ®/æŒ‡æ ‡å‚è€ƒ**ï¼šæ˜¯å¦éœ€è¦äº†è§£è¡Œä¸šåŸºå‡†æ•°æ®ã€å¸¸è§å‚æ•°èŒƒå›´ï¼Ÿ

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š
{{
    "need_search": trueæˆ–false,
    "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±ï¼ˆ20å­—ä»¥å†…ï¼‰",
    "search_query": "å¦‚æœéœ€è¦æœç´¢ï¼Œç»™å‡ºæœ€ä½³æœç´¢è¯ï¼ˆè¦ç²¾å‡†ã€å…·ä½“ï¼Œ15å­—ä»¥å†…ï¼‰ï¼›ä¸éœ€è¦æœç´¢åˆ™ç•™ç©º"
}}"""

    try:
        result = None
        attempts = [
            {
                "max_tokens": 220,
                "timeout": 10.0,
                "extra_instruction": "",
            },
            {
                "max_tokens": 420,
                "timeout": 18.0,
                "extra_instruction": "åªè¾“å‡ºä¸€è¡Œ JSONï¼Œä¸è¦ markdown ä»£ç å—ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œä¸è¦æ¢è¡Œã€‚",
            },
        ]

        for idx, attempt in enumerate(attempts, start=1):
            try:
                attempt_prompt = prompt
                if attempt["extra_instruction"]:
                    attempt_prompt = f"{prompt}\n\nã€é¢å¤–è¦æ±‚ã€‘{attempt['extra_instruction']}"

                response = claude_client.messages.create(
                    model=resolve_model_name(call_type="search_decision"),
                    max_tokens=attempt["max_tokens"],
                    timeout=attempt["timeout"],
                    messages=[{"role": "user", "content": attempt_prompt}]
                )

                result_text = extract_message_text(response)
                if not result_text:
                    raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«å¯ç”¨æ–‡æœ¬å†…å®¹")

                result = parse_json_object_response(
                    result_text,
                    required_keys=["need_search", "reason", "search_query"]
                )
                break
            except Exception as retry_error:
                if ENABLE_DEBUG_LOG:
                    print(f"âš ï¸  AIæœç´¢å†³ç­–ç¬¬{idx}æ¬¡è§£æå¤±è´¥: {retry_error}")

        if result is None:
            raise ValueError("AI æœç´¢å†³ç­–ç»“æœè§£æå¤±è´¥")

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– AIæœç´¢å†³ç­–: need_search={result.get('need_search')}, reason={result.get('reason')}")

        need_search = result.get("need_search", False)
        if isinstance(need_search, str):
            need_search = need_search.strip().lower() in ["true", "1", "yes", "y"]

        return {
            "need_search": bool(need_search),
            "reason": str(result.get("reason", "") or ""),
            "search_query": str(result.get("search_query", "") or "")
        }

    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  AIæœç´¢å†³ç­–å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è¿”å›ä¸æœç´¢ï¼Œé¿å…é˜»å¡æµç¨‹
        return {"need_search": False, "reason": f"å†³ç­–å¤±è´¥: {e}", "search_query": ""}


def smart_search_decision(topic: str, dimension: str, context: dict, recent_qa: list = None) -> tuple:
    """
    æ™ºèƒ½æœç´¢å†³ç­–ï¼šè§„åˆ™é¢„åˆ¤ + AI æœ€ç»ˆåˆ¤æ–­
    è¿”å›: (need_search: bool, search_query: str, reason: str)
    """
    if not ENABLE_WEB_SEARCH:
        return (False, "", "æœç´¢åŠŸèƒ½æœªå¯ç”¨")

    # ç¬¬ä¸€æ­¥ï¼šè§„åˆ™é¢„åˆ¤
    rule_suggests_search = should_search(topic, dimension, context)

    if not rule_suggests_search:
        # è§„åˆ™åˆ¤æ–­ä¸éœ€è¦æœç´¢ï¼Œä½†è®© AI åšäºŒæ¬¡ç¡®è®¤ï¼ˆå¯èƒ½æ¼æ‰çš„åœºæ™¯ï¼‰
        ai_result = ai_evaluate_search_need(topic, dimension, context, recent_qa or [])
        if ai_result["need_search"]:
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ” è§„åˆ™æœªè§¦å‘ï¼Œä½†AIå»ºè®®æœç´¢: {ai_result['reason']}")
            return (True, ai_result["search_query"], f"AIå»ºè®®: {ai_result['reason']}")
        else:
            return (False, "", "è§„åˆ™å’ŒAIå‡åˆ¤æ–­ä¸éœ€è¦æœç´¢")

    # ç¬¬äºŒæ­¥ï¼šè§„åˆ™å»ºè®®æœç´¢ï¼Œè®© AI ç”Ÿæˆç²¾å‡†æœç´¢è¯
    ai_result = ai_evaluate_search_need(topic, dimension, context, recent_qa or [])

    if ai_result["need_search"] and ai_result["search_query"]:
        # AI ç¡®è®¤éœ€è¦æœç´¢ï¼Œä½¿ç”¨ AI ç”Ÿæˆçš„æœç´¢è¯
        return (True, ai_result["search_query"], ai_result["reason"])
    elif ai_result["need_search"]:
        # AI ç¡®è®¤éœ€è¦ä½†æ²¡ç»™æœç´¢è¯ï¼Œä½¿ç”¨å…œåº•æ¨¡æ¿
        fallback_query = generate_search_query(topic, dimension, context)
        return (True, fallback_query, "AIç¡®è®¤éœ€è¦ï¼Œä½¿ç”¨æ¨¡æ¿æœç´¢è¯")
    else:
        # AI åˆ¤æ–­å®é™…ä¸éœ€è¦æœç´¢ï¼ˆè§„åˆ™è¯¯è§¦å‘ï¼‰
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” è§„åˆ™è§¦å‘ä½†AIåˆ¤æ–­ä¸éœ€è¦: {ai_result['reason']}")
        return (False, "", f"AIåˆ¤æ–­ä¸éœ€è¦: {ai_result['reason']}")


def generate_search_query(topic: str, dimension: str, context: dict) -> str:
    """ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼ˆå…œåº•æ¨¡æ¿ï¼Œå½“ AI æœªç”Ÿæˆæœç´¢è¯æ—¶ä½¿ç”¨ï¼‰"""
    gen_query_dim_info = get_dimension_info_for_session(context) if context else DIMENSION_INFO
    dim_info = gen_query_dim_info.get(dimension, {})
    dim_name = dim_info.get("name", dimension)

    # æ„å»ºæœç´¢æŸ¥è¯¢ - å¯¹äºéé»˜è®¤ç»´åº¦ä½¿ç”¨é€šç”¨æ¨¡æ¿
    if dimension == "tech_constraints":
        return f"{topic} æŠ€æœ¯é€‰å‹ æœ€ä½³å®è·µ 2026"
    elif dimension == "customer_needs":
        return f"{topic} ç”¨æˆ·éœ€æ±‚ è¡Œä¸šç—›ç‚¹ 2026"
    elif dimension == "business_process":
        return f"{topic} ä¸šåŠ¡æµç¨‹ æœ€ä½³å®è·µ"
    elif dimension == "project_constraints":
        return f"{topic} é¡¹ç›®å®æ–½ æˆæœ¬é¢„ç®— å‘¨æœŸ"
    else:
        # éé»˜è®¤ç»´åº¦ï¼Œä½¿ç”¨ç»´åº¦åç§°æ„å»ºé€šç”¨æœç´¢è¯
        return f"{topic} {dim_name}"


# ============ Deep Vision AI æ ¸å¿ƒé€»è¾‘ ============

DIMENSION_INFO = {
    "customer_needs": {
        "name": "å®¢æˆ·éœ€æ±‚",
        "description": "æ ¸å¿ƒç—›ç‚¹ã€æœŸæœ›ä»·å€¼ã€ä½¿ç”¨åœºæ™¯ã€ç”¨æˆ·è§’è‰²",
        "key_aspects": ["æ ¸å¿ƒç—›ç‚¹", "æœŸæœ›ä»·å€¼", "ä½¿ç”¨åœºæ™¯", "ç”¨æˆ·è§’è‰²"]
    },
    "business_process": {
        "name": "ä¸šåŠ¡æµç¨‹",
        "description": "å…³é”®æµç¨‹èŠ‚ç‚¹ã€è§’è‰²åˆ†å·¥ã€è§¦å‘äº‹ä»¶ã€å¼‚å¸¸å¤„ç†",
        "key_aspects": ["å…³é”®æµç¨‹", "è§’è‰²åˆ†å·¥", "è§¦å‘äº‹ä»¶", "å¼‚å¸¸å¤„ç†"]
    },
    "tech_constraints": {
        "name": "æŠ€æœ¯çº¦æŸ",
        "description": "ç°æœ‰æŠ€æœ¯æ ˆã€é›†æˆæ¥å£è¦æ±‚ã€æ€§èƒ½æŒ‡æ ‡ã€å®‰å…¨åˆè§„",
        "key_aspects": ["éƒ¨ç½²æ–¹å¼", "ç³»ç»Ÿé›†æˆ", "æ€§èƒ½è¦æ±‚", "å®‰å…¨åˆè§„"]
    },
    "project_constraints": {
        "name": "é¡¹ç›®çº¦æŸ",
        "description": "é¢„ç®—èŒƒå›´ã€æ—¶é—´èŠ‚ç‚¹ã€èµ„æºé™åˆ¶ã€å…¶ä»–çº¦æŸ",
        "key_aspects": ["é¢„ç®—èŒƒå›´", "æ—¶é—´èŠ‚ç‚¹", "èµ„æºé™åˆ¶", "ä¼˜å…ˆçº§"]
    }
}


def get_dimension_info_for_session(session: dict) -> dict:
    """
    è·å–ä¼šè¯çš„ç»´åº¦ä¿¡æ¯ï¼ˆæ”¯æŒåŠ¨æ€åœºæ™¯ï¼‰

    ä¼˜å…ˆä»ä¼šè¯çš„ scenario_config ä¸­è·å–ï¼Œ
    å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤çš„ DIMENSION_INFOã€‚

    Args:
        session: ä¼šè¯æ•°æ®

    Returns:
        ç»´åº¦ä¿¡æ¯å­—å…¸ {dim_id: {name, description, key_aspects}}
    """
    scenario_config = session.get("scenario_config")

    if scenario_config and "dimensions" in scenario_config:
        return {
            dim["id"]: {
                "name": dim.get("name", dim["id"]),
                "description": dim.get("description", ""),
                "key_aspects": dim.get("key_aspects", []),
                "weight": dim.get("weight"),
                "scoring_criteria": dim.get("scoring_criteria")
            }
            for dim in scenario_config["dimensions"]
        }

    # å‘åå…¼å®¹ï¼šè¿”å›é»˜è®¤ç»´åº¦
    return DIMENSION_INFO


def get_dimension_order_for_session(session: dict) -> list:
    """
    è·å–ä¼šè¯çš„ç»´åº¦é¡ºåºåˆ—è¡¨

    Args:
        session: ä¼šè¯æ•°æ®

    Returns:
        ç»´åº¦ ID åˆ—è¡¨
    """
    scenario_config = session.get("scenario_config")

    if scenario_config and "dimensions" in scenario_config:
        return [dim["id"] for dim in scenario_config["dimensions"]]

    # å‘åå…¼å®¹ï¼šè¿”å›é»˜è®¤é¡ºåº
    return list(DIMENSION_INFO.keys())


# ============ æ»‘åŠ¨çª—å£ä¸Šä¸‹æ–‡ç®¡ç† ============

# è¿è¡Œç­–ç•¥é…ç½®ï¼šä¼˜å…ˆè¯»å– config.pyï¼ˆæˆ–åŒåç¯å¢ƒå˜é‡ï¼‰ï¼Œç¼ºå¤±æ—¶ä½¿ç”¨é»˜è®¤å€¼
def _runtime_cfg(name: str, default):
    if runtime_config and hasattr(runtime_config, name):
        return getattr(runtime_config, name)
    env_val = os.environ.get(name)
    if env_val is not None:
        return env_val
    return default


def _runtime_cfg_int(name: str, default: int) -> int:
    try:
        return int(_runtime_cfg(name, default))
    except Exception:
        return default


def _runtime_cfg_float(name: str, default: float) -> float:
    try:
        return float(_runtime_cfg(name, default))
    except Exception:
        return default


def _runtime_cfg_bool(name: str, default: bool) -> bool:
    value = _runtime_cfg(name, default)
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return bool(value)
    if isinstance(value, str):
        v = value.strip().lower()
        if v in {"1", "true", "yes", "on", "y"}:
            return True
        if v in {"0", "false", "no", "off", "n"}:
            return False
    return default


# é…ç½®å‚æ•°
CONTEXT_WINDOW_SIZE = _runtime_cfg_int("CONTEXT_WINDOW_SIZE", 5)  # ä¿ç•™æœ€è¿‘Næ¡å®Œæ•´é—®ç­”
SUMMARY_THRESHOLD = _runtime_cfg_int("SUMMARY_THRESHOLD", 8)      # è¶…è¿‡æ­¤æ•°é‡æ—¶è§¦å‘æ‘˜è¦ç”Ÿæˆ
MAX_DOC_LENGTH = _runtime_cfg_int("MAX_DOC_LENGTH", 2000)         # å•ä¸ªæ–‡æ¡£æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
MAX_TOTAL_DOCS = _runtime_cfg_int("MAX_TOTAL_DOCS", 5000)         # æ‰€æœ‰æ–‡æ¡£æ€»é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦ï¼‰
API_TIMEOUT = _runtime_cfg_float("API_TIMEOUT", 90.0)             # é€šç”¨ API è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
# æŠ¥å‘Šç”Ÿæˆé€šå¸¸æ¯”é—®ç­”æ›´è€—æ—¶ï¼Œå•ç‹¬æ”¾å®½è¶…æ—¶ä»¥æå‡ç¨³å®šæ€§
REPORT_API_TIMEOUT = _runtime_cfg_float("REPORT_API_TIMEOUT", 210.0)
if REPORT_API_TIMEOUT < API_TIMEOUT:
    REPORT_API_TIMEOUT = API_TIMEOUT

# ============ æ™ºèƒ½æ–‡æ¡£æ‘˜è¦é…ç½®ï¼ˆç¬¬ä¸‰é˜¶æ®µä¼˜åŒ–ï¼‰ ============
ENABLE_SMART_SUMMARY = _runtime_cfg_bool("ENABLE_SMART_SUMMARY", True)        # å¯ç”¨æ™ºèƒ½æ–‡æ¡£æ‘˜è¦
SMART_SUMMARY_THRESHOLD = _runtime_cfg_int("SMART_SUMMARY_THRESHOLD", 1500)    # è§¦å‘æ™ºèƒ½æ‘˜è¦é˜ˆå€¼ï¼ˆå­—ç¬¦ï¼‰
SMART_SUMMARY_TARGET = _runtime_cfg_int("SMART_SUMMARY_TARGET", 800)           # æ‘˜è¦ç›®æ ‡é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
SUMMARY_CACHE_ENABLED = _runtime_cfg_bool("SUMMARY_CACHE_ENABLED", True)       # å¯ç”¨æ‘˜è¦ç¼“å­˜
MAX_TOKENS_SUMMARY = _runtime_cfg_int("MAX_TOKENS_SUMMARY", 500)               # æ‘˜è¦ç”Ÿæˆæœ€å¤§ token


# ============ æ™ºèƒ½æ–‡æ¡£æ‘˜è¦å®ç° ============

def get_document_hash(content: str) -> str:
    """è®¡ç®—æ–‡æ¡£å†…å®¹çš„hashå€¼ï¼Œç”¨äºæ‘˜è¦ç¼“å­˜"""
    import hashlib
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]


def get_cached_summary(doc_hash: str) -> Optional[str]:
    """è·å–ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦"""
    if not SUMMARY_CACHE_ENABLED:
        return None

    cache_file = SUMMARIES_DIR / f"{doc_hash}.txt"
    if cache_file.exists():
        try:
            summary = cache_file.read_text(encoding='utf-8')
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦: {doc_hash}")
            return summary
        except Exception as e:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸  è¯»å–æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")
    return None


def save_summary_cache(doc_hash: str, summary: str) -> None:
    """ä¿å­˜æ–‡æ¡£æ‘˜è¦åˆ°ç¼“å­˜"""
    if not SUMMARY_CACHE_ENABLED:
        return

    cache_file = SUMMARIES_DIR / f"{doc_hash}.txt"
    try:
        cache_file.write_text(summary, encoding='utf-8')
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ’¾ æ‘˜è¦å·²ç¼“å­˜: {doc_hash}")
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  ä¿å­˜æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")


def summarize_document(content: str, doc_name: str = "æ–‡æ¡£", topic: str = "") -> tuple[str, bool]:
    """
    æ™ºèƒ½æ–‡æ¡£æ‘˜è¦ç”Ÿæˆï¼ˆç¬¬ä¸‰é˜¶æ®µä¼˜åŒ–æ ¸å¿ƒåŠŸèƒ½ï¼‰

    å½“æ–‡æ¡£è¿‡é•¿æ—¶ï¼Œä½¿ç”¨AIç”Ÿæˆä¿ç•™å…³é”®ä¿¡æ¯çš„æ‘˜è¦ï¼Œè€Œéç®€å•æˆªæ–­ã€‚

    Args:
        content: æ–‡æ¡£åŸå§‹å†…å®¹
        doc_name: æ–‡æ¡£åç§°ï¼ˆç”¨äºæç¤ºï¼‰
        topic: è®¿è°ˆä¸»é¢˜ï¼ˆç”¨äºç”Ÿæˆæ›´ç›¸å…³çš„æ‘˜è¦ï¼‰

    Returns:
        tuple[str, bool]: (å¤„ç†åçš„å†…å®¹, æ˜¯å¦ç”Ÿæˆäº†æ‘˜è¦)
    """
    original_length = len(content)

    # å¦‚æœæ–‡æ¡£é•¿åº¦æœªè¶…è¿‡é˜ˆå€¼ï¼Œç›´æ¥è¿”å›åŸæ–‡
    if original_length <= SMART_SUMMARY_THRESHOLD:
        return content, False

    # å¦‚æœæœªå¯ç”¨æ™ºèƒ½æ‘˜è¦æˆ–æ²¡æœ‰AIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç®€å•æˆªæ–­
    if not ENABLE_SMART_SUMMARY or not claude_client:
        truncated = content[:MAX_DOC_LENGTH]
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“„ æ–‡æ¡£ {doc_name} ä½¿ç”¨ç®€å•æˆªæ–­: {original_length} -> {MAX_DOC_LENGTH} å­—ç¬¦")
        return truncated, False

    # æ£€æŸ¥ç¼“å­˜
    doc_hash = get_document_hash(content)
    cached = get_cached_summary(doc_hash)
    if cached:
        return cached, True

    # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
    if ENABLE_DEBUG_LOG:
        print(f"ğŸ¤– ä¸ºæ–‡æ¡£ {doc_name} ç”Ÿæˆæ™ºèƒ½æ‘˜è¦: {original_length} -> ~{SMART_SUMMARY_TARGET} å­—ç¬¦")

    # æ„å»ºæ‘˜è¦ç”Ÿæˆprompt
    summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ‘˜è¦ã€‚

## è¦æ±‚
1. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨ {SMART_SUMMARY_TARGET} å­—ç¬¦ä»¥å†…
2. ä¿ç•™æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ã€æ ¸å¿ƒè§‚ç‚¹å’Œé‡è¦æ•°æ®
3. å¦‚æœæ–‡æ¡£ä¸"{topic}"ä¸»é¢˜ç›¸å…³ï¼Œä¼˜å…ˆä¿ç•™ä¸ä¸»é¢˜ç›¸å…³çš„å†…å®¹
4. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€ï¼Œé¿å…å†—ä½™
5. ä¿æŒä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹

## æ–‡æ¡£åç§°
{doc_name}

## æ–‡æ¡£å†…å®¹
{content[:8000]}

## è¾“å‡ºæ ¼å¼
ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ "æ‘˜è¦ï¼š"ç­‰å‰ç¼€ã€‚"""

    try:
        import time
        start_time = time.time()

        response = claude_client.messages.create(
            model=resolve_model_name(call_type="summary"),
            max_tokens=MAX_TOKENS_SUMMARY,
            timeout=60.0,  # æ‘˜è¦ç”Ÿæˆç”¨è¾ƒçŸ­è¶…æ—¶
            messages=[{"role": "user", "content": summary_prompt}]
        )

        response_time = time.time() - start_time
        summary = extract_message_text(response)
        if not summary:
            raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«æ‘˜è¦æ–‡æœ¬")

        # è®°å½•metrics
        metrics_collector.record_api_call(
            call_type="doc_summary",
            prompt_length=len(summary_prompt),
            response_time=response_time,
            success=True,
            timeout=False,
            max_tokens=MAX_TOKENS_SUMMARY
        )

        # ä¿å­˜åˆ°ç¼“å­˜
        save_summary_cache(doc_hash, summary)

        if ENABLE_DEBUG_LOG:
            print(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ: {original_length} -> {len(summary)} å­—ç¬¦ ({response_time:.1f}s)")

        return summary, True

    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æˆªæ–­: {e}")

        # è®°å½•å¤±è´¥çš„metrics
        metrics_collector.record_api_call(
            call_type="doc_summary",
            prompt_length=len(summary_prompt) if 'summary_prompt' in locals() else 0,
            response_time=0,
            success=False,
            timeout="timeout" in str(e).lower(),
            error_msg=str(e),
            max_tokens=MAX_TOKENS_SUMMARY
        )

        # å›é€€åˆ°ç®€å•æˆªæ–­
        return content[:MAX_DOC_LENGTH], False


def process_document_for_context(doc: dict, remaining_length: int, topic: str = "") -> tuple[str, str, int, bool]:
    """
    å¤„ç†æ–‡æ¡£ä»¥ç”¨äºä¸Šä¸‹æ–‡ï¼ˆç»Ÿä¸€çš„æ–‡æ¡£å¤„ç†å…¥å£ï¼‰

    Args:
        doc: æ–‡æ¡£å­—å…¸ï¼ŒåŒ…å« name å’Œ content
        remaining_length: å‰©ä½™å¯ç”¨é•¿åº¦
        topic: è®¿è°ˆä¸»é¢˜

    Returns:
        tuple[str, str, int, bool]: (æ–‡æ¡£å, å¤„ç†åçš„å†…å®¹, ä½¿ç”¨çš„é•¿åº¦, æ˜¯å¦è¢«æ‘˜è¦/æˆªæ–­)
    """
    doc_name = doc.get('name', 'æ–‡æ¡£')
    content = doc.get('content', '')

    if not content:
        return doc_name, '', 0, False

    original_length = len(content)
    max_allowed = min(MAX_DOC_LENGTH, remaining_length)

    # å¦‚æœæ–‡æ¡£å¾ˆçŸ­ï¼ˆä¸è¶…è¿‡æ‘˜è¦é˜ˆå€¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    if original_length <= SMART_SUMMARY_THRESHOLD:
        # ä½†å¦‚æœè¶…è¿‡max_allowedï¼Œä»éœ€æˆªæ–­
        if original_length > max_allowed:
            return doc_name, content[:max_allowed], max_allowed, True
        return doc_name, content, original_length, False

    # æ–‡æ¡£è¶…è¿‡æ‘˜è¦é˜ˆå€¼ï¼Œå°è¯•æ™ºèƒ½æ‘˜è¦
    if ENABLE_SMART_SUMMARY:
        processed_content, is_summarized = summarize_document(content, doc_name, topic)

        # å¦‚æœæ‘˜è¦åä»ç„¶è¿‡é•¿ï¼Œå†æˆªæ–­
        if len(processed_content) > max_allowed:
            processed_content = processed_content[:max_allowed]

        return doc_name, processed_content, len(processed_content), True

    # æœªå¯ç”¨æ™ºèƒ½æ‘˜è¦ï¼Œç®€å•æˆªæ–­
    truncated = content[:max_allowed]
    return doc_name, truncated, len(truncated), True


def generate_history_summary(session: dict, exclude_recent: int = 5) -> Optional[str]:
    """
    ç”Ÿæˆå†å²è®¿è°ˆè®°å½•çš„æ‘˜è¦

    Args:
        session: ä¼šè¯æ•°æ®
        exclude_recent: æ’é™¤æœ€è¿‘Næ¡è®°å½•ï¼ˆè¿™äº›ä¼šä¿ç•™å®Œæ•´å†…å®¹ï¼‰

    Returns:
        æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœæ— éœ€æ‘˜è¦åˆ™è¿”å› None
    """
    interview_log = session.get("interview_log", [])

    # å¦‚æœè®°å½•å¤ªå°‘ï¼Œä¸éœ€è¦æ‘˜è¦
    if len(interview_log) <= exclude_recent:
        return None

    # è·å–éœ€è¦æ‘˜è¦çš„å†å²è®°å½•
    history_logs = interview_log[:-exclude_recent] if exclude_recent > 0 else interview_log

    if not history_logs:
        return None

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ‘˜è¦
    cached_summary = session.get("context_summary", {})
    cached_count = cached_summary.get("log_count", 0)

    # å¦‚æœç¼“å­˜çš„æ‘˜è¦è¦†ç›–äº†ç›¸åŒæ•°é‡çš„è®°å½•ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
    if cached_count == len(history_logs) and cached_summary.get("text"):
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ï¼ˆè¦†ç›– {cached_count} æ¡è®°å½•ï¼‰")
        return cached_summary["text"]

    # éœ€è¦ç”Ÿæˆæ–°æ‘˜è¦
    if not claude_client:
        # æ—  AI æ—¶ä½¿ç”¨ç®€å•æ‘˜è¦
        return _generate_simple_summary(history_logs, session)

    # æ„å»ºæ‘˜è¦ç”Ÿæˆ prompt
    summary_prompt = _build_summary_prompt(session.get("topic", ""), history_logs, session)

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ—œï¸ æ­£åœ¨ç”Ÿæˆå†å²æ‘˜è¦ï¼ˆ{len(history_logs)} æ¡è®°å½•ï¼‰...")

        summary_text = call_claude(summary_prompt, max_tokens=300, call_type="summary")

        if summary_text:
            if ENABLE_DEBUG_LOG:
                print(f"âœ… å†å²æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary_text)} å­—ç¬¦")
            return summary_text
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")

    # å¤±è´¥æ—¶å›é€€åˆ°ç®€å•æ‘˜è¦
    return _generate_simple_summary(history_logs, session)


def _build_summary_prompt(topic: str, logs: list, session: dict = None) -> str:
    """æ„å»ºæ‘˜è¦ç”Ÿæˆçš„ prompt"""
    # è·å–ç»´åº¦ä¿¡æ¯
    summary_dim_info = get_dimension_info_for_session(session) if session else DIMENSION_INFO

    # æŒ‰ç»´åº¦æ•´ç†
    by_dim = {}
    for log in logs:
        dim = log.get("dimension", "other")
        if dim not in by_dim:
            by_dim[dim] = []
        by_dim[dim].append(log)

    logs_text = ""
    for dim, dim_logs in by_dim.items():
        dim_name = summary_dim_info.get(dim, {}).get("name", dim)
        logs_text += f"\nã€{dim_name}ã€‘\n"
        for log in dim_logs:
            logs_text += f"Q: {log['question'][:80]}\nA: {log['answer'][:100]}\n"

    return f"""è¯·å°†ä»¥ä¸‹è®¿è°ˆè®°å½•å‹ç¼©ä¸ºç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ç‚¹ã€‚

è®¿è°ˆä¸»é¢˜ï¼š{topic}

è®¿è°ˆè®°å½•ï¼š
{logs_text}

è¦æ±‚ï¼š
1. æŒ‰ç»´åº¦æ•´ç†å…³é”®ä¿¡æ¯
2. æ¯ä¸ªç»´åº¦ç”¨1-2å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹
3. ä¿ç•™å…·ä½“çš„æ•°æ®ã€æŒ‡æ ‡ã€é€‰æ‹©
4. æ€»é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜

æ‘˜è¦ï¼š"""


def _generate_simple_summary(logs: list, session: dict = None) -> str:
    """ç”Ÿæˆç®€å•æ‘˜è¦ï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    simple_sum_dim_info = get_dimension_info_for_session(session) if session else DIMENSION_INFO
    by_dim = {}
    for log in logs:
        dim = log.get("dimension", "other")
        dim_name = simple_sum_dim_info.get(dim, {}).get("name", dim)
        if dim_name not in by_dim:
            by_dim[dim_name] = []
        # åªä¿ç•™ç­”æ¡ˆçš„å…³é”®éƒ¨åˆ†
        answer = log.get("answer", "")[:50]
        by_dim[dim_name].append(answer)

    parts = []
    for dim_name, answers in by_dim.items():
        parts.append(f"ã€{dim_name}ã€‘: {'; '.join(answers[:3])}")

    return " | ".join(parts)


def update_context_summary(session: dict, session_file) -> None:
    """
    æ›´æ–°ä¼šè¯çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆåœ¨æäº¤å›ç­”åè°ƒç”¨ï¼‰

    åªæœ‰å½“å†å²è®°å½•è¶…è¿‡é˜ˆå€¼æ—¶æ‰ç”Ÿæˆæ‘˜è¦
    """
    interview_log = session.get("interview_log", [])

    # æœªè¶…è¿‡é˜ˆå€¼ï¼Œä¸éœ€è¦æ‘˜è¦
    if len(interview_log) < SUMMARY_THRESHOLD:
        return

    # è®¡ç®—éœ€è¦æ‘˜è¦çš„è®°å½•æ•°
    history_count = len(interview_log) - CONTEXT_WINDOW_SIZE
    if history_count <= 0:
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ‘˜è¦
    cached_summary = session.get("context_summary", {})
    if cached_summary.get("log_count", 0) >= history_count:
        return  # ç¼“å­˜ä»ç„¶æœ‰æ•ˆ

    # ç”Ÿæˆæ–°æ‘˜è¦
    history_logs = interview_log[:history_count]

    if claude_client:
        summary_prompt = _build_summary_prompt(session.get("topic", ""), history_logs, session)
        try:
            summary_text = call_claude(summary_prompt, max_tokens=300, call_type="summary")
            if summary_text:
                session["context_summary"] = {
                    "text": summary_text,
                    "log_count": history_count,
                    "updated_at": get_utc_now()
                }
                # ä¿å­˜æ›´æ–°
                session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")
                if ENABLE_DEBUG_LOG:
                    print(f"ğŸ“ å·²æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆè¦†ç›– {history_count} æ¡å†å²è®°å½•ï¼‰")
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦å¤±è´¥: {e}")
    else:
        # æ—  AI æ—¶ä½¿ç”¨ç®€å•æ‘˜è¦
        session["context_summary"] = {
            "text": _generate_simple_summary(history_logs, session),
            "log_count": history_count,
            "updated_at": get_utc_now()
        }
        session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")


# ============ æ™ºèƒ½è¿½é—®è¯„ä¼° ============

# ç»´åº¦è¿½é—®æ•æ„Ÿåº¦ï¼ˆè¶Šé«˜è¶Šå®¹æ˜“è§¦å‘è¿½é—®ï¼‰
DIMENSION_FOLLOW_UP_SENSITIVITY = {
    "customer_needs": 0.8,       # å®¢æˆ·éœ€æ±‚æœ€éœ€è¦æ·±æŒ–
    "business_process": 0.6,     # ä¸šåŠ¡æµç¨‹éœ€è¦ä¸€å®šæ·±åº¦
    "tech_constraints": 0.5,     # æŠ€æœ¯çº¦æŸç›¸å¯¹æ˜ç¡®
    "project_constraints": 0.4,  # é¡¹ç›®çº¦æŸé€šå¸¸è¾ƒç›´æ¥
}

# ============ è¿½é—®ä¼˜åŒ–ç³»ç»Ÿé…ç½® ============

# è®¿è°ˆæ¨¡å¼é…ç½®
INTERVIEW_MODES = {
    "quick": {
        "name": "å¿«é€Ÿæ¨¡å¼",
        "formal_questions_per_dim": 2,
        "max_formal_questions_per_dim": 2,
        "follow_up_budget_per_dim": 2,
        "total_follow_up_budget": 8,
        "max_questions_per_formal": 1,  # æ¯ä¸ªæ­£å¼é—®é¢˜æœ€å¤šè¿½é—®æ¬¡æ•°
        "estimated_questions": "12-16"
    },
    "standard": {
        "name": "æ ‡å‡†æ¨¡å¼",
        "formal_questions_per_dim": 3,
        "max_formal_questions_per_dim": 3,
        "follow_up_budget_per_dim": 4,
        "total_follow_up_budget": 16,
        "max_questions_per_formal": 2,
        "estimated_questions": "20-28"
    },
    "deep": {
        "name": "æ·±åº¦æ¨¡å¼",
        "formal_questions_per_dim": 4,
        "max_formal_questions_per_dim": 4,
        "follow_up_budget_per_dim": 6,
        "total_follow_up_budget": 24,
        "max_questions_per_formal": 3,
        "estimated_questions": "28-40"
    }
}

# è®¿è°ˆæ¨¡å¼ V2 é…ç½®ï¼ˆæ·±åº¦å¢å¼ºï¼‰
INTERVIEW_MODES_V2 = {
    "quick": {
        "formal_questions_per_dim": 2,
        "max_formal_questions_per_dim": 3,
        "follow_up_budget_per_dim": 3,
        "total_follow_up_budget": 10,
        "max_questions_per_formal": 1,
        "estimated_questions": "14-20",
        "quality_thresholds": {
            "coverage": 0.65,
            "depth": 0.45,
            "volume": 0.35,
            "high": 0.65,
            "medium": 0.5,
            "low": 0.35,
        },
    },
    "standard": {
        "formal_questions_per_dim": 3,
        "max_formal_questions_per_dim": 4,
        "follow_up_budget_per_dim": 5,
        "total_follow_up_budget": 18,
        "max_questions_per_formal": 2,
        "estimated_questions": "24-34",
        "quality_thresholds": {
            "coverage": 0.8,
            "depth": 0.6,
            "volume": 0.45,
            "high": 0.8,
            "medium": 0.65,
            "low": 0.45,
        },
    },
    "deep": {
        "formal_questions_per_dim": 4,
        "max_formal_questions_per_dim": 6,
        "follow_up_budget_per_dim": 8,
        "total_follow_up_budget": 30,
        "max_questions_per_formal": 3,
        "estimated_questions": "34-52",
        "quality_thresholds": {
            "coverage": 0.9,
            "depth": 0.72,
            "volume": 0.6,
            "high": 0.9,
            "medium": 0.75,
            "low": 0.6,
        },
    },
}

# é»˜è®¤æ¨¡å¼
DEFAULT_INTERVIEW_MODE = "standard"

# è¿½é—®ç¡¬è§¦å‘ä¿¡å·ï¼ˆV2ï¼‰
HARD_FOLLOW_UP_SIGNALS = {
    "vague_expression",
    "generic_answer",
    "option_only",
    "contradiction_detected",
}

# ç–²åŠ³åº¦ä¿¡å·æƒé‡
FATIGUE_SIGNALS = {
    "consecutive_short": {
        "description": "è¿ç»­ 3 ä¸ªå›ç­”å°‘äº 30 å­—ç¬¦",
        "threshold": 3,
        "weight": 0.3
    },
    "option_only_streak": {
        "description": "è¿ç»­ 3 æ¬¡åªé€‰é€‰é¡¹ä¸è¡¥å……",
        "threshold": 3,
        "weight": 0.25
    },
    "same_dimension_too_long": {
        "description": "åŒä¸€ç»´åº¦å·²é—® 8+ é—®é¢˜",
        "threshold": 8,
        "weight": 0.25
    },
    "total_questions_high": {
        "description": "æ€»é—®é¢˜æ•°è¶…è¿‡ 25",
        "threshold": 25,
        "weight": 0.2
    }
}

# ä¿¡æ¯é¥±å’Œåº¦é˜ˆå€¼
SATURATION_THRESHOLDS = {
    "high": 0.8,       # é«˜é¥±å’Œåº¦ï¼Œåœæ­¢è¿½é—®
    "medium": 0.6,     # ä¸­ç­‰é¥±å’Œåº¦ï¼Œæœ€å¤šå†è¿½é—®1æ¬¡
    "low": 0.4         # ä½é¥±å’Œåº¦ï¼Œæ­£å¸¸è¿½é—®
}


def get_mode_identifier(session: dict) -> str:
    """è·å–ä¼šè¯æ¨¡å¼IDï¼ˆåšå®¹é”™ï¼‰ã€‚"""
    if not isinstance(session, dict):
        return DEFAULT_INTERVIEW_MODE
    mode = session.get("interview_mode", DEFAULT_INTERVIEW_MODE)
    if mode not in INTERVIEW_MODES:
        return DEFAULT_INTERVIEW_MODE
    return mode


def get_mode_saturation_thresholds(session: dict) -> dict:
    """è·å–å½“å‰æ¨¡å¼çš„é¥±å’Œåº¦é˜ˆå€¼ã€‚"""
    mode_config = get_interview_mode_config(session)
    quality = mode_config.get("quality_thresholds") or {}
    return {
        "high": quality.get("high", SATURATION_THRESHOLDS["high"]),
        "medium": quality.get("medium", SATURATION_THRESHOLDS["medium"]),
        "low": quality.get("low", SATURATION_THRESHOLDS["low"]),
    }


def get_interview_mode_config(session: dict) -> dict:
    """è·å–ä¼šè¯çš„è®¿è°ˆæ¨¡å¼é…ç½®"""
    mode = get_mode_identifier(session)
    base_config = dict(INTERVIEW_MODES.get(mode, INTERVIEW_MODES[DEFAULT_INTERVIEW_MODE]))
    v2_override = INTERVIEW_MODES_V2.get(mode, {})
    return {**base_config, **v2_override}


def get_interview_mode_display_config(mode: str) -> dict:
    """è·å–æŸä¸ªæ¨¡å¼çš„å±•ç¤ºé…ç½®ï¼ˆç”¨äºå‰ç«¯UIæ¸²æŸ“ï¼‰ã€‚"""
    base = dict(INTERVIEW_MODES.get(mode, INTERVIEW_MODES[DEFAULT_INTERVIEW_MODE]))
    v2_override = INTERVIEW_MODES_V2.get(mode, {})
    return {**base, **v2_override}


def calculate_dimension_coverage(session: dict, dimension: str) -> int:
    """è®¡ç®—ç»´åº¦è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼‰"""
    formal_count = len([log for log in session.get("interview_log", [])
                       if log.get("dimension") == dimension and not log.get("is_follow_up", False)])
    mode_config = get_interview_mode_config(session)
    required_questions = mode_config.get("max_formal_questions_per_dim", mode_config.get("formal_questions_per_dim", 3))
    if required_questions <= 0:
        return 100
    return min(100, int(formal_count / required_questions * 100))


def get_follow_up_budget_status(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—è¿½é—®é¢„ç®—ä½¿ç”¨æƒ…å†µ

    Returns:
        {
            "total_used": int,           # å·²ä½¿ç”¨çš„æ€»è¿½é—®æ¬¡æ•°
            "total_budget": int,         # æ€»é¢„ç®—
            "dimension_used": int,       # å½“å‰ç»´åº¦å·²ä½¿ç”¨è¿½é—®æ¬¡æ•°
            "dimension_budget": int,     # å½“å‰ç»´åº¦é¢„ç®—
            "current_question_used": int, # å½“å‰æ­£å¼é—®é¢˜å·²è¿½é—®æ¬¡æ•°
            "current_question_budget": int, # å½“å‰æ­£å¼é—®é¢˜è¿½é—®é¢„ç®—
            "can_follow_up": bool,       # æ˜¯å¦è¿˜èƒ½è¿½é—®
            "budget_exhausted_reason": str or None  # é¢„ç®—è€—å°½åŸå› 
        }
    """
    mode_config = get_interview_mode_config(session)
    interview_log = session.get("interview_log", [])

    # è®¡ç®—æ€»è¿½é—®æ¬¡æ•°
    total_follow_ups = len([log for log in interview_log if log.get("is_follow_up", False)])
    total_budget = mode_config["total_follow_up_budget"]

    # è®¡ç®—å½“å‰ç»´åº¦çš„è¿½é—®æ¬¡æ•°
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]
    dim_follow_ups = len([log for log in dim_logs if log.get("is_follow_up", False)])
    dim_budget = mode_config["follow_up_budget_per_dim"]

    # è®¡ç®—å½“å‰æ­£å¼é—®é¢˜çš„è¿½é—®æ¬¡æ•°
    # æ‰¾åˆ°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜çš„ç´¢å¼•
    formal_indices = [i for i, log in enumerate(dim_logs) if not log.get("is_follow_up", False)]
    if formal_indices:
        last_formal_idx = formal_indices[-1]
        # ç»Ÿè®¡è¿™ä¸ªæ­£å¼é—®é¢˜ä¹‹åçš„è¿½é—®æ•°
        current_question_follow_ups = len([
            log for log in dim_logs[last_formal_idx + 1:]
            if log.get("is_follow_up", False)
        ])
    else:
        current_question_follow_ups = 0
    current_question_budget = mode_config["max_questions_per_formal"]

    # åˆ¤æ–­æ˜¯å¦èƒ½ç»§ç»­è¿½é—®
    can_follow_up = True
    budget_exhausted_reason = None

    if total_follow_ups >= total_budget:
        can_follow_up = False
        budget_exhausted_reason = "total_budget_exhausted"
    elif dim_follow_ups >= dim_budget:
        can_follow_up = False
        budget_exhausted_reason = "dimension_budget_exhausted"
    elif current_question_follow_ups >= current_question_budget:
        can_follow_up = False
        budget_exhausted_reason = "question_budget_exhausted"

    return {
        "total_used": total_follow_ups,
        "total_budget": total_budget,
        "dimension_used": dim_follow_ups,
        "dimension_budget": dim_budget,
        "current_question_used": current_question_follow_ups,
        "current_question_budget": current_question_budget,
        "can_follow_up": can_follow_up,
        "budget_exhausted_reason": budget_exhausted_reason
    }


def calculate_dimension_saturation(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—ç»´åº¦çš„ä¿¡æ¯é¥±å’Œåº¦

    Returns:
        {
            "saturation_score": float,   # 0-1 é¥±å’Œåº¦åˆ†æ•°
            "coverage_score": float,     # å…³é”®æ–¹é¢è¦†ç›–åº¦
            "depth_score": float,        # ä¿¡æ¯æ·±åº¦
            "volume_score": float,       # ä¿¡æ¯é‡
            "covered_aspects": list,     # å·²è¦†ç›–çš„å…³é”®æ–¹é¢
            "level": str                 # "high", "medium", "low"
        }
    """
    interview_log = session.get("interview_log", [])
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]

    if not dim_logs:
        return {
            "saturation_score": 0,
            "coverage_score": 0,
            "depth_score": 0,
            "volume_score": 0,
            "covered_aspects": [],
            "level": "low"
        }

    session_dim_info = get_dimension_info_for_session(session)
    dim_info = session_dim_info.get(dimension, {})
    key_aspects = dim_info.get("key_aspects", [])

    # 1. ä¿¡æ¯è¦†ç›–åº¦ï¼šæ£€æŸ¥å…³é”®æ–¹é¢æ˜¯å¦è¢«æåŠ
    all_answers = " ".join([log.get("answer", "") for log in dim_logs])
    all_questions = " ".join([log.get("question", "") for log in dim_logs])
    combined_text = all_answers + all_questions

    covered_aspects = []
    for aspect in key_aspects:
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨é—®ç­”ä¸­
        if aspect in combined_text:
            covered_aspects.append(aspect)
        else:
            # æ£€æŸ¥ç›¸å…³è¯
            aspect_keywords = {
                "æ ¸å¿ƒç—›ç‚¹": ["ç—›ç‚¹", "é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "å›°æ‰°"],
                "æœŸæœ›ä»·å€¼": ["ä»·å€¼", "æ”¶ç›Š", "æ•ˆæœ", "ç›®æ ‡", "æœŸæœ›"],
                "ä½¿ç”¨åœºæ™¯": ["åœºæ™¯", "æƒ…å†µ", "ä½¿ç”¨", "åº”ç”¨", "ä½•æ—¶"],
                "ç”¨æˆ·è§’è‰²": ["ç”¨æˆ·", "è§’è‰²", "äººå‘˜", "è°", "ä½¿ç”¨è€…"],
                "å…³é”®æµç¨‹": ["æµç¨‹", "æ­¥éª¤", "ç¯èŠ‚", "è¿‡ç¨‹"],
                "è§’è‰²åˆ†å·¥": ["åˆ†å·¥", "èŒè´£", "è´Ÿè´£", "éƒ¨é—¨"],
                "è§¦å‘äº‹ä»¶": ["è§¦å‘", "å¼€å§‹", "å¯åŠ¨", "ä½•æ—¶"],
                "å¼‚å¸¸å¤„ç†": ["å¼‚å¸¸", "é”™è¯¯", "å¤±è´¥", "ä¾‹å¤–"],
                "éƒ¨ç½²æ–¹å¼": ["éƒ¨ç½²", "äº‘", "æœ¬åœ°", "æœåŠ¡å™¨"],
                "ç³»ç»Ÿé›†æˆ": ["é›†æˆ", "å¯¹æ¥", "æ¥å£", "ç³»ç»Ÿ"],
                "æ€§èƒ½è¦æ±‚": ["æ€§èƒ½", "å“åº”", "å¹¶å‘", "é€Ÿåº¦"],
                "å®‰å…¨åˆè§„": ["å®‰å…¨", "åˆè§„", "æƒé™", "åŠ å¯†"],
                "é¢„ç®—èŒƒå›´": ["é¢„ç®—", "è´¹ç”¨", "æˆæœ¬", "ä»·æ ¼"],
                "æ—¶é—´èŠ‚ç‚¹": ["æ—¶é—´", "æœŸé™", "å‘¨æœŸ", "ä½•æ—¶"],
                "èµ„æºé™åˆ¶": ["èµ„æº", "äººåŠ›", "å›¢é˜Ÿ", "é™åˆ¶"],
                "ä¼˜å…ˆçº§": ["ä¼˜å…ˆ", "é‡è¦", "ç´§æ€¥", "å…ˆå"]
            }
            for keyword in aspect_keywords.get(aspect, []):
                if keyword in combined_text:
                    covered_aspects.append(aspect)
                    break

    coverage_score = len(covered_aspects) / len(key_aspects) if key_aspects else 0

    # 2. ä¿¡æ¯æ·±åº¦ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡åŒ–ã€å…·ä½“åœºæ™¯ã€å¯¹æ¯”ç­‰æ·±åº¦ä¿¡å·
    depth_signals = 0
    # æ£€æŸ¥æ•°å­—ï¼ˆé‡åŒ–ä¿¡æ¯ï¼‰
    if any(c.isdigit() for c in all_answers):
        depth_signals += 1
    # æ£€æŸ¥å…·ä½“åœºæ™¯æè¿°ï¼ˆåŒ…å«"æ¯”å¦‚"ã€"ä¾‹å¦‚"ã€"å½“...æ—¶"ç­‰ï¼‰
    scenario_keywords = ["æ¯”å¦‚", "ä¾‹å¦‚", "å½“", "å¦‚æœ", "åœºæ™¯", "æƒ…å†µä¸‹"]
    if any(kw in all_answers for kw in scenario_keywords):
        depth_signals += 1
    # æ£€æŸ¥å¯¹æ¯”æˆ–é€‰æ‹©ï¼ˆ"è€Œä¸æ˜¯"ã€"ä¼˜å…ˆ"ã€"ç›¸æ¯”"ï¼‰
    comparison_keywords = ["è€Œä¸æ˜¯", "ä¼˜å…ˆ", "ç›¸æ¯”", "æ›´é‡è¦", "é¦–å…ˆ"]
    if any(kw in all_answers for kw in comparison_keywords):
        depth_signals += 1
    # æ£€æŸ¥åŸå› è¯´æ˜ï¼ˆ"å› ä¸º"ã€"ç”±äº"ã€"æ‰€ä»¥"ï¼‰
    reason_keywords = ["å› ä¸º", "ç”±äº", "æ‰€ä»¥", "åŸå› æ˜¯"]
    if any(kw in all_answers for kw in reason_keywords):
        depth_signals += 1
    # æ£€æŸ¥å¤šç‚¹å›ç­”
    if "ï¼›" in all_answers or "ã€" in all_answers:
        depth_signals += 1

    depth_score = min(1.0, depth_signals / 5)

    # 3. ä¿¡æ¯é‡ï¼šåŸºäºæ€»å­—ç¬¦æ•°
    total_chars = sum(len(log.get("answer", "")) for log in dim_logs)
    # æœŸæœ›æ¯ä¸ªç»´åº¦è‡³å°‘æ”¶é›† 300 å­—ç¬¦çš„æœ‰æ•ˆä¿¡æ¯
    volume_score = min(1.0, total_chars / 300)

    # ç»¼åˆé¥±å’Œåº¦
    saturation_score = coverage_score * 0.4 + depth_score * 0.3 + volume_score * 0.3

    thresholds = get_mode_saturation_thresholds(session)

    # ç¡®å®šé¥±å’Œåº¦çº§åˆ«
    if saturation_score >= thresholds["high"]:
        level = "high"
    elif saturation_score >= thresholds["medium"]:
        level = "medium"
    else:
        level = "low"

    return {
        "saturation_score": round(saturation_score, 2),
        "coverage_score": round(coverage_score, 2),
        "depth_score": round(depth_score, 2),
        "volume_score": round(volume_score, 2),
        "covered_aspects": covered_aspects,
        "level": level
    }


def calculate_user_fatigue(session: dict, dimension: str) -> dict:
    """
    è®¡ç®—ç”¨æˆ·ç–²åŠ³åº¦

    Returns:
        {
            "fatigue_score": float,      # 0-1 ç–²åŠ³åº¦åˆ†æ•°
            "detected_signals": list,    # æ£€æµ‹åˆ°çš„ç–²åŠ³ä¿¡å·
            "sensitivity_modifier": float, # è¿½é—®æ•æ„Ÿåº¦è°ƒæ•´ç³»æ•° (0.5-1.0)
            "should_force_progress": bool  # æ˜¯å¦åº”è¯¥å¼ºåˆ¶æ¨è¿›
        }
    """
    interview_log = session.get("interview_log", [])
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]

    detected_signals = []
    fatigue_score = 0

    # 1. æ£€æŸ¥è¿ç»­ç®€çŸ­å›ç­”
    recent_answers = [log.get("answer", "") for log in interview_log[-5:]]
    short_count = sum(1 for ans in recent_answers if len(ans.strip()) < 30)
    if short_count >= FATIGUE_SIGNALS["consecutive_short"]["threshold"]:
        detected_signals.append("consecutive_short")
        fatigue_score += FATIGUE_SIGNALS["consecutive_short"]["weight"]

    # 2. æ£€æŸ¥è¿ç»­åªé€‰é€‰é¡¹
    recent_logs = interview_log[-5:]
    option_only_count = 0
    for log in recent_logs:
        options = log.get("options", [])
        answer = log.get("answer", "")
        if options and answer in options and len(answer) < 40:
            option_only_count += 1
    if option_only_count >= FATIGUE_SIGNALS["option_only_streak"]["threshold"]:
        detected_signals.append("option_only_streak")
        fatigue_score += FATIGUE_SIGNALS["option_only_streak"]["weight"]

    # 3. æ£€æŸ¥åŒä¸€ç»´åº¦é—®é¢˜è¿‡å¤š
    if len(dim_logs) >= FATIGUE_SIGNALS["same_dimension_too_long"]["threshold"]:
        detected_signals.append("same_dimension_too_long")
        fatigue_score += FATIGUE_SIGNALS["same_dimension_too_long"]["weight"]

    # 4. æ£€æŸ¥æ€»é—®é¢˜æ•°
    if len(interview_log) >= FATIGUE_SIGNALS["total_questions_high"]["threshold"]:
        detected_signals.append("total_questions_high")
        fatigue_score += FATIGUE_SIGNALS["total_questions_high"]["weight"]

    fatigue_score = min(1.0, fatigue_score)

    # è®¡ç®—æ•æ„Ÿåº¦è°ƒæ•´ç³»æ•°ï¼ˆç–²åŠ³åº¦è¶Šé«˜ï¼Œè¿½é—®æ•æ„Ÿåº¦è¶Šä½ï¼‰
    # å½“ fatigue_score = 0 æ—¶ï¼Œmodifier = 1.0
    # å½“ fatigue_score = 1 æ—¶ï¼Œmodifier = 0.5
    sensitivity_modifier = 1.0 - (fatigue_score * 0.5)

    # åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶æ¨è¿›
    should_force_progress = fatigue_score >= 0.8

    return {
        "fatigue_score": round(fatigue_score, 2),
        "detected_signals": detected_signals,
        "sensitivity_modifier": round(sensitivity_modifier, 2),
        "should_force_progress": should_force_progress
    }


def get_dimension_missing_aspects(session: dict, dimension: str) -> list:
    """è·å–å½“å‰ç»´åº¦å°šæœªè¦†ç›–çš„å…³é”®æ–¹é¢ã€‚"""
    saturation = calculate_dimension_saturation(session, dimension)
    covered = set(saturation.get("covered_aspects", []))
    dim_info = get_dimension_info_for_session(session).get(dimension, {})
    key_aspects = dim_info.get("key_aspects", [])
    return [aspect for aspect in key_aspects if aspect and aspect not in covered]


def get_follow_up_round_for_dimension_logs(dim_logs: list) -> int:
    """è®¡ç®—å½“å‰ç»´åº¦æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜çš„è¿½é—®è½®æ¬¡ã€‚"""
    formal_indices = [i for i, log in enumerate(dim_logs) if not log.get("is_follow_up", False)]
    if not formal_indices:
        return 0

    last_formal_idx = formal_indices[-1]
    chained = 0
    for log in dim_logs[last_formal_idx + 1:]:
        if log.get("is_follow_up", False):
            chained += 1
        else:
            break
    return chained


def has_pending_forced_follow_up(session: dict, dimension: str) -> bool:
    """åˆ¤æ–­å½“å‰ç»´åº¦æ˜¯å¦å­˜åœ¨å¾…æ‰§è¡Œçš„å¼ºåˆ¶è¿½é—®ã€‚"""
    dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
    if not dim_logs:
        return False

    last_log = dim_logs[-1]
    if last_log.get("is_follow_up", False):
        return False

    if last_log.get("hard_triggered") and not last_log.get("user_skip_follow_up", False):
        return True

    return False


def evaluate_answer_quality(eval_result: dict, answer: str, is_follow_up: bool, follow_up_round: int) -> dict:
    """å°†å›ç­”è¯„ä¼°ç»“æœæ˜ å°„ä¸ºè´¨é‡åˆ†æ•°ä¸æ ‡ç­¾ã€‚"""
    signals = eval_result.get("signals", [])
    hard_triggered = eval_result.get("hard_triggered", False)

    quality_score = 0.5
    answer_len = len((answer or "").strip())

    if answer_len >= 120:
        quality_score += 0.2
    elif answer_len >= 80:
        quality_score += 0.1

    if eval_result.get("has_numbers"):
        quality_score += 0.1

    scenario_keywords = ["æ¯”å¦‚", "ä¾‹å¦‚", "å½“", "å¦‚æœ", "åœºæ™¯", "æ¡ˆä¾‹"]
    if any(keyword in (answer or "") for keyword in scenario_keywords):
        quality_score += 0.1

    negative_penalty = {
        "too_short": 0.2,
        "vague_expression": 0.25,
        "generic_answer": 0.3,
        "option_only": 0.2,
        "no_quantification": 0.1,
        "single_selection": 0.1,
        "contradiction_detected": 0.25,
    }
    quality_score -= sum(negative_penalty.get(signal, 0.05) for signal in signals)

    if is_follow_up and follow_up_round >= 1 and not hard_triggered:
        quality_score += 0.05

    quality_score = max(0.0, min(1.0, quality_score))

    quality_signals = []
    if quality_score >= 0.75:
        quality_signals.append("high_quality")
    if eval_result.get("has_numbers"):
        quality_signals.append("quantified")
    if answer_len >= 80:
        quality_signals.append("detailed")

    for signal in signals:
        if signal not in quality_signals:
            quality_signals.append(signal)

    return {
        "quality_score": round(quality_score, 2),
        "quality_signals": quality_signals,
        "hard_triggered": bool(hard_triggered),
    }


def evaluate_dimension_completion_v2(session: dict, dimension: str) -> dict:
    """ç»´åº¦å®Œæˆé—¨ç¦ï¼ˆV2ï¼‰ï¼šé¢˜é‡ + è´¨é‡ + å¼ºåˆ¶è¿½é—®ã€‚"""
    mode_config = get_interview_mode_config(session)
    dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
    formal_count = len([log for log in dim_logs if not log.get("is_follow_up", False)])
    min_formal = mode_config.get("formal_questions_per_dim", 3)
    max_formal = mode_config.get("max_formal_questions_per_dim", min_formal)

    budget_status = get_follow_up_budget_status(session, dimension)
    saturation = calculate_dimension_saturation(session, dimension)
    fatigue = calculate_user_fatigue(session, dimension)

    quality_thresholds = mode_config.get("quality_thresholds") or {}
    coverage_threshold = quality_thresholds.get("coverage", 0.8)
    depth_threshold = quality_thresholds.get("depth", 0.6)
    volume_threshold = quality_thresholds.get("volume", 0.45)

    missing_aspects = get_dimension_missing_aspects(session, dimension)
    follow_up_round = get_follow_up_round_for_dimension_logs(dim_logs)
    pending_forced_follow_up = has_pending_forced_follow_up(session, dimension)

    snapshot = {
        "formal_count": formal_count,
        "min_formal": min_formal,
        "max_formal": max_formal,
        "coverage": saturation.get("coverage_score", 0),
        "depth": saturation.get("depth_score", 0),
        "volume": saturation.get("volume_score", 0),
        "saturation": saturation.get("saturation_score", 0),
        "missing_aspects": missing_aspects,
        "follow_up_round": follow_up_round,
        "pending_forced_follow_up": pending_forced_follow_up,
        "budget_status": budget_status,
        "fatigue": fatigue,
    }

    # 1. å¾…æ‰§è¡Œå¼ºåˆ¶è¿½é—®
    if pending_forced_follow_up and follow_up_round < 1:
        return {
            "can_complete": False,
            "reason": "å­˜åœ¨å¾…æ‰§è¡Œçš„å…³é”®è¿½é—®ï¼Œéœ€è¦è‡³å°‘è¿½é—®1æ¬¡",
            "action": "continue",
            "quality_warning": False,
            "snapshot": snapshot,
        }

    # 2. æœ€ä½æ­£å¼é¢˜æœªè¾¾æ ‡
    if formal_count < min_formal:
        return {
            "can_complete": False,
            "reason": f"æ­£å¼é—®é¢˜æ•°é‡ä¸è¶³ï¼ˆ{formal_count}/{min_formal}ï¼‰",
            "action": "continue",
            "quality_warning": False,
            "snapshot": snapshot,
        }

    meets_quality = (
        saturation.get("coverage_score", 0) >= coverage_threshold
        and saturation.get("depth_score", 0) >= depth_threshold
        and saturation.get("volume_score", 0) >= volume_threshold
        and not missing_aspects
    )

    # 3. è¾¾åˆ°è´¨é‡é—¨æ§›å¯å®Œæˆ
    if meets_quality and not pending_forced_follow_up:
        return {
            "can_complete": True,
            "reason": "è¾¾åˆ°ç»´åº¦è´¨é‡é—¨æ§›",
            "action": "complete",
            "quality_warning": False,
            "snapshot": snapshot,
        }

    # 4. è¾¾åˆ°ä¸Šé™ä¿æŠ¤æˆ–é¢„ç®—è€—å°½ï¼Œå¼ºåˆ¶å®Œæˆ
    budget_exhausted = not budget_status.get("can_follow_up", True)
    reached_upper_bound = formal_count >= max_formal
    if reached_upper_bound or budget_exhausted:
        return {
            "can_complete": True,
            "reason": "è¾¾åˆ°ä¸Šé™ä¿æŠ¤ï¼Œå…è®¸å¼ºåˆ¶å®Œæˆ",
            "action": "force_complete",
            "quality_warning": not meets_quality,
            "snapshot": snapshot,
        }

    # 5. ç»§ç»­æé—®
    return {
        "can_complete": False,
        "reason": "è´¨é‡é—¨æ§›æœªè¾¾æˆï¼Œç»§ç»­æé—®",
        "action": "continue",
        "quality_warning": False,
        "snapshot": snapshot,
    }


def should_follow_up_comprehensive(session: dict, dimension: str,
                                    rule_based_result: dict) -> dict:
    """
    ç»¼åˆå†³ç­–æ˜¯å¦åº”è¯¥è¿½é—®

    æ•´åˆï¼šè§„åˆ™è¯„ä¼° + é¢„ç®—æ£€æŸ¥ + é¥±å’Œåº¦ + ç–²åŠ³åº¦

    Returns:
        {
            "should_follow_up": bool,
            "reason": str,
            "budget_status": dict,
            "saturation": dict,
            "fatigue": dict,
            "decision_factors": list  # å½±å“å†³ç­–çš„å› ç´ 
        }
    """
    decision_factors = []

    # 1. æ£€æŸ¥é¢„ç®—
    budget_status = get_follow_up_budget_status(session, dimension)
    if not budget_status["can_follow_up"]:
        reason_map = {
            "total_budget_exhausted": "ä¼šè¯è¿½é—®é¢„ç®—å·²ç”¨å®Œ",
            "dimension_budget_exhausted": "å½“å‰ç»´åº¦è¿½é—®é¢„ç®—å·²ç”¨å®Œ",
            "question_budget_exhausted": "å½“å‰é—®é¢˜è¿½é—®æ¬¡æ•°å·²è¾¾ä¸Šé™"
        }
        return {
            "should_follow_up": False,
            "reason": reason_map.get(budget_status["budget_exhausted_reason"], "é¢„ç®—å·²ç”¨å®Œ"),
            "budget_status": budget_status,
            "saturation": {},
            "fatigue": {},
            "decision_factors": ["budget_exhausted"]
        }

    fatigue = calculate_user_fatigue(session, dimension)

    # V2: ç¡¬è§¦å‘ä¿¡å·ä¼˜å…ˆï¼ˆåœ¨ç–²åŠ³ä¿æŠ¤ä¹‹å¤–ï¼‰
    hard_triggered = bool(rule_based_result.get("hard_triggered", False))
    if hard_triggered and fatigue.get("fatigue_score", 0) < 0.9:
        return {
            "should_follow_up": True,
            "reason": rule_based_result.get("reason") or "æ£€æµ‹åˆ°å…³é”®æ¨¡ç³Š/å†²çªï¼Œéœ€è¦è‡³å°‘è¿½é—®ä¸€æ¬¡",
            "budget_status": budget_status,
            "saturation": calculate_dimension_saturation(session, dimension),
            "fatigue": fatigue,
            "decision_factors": ["hard_signal_forced_follow_up"]
        }

    # 2. æ£€æŸ¥é¥±å’Œåº¦
    saturation = calculate_dimension_saturation(session, dimension)
    if saturation["level"] == "high":
        decision_factors.append("high_saturation")
        return {
            "should_follow_up": False,
            "reason": f"ä¿¡æ¯å·²å……åˆ†ï¼ˆé¥±å’Œåº¦ {saturation['saturation_score']:.0%}ï¼‰",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": None,
            "decision_factors": decision_factors
        }

    # 3. æ£€æŸ¥ç–²åŠ³åº¦
    if fatigue["should_force_progress"]:
        decision_factors.append("user_fatigue")
        return {
            "should_follow_up": False,
            "reason": "æ£€æµ‹åˆ°ç”¨æˆ·ç–²åŠ³ï¼Œæš‚åœè¿½é—®",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": fatigue,
            "decision_factors": decision_factors
        }

    # 4. åŸºäºè§„åˆ™è¯„ä¼°ç»“æœï¼Œä½†åº”ç”¨ç–²åŠ³åº¦è°ƒæ•´
    original_needs_follow_up = rule_based_result.get("needs_follow_up", False)

    if not original_needs_follow_up:
        return {
            "should_follow_up": False,
            "reason": "å›ç­”å·²å……åˆ†",
            "budget_status": budget_status,
            "saturation": saturation,
            "fatigue": fatigue,
            "decision_factors": ["sufficient_answer"]
        }

    # ä¸­ç­‰é¥±å’Œåº¦æ—¶é™åˆ¶è¿½é—®
    if saturation["level"] == "medium":
        # æ£€æŸ¥æ˜¯å¦å·²ç»è¿½é—®è¿‡
        if budget_status["current_question_used"] >= 1:
            decision_factors.append("medium_saturation_limit")
            return {
                "should_follow_up": False,
                "reason": "ä¿¡æ¯æ¥è¿‘å……åˆ†ï¼Œä¸å†è¿½é—®",
                "budget_status": budget_status,
                "saturation": saturation,
                "fatigue": fatigue,
                "decision_factors": decision_factors
            }

    # ç–²åŠ³åº¦è¾ƒé«˜æ—¶ï¼Œæé«˜è¿½é—®é—¨æ§›
    if fatigue["fatigue_score"] >= 0.5:
        decision_factors.append("elevated_threshold")
        # åªæœ‰éå¸¸æ˜æ˜¾éœ€è¦è¿½é—®çš„æƒ…å†µæ‰è¿½é—®
        if len(rule_based_result.get("signals", [])) < 2:
            return {
                "should_follow_up": False,
                "reason": "ç”¨æˆ·å¯èƒ½ç–²åŠ³ï¼Œè·³è¿‡éå…³é”®è¿½é—®",
                "budget_status": budget_status,
                "saturation": saturation,
                "fatigue": fatigue,
                "decision_factors": decision_factors
            }

    # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œå¯ä»¥è¿½é—®
    decision_factors.append("rule_based_follow_up")
    return {
        "should_follow_up": True,
        "reason": rule_based_result.get("reason", "éœ€è¦è¿›ä¸€æ­¥äº†è§£"),
        "budget_status": budget_status,
        "saturation": saturation,
        "fatigue": fatigue,
        "decision_factors": decision_factors
    }


def score_assessment_answer(session: dict, dimension: str, question: str, answer: str) -> Optional[float]:
    """
    ä¸ºè¯„ä¼°åœºæ™¯çš„å›ç­”æ‰“åˆ†ï¼ˆ1-5åˆ†ï¼‰

    Args:
        session: ä¼šè¯æ•°æ®
        dimension: ç»´åº¦ID
        question: é—®é¢˜
        answer: å›ç­”

    Returns:
        float: 1.0-5.0 çš„åˆ†æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if not claude_client:
        return None

    # è·å–ç»´åº¦é…ç½®
    scenario_config = session.get("scenario_config", {})
    dim_config = None
    for dim in scenario_config.get("dimensions", []):
        if dim.get("id") == dimension:
            dim_config = dim
            break

    if not dim_config:
        return None

    # æ„å»ºè¯„åˆ†æ ‡å‡†æ–‡æœ¬
    scoring_criteria = dim_config.get("scoring_criteria", {})
    criteria_text = "\n".join(
        f"  {score}åˆ†: {desc}"
        for score, desc in sorted(scoring_criteria.items(), key=lambda x: int(x[0]), reverse=True)
    )

    if not criteria_text:
        criteria_text = """  5åˆ†: å›ç­”éå¸¸ä¼˜ç§€ï¼Œå±•ç°æ·±åšä¸“ä¸šèƒ½åŠ›
  4åˆ†: å›ç­”è‰¯å¥½ï¼Œæœ‰æ¸…æ™°çš„æ€è·¯å’Œè§è§£
  3åˆ†: å›ç­”åŸºæœ¬åˆæ ¼ï¼Œä½†ç¼ºä¹æ·±åº¦
  2åˆ†: å›ç­”æœ‰æ˜æ˜¾ä¸è¶³æˆ–åå·®
  1åˆ†: å›ç­”å¾ˆå·®ï¼Œæ— æ³•å±•ç°ç›¸å…³èƒ½åŠ›"""

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šé¢è¯•å®˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ†æ ‡å‡†ï¼Œå¯¹å€™é€‰äººçš„å›ç­”è¿›è¡Œè¯„åˆ†ã€‚

ã€è¯„ä¼°ç»´åº¦ã€‘{dim_config.get("name", dimension)}
ã€ç»´åº¦è¯´æ˜ã€‘{dim_config.get("description", "")}

ã€è¯„åˆ†æ ‡å‡†ã€‘
{criteria_text}

ã€é¢è¯•é—®é¢˜ã€‘
{question}

ã€å€™é€‰äººå›ç­”ã€‘
{answer}

è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œåªè¿”å›ä¸€ä¸ªæ•°å­—ï¼ˆ1-5ä¹‹é—´çš„æ•´æ•°æˆ–å°æ•°ï¼Œå¦‚ 3.5ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼š"""

    try:
        response = claude_client.messages.create(
            model=resolve_model_name(call_type="assessment_score"),
            max_tokens=96,  # MiniMax åœ¨ä½ token ä¸‹å®¹æ˜“åªè¿”å› thinkingï¼Œé€‚å½“æé«˜ç¨³å®šæ€§
            timeout=15.0,
            messages=[{"role": "user", "content": prompt}]
        )
        raw = extract_message_text(response)
        if not raw:
            raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«è¯„åˆ†æ–‡æœ¬")
        # æå–æ•°å­—
        import re
        match = re.search(r'(\d+\.?\d*)', raw)
        if match:
            score = float(match.group(1))
            return max(1.0, min(5.0, score))  # é™åˆ¶åœ¨ 1-5 èŒƒå›´
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ è¯„åˆ†å¤±è´¥: {e}")

    return None


def evaluate_answer_depth(question: str, answer: str, dimension: str,
                          options: list = None, is_follow_up: bool = False) -> dict:
    """
    è¯„ä¼°å›ç­”æ·±åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿½é—®

    ä¸‰å±‚åˆ¤æ–­ï¼š
    1. æ˜ç¡®éœ€è¦è¿½é—®ï¼ˆå›ç­”å¤ªå¼±ï¼‰
    2. æ˜ç¡®ä¸éœ€è¦è¿½é—®ï¼ˆå›ç­”å·²å……åˆ†ï¼‰
    3. å»ºè®®AIè¯„ä¼°ï¼ˆäº¤ç»™AIåœ¨ç”Ÿæˆä¸‹ä¸€é¢˜æ—¶åˆ¤æ–­ï¼‰

    Returns:
        {
            "needs_follow_up": bool,       # è§„åˆ™å±‚åˆ¤æ–­ç»“æœ
            "suggest_ai_eval": bool,       # æ˜¯å¦å»ºè®®AIå†æ¬¡è¯„ä¼°
            "reason": str or None,         # è¿½é—®åŸå› 
            "signals": list                # æ£€æµ‹åˆ°çš„ä¿¡å·
        }
    """
    signals = []
    answer_stripped = answer.strip()
    answer_len = len(answer_stripped)
    sensitivity = DIMENSION_FOLLOW_UP_SENSITIVITY.get(dimension, 0.5)

    # ---- ç¬¬ä¸€å±‚ï¼šæ˜ç¡®éœ€è¦è¿½é—®çš„æƒ…å†µ ----

    # 1. å›ç­”è¿‡çŸ­ï¼ˆæ ¹æ®ç»´åº¦æ•æ„Ÿåº¦è°ƒæ•´é˜ˆå€¼ï¼‰
    short_threshold = int(20 + sensitivity * 20)  # å®¢æˆ·éœ€æ±‚36å­—ç¬¦ï¼Œé¡¹ç›®çº¦æŸ28å­—ç¬¦
    if answer_len < short_threshold:
        signals.append("too_short")

    # 2. æ¨¡ç³Šè¡¨è¾¾æ£€æµ‹ï¼ˆæ‰©å±•è¯åº“ï¼‰
    vague_indicators = [
        # ä¸ç¡®å®šç±»
        "çœ‹æƒ…å†µ", "ä¸ä¸€å®š", "å¯èƒ½", "æˆ–è®¸", "å¤§æ¦‚", "å·®ä¸å¤š", "åˆ°æ—¶å€™",
        "å†è¯´", "è¿˜æ²¡æƒ³å¥½", "ä¸ç¡®å®š", "çœ‹å…·ä½“", "æ ¹æ®æƒ…å†µ", "å¾…å®š",
        "ä»¥åå†è¯´", "æš‚æ—¶ä¸æ¸…æ¥š", "ç›®å‰è¿˜ä¸å¥½è¯´",
        # ç¬¼ç»Ÿç±»
        "éƒ½å¯ä»¥", "éƒ½è¡Œ", "éšä¾¿", "æ— æ‰€è°“", "å·®ä¸å¤š", "ä¸€èˆ¬",
        # å›é¿ç±»
        "ä¸å¤ªäº†è§£", "æ²¡æƒ³è¿‡", "ä¸çŸ¥é“", "è¯´ä¸å¥½", "å¾ˆéš¾è¯´",
    ]
    matched_vague = [v for v in vague_indicators if v in answer_stripped]
    if matched_vague:
        signals.append("vague_expression")

    # 3. å®Œå…¨åŒ¹é…æ³›æ³›å›ç­”
    generic_answers = [
        "å¥½çš„", "æ˜¯çš„", "å¯ä»¥", "æ²¡é—®é¢˜", "éœ€è¦", "åº”è¯¥è¦",
        "å¯¹", "å—¯", "è¡Œ", "åŒæ„", "æ²¡æœ‰", "ä¸éœ€è¦",
    ]
    if answer_stripped in generic_answers:
        signals.append("generic_answer")

    # 4. ä»…é€‰æ‹©äº†é¢„è®¾é€‰é¡¹æ²¡æœ‰è¡¥å……ï¼ˆç­”æ¡ˆç­‰äºæŸä¸ªé€‰é¡¹åŸæ–‡ï¼‰
    if options:
        is_exact_option = answer_stripped in options
        # å•é€‰ä¸”ç­”æ¡ˆå°±æ˜¯é€‰é¡¹åŸæ–‡ï¼Œç¼ºä¹è‡ªå·±çš„æ€è€ƒ
        if is_exact_option and answer_len < 40:
            signals.append("option_only")

    # 5. ç¼ºä¹é‡åŒ–ä¿¡æ¯ï¼ˆå¯¹æŸäº›ç»´åº¦é‡è¦ï¼‰
    has_numbers = any(c.isdigit() for c in answer_stripped)
    quantitative_dimensions = ["tech_constraints", "project_constraints"]
    if dimension in quantitative_dimensions and not has_numbers and answer_len < 60:
        signals.append("no_quantification")

    # 6. å¤šé€‰ä½†åªé€‰äº†ä¸€ä¸ªï¼ˆå¯èƒ½éœ€è¦è¡¥å……ï¼‰
    if options and "ï¼›" not in answer_stripped and len(options) >= 3:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šé€‰é¢˜ä½†åªé€‰äº†ä¸€ä¸ª
        selected_count = sum(1 for opt in options if opt in answer_stripped)
        if selected_count <= 1 and answer_len < 30:
            signals.append("single_selection")

    # 7. è½»é‡çŸ›ç›¾æ£€æµ‹
    contradiction_pairs = [
        ("éœ€è¦", "ä¸éœ€è¦"),
        ("å¯ä»¥", "ä¸å¯ä»¥"),
        ("æ”¯æŒ", "ä¸æ”¯æŒ"),
        ("å·²ç»", "è¿˜æ²¡"),
        ("æœ‰", "æ²¡æœ‰"),
        ("å¿…é¡»", "å¯é€‰"),
    ]
    contradiction_detected = any(
        left in answer_stripped and right in answer_stripped
        for left, right in contradiction_pairs
    )
    if contradiction_detected:
        signals.append("contradiction_detected")

    # ---- ç¬¬äºŒå±‚ï¼šåˆ¤æ–­æ˜¯å¦æ˜ç¡®ä¸éœ€è¦è¿½é—® ----

    # å›ç­”è¶³å¤Ÿè¯¦ç»†ï¼Œä¸éœ€è¦è¿½é—®
    sufficient_signals = []
    if answer_len > 80:
        sufficient_signals.append("detailed_answer")
    if "ï¼›" in answer_stripped and answer_len > 40:
        sufficient_signals.append("multi_point_answer")
    if has_numbers and answer_len > 30:
        sufficient_signals.append("quantified_answer")

    # ---- ç¬¬ä¸‰å±‚ï¼šç»¼åˆåˆ¤æ–­ ----

    # è®¡ç®—è¿½é—®å¾—åˆ†ï¼ˆä¿¡å·è¶Šå¤šè¶Šéœ€è¦è¿½é—®ï¼‰
    signal_weights = {
        "too_short": 0.4,
        "vague_expression": 0.5,
        "generic_answer": 0.8,
        "option_only": 0.3,
        "no_quantification": 0.2,
        "single_selection": 0.2,
        "contradiction_detected": 0.6,
    }
    follow_up_score = sum(signal_weights.get(s, 0.1) for s in signals)
    follow_up_score *= sensitivity  # åº”ç”¨ç»´åº¦æ•æ„Ÿåº¦

    # å‡å»å……åˆ†åº¦ä¿¡å·
    sufficient_weights = {
        "detailed_answer": 0.5,
        "multi_point_answer": 0.3,
        "quantified_answer": 0.2,
    }
    sufficient_score = sum(sufficient_weights.get(s, 0) for s in sufficient_signals)
    follow_up_score -= sufficient_score

    hard_triggered = any(signal in HARD_FOLLOW_UP_SIGNALS for signal in signals)

    # åˆ¤æ–­ç»“æœ
    if follow_up_score >= 0.4:
        # æ˜ç¡®éœ€è¦è¿½é—®
        reason = _build_follow_up_reason(signals)
        return {"needs_follow_up": True, "suggest_ai_eval": False,
                "reason": reason, "signals": signals,
                "hard_triggered": hard_triggered,
                "has_numbers": has_numbers,
                "follow_up_score": round(follow_up_score, 2)}
    elif follow_up_score >= 0.15 and not sufficient_signals:
        # è¾¹ç•Œæƒ…å†µï¼Œå»ºè®®è®©AIè¯„ä¼°
        reason = _build_follow_up_reason(signals)
        return {"needs_follow_up": False, "suggest_ai_eval": True,
                "reason": reason, "signals": signals,
                "hard_triggered": hard_triggered,
                "has_numbers": has_numbers,
                "follow_up_score": round(follow_up_score, 2)}
    else:
        # ä¸éœ€è¦è¿½é—®
        return {"needs_follow_up": False, "suggest_ai_eval": False,
                "reason": None, "signals": signals,
                "hard_triggered": hard_triggered,
                "has_numbers": has_numbers,
                "follow_up_score": round(follow_up_score, 2)}


def _build_follow_up_reason(signals: list) -> str:
    """æ ¹æ®æ£€æµ‹åˆ°çš„ä¿¡å·æ„å»ºè¿½é—®åŸå› """
    reason_map = {
        "too_short": "å›ç­”è¿‡äºç®€çŸ­ï¼Œéœ€è¦è¡¥å……å…·ä½“ç»†èŠ‚",
        "vague_expression": "å›ç­”åŒ…å«æ¨¡ç³Šè¡¨è¿°ï¼Œéœ€è¦æ˜ç¡®å…·ä½“è¦æ±‚",
        "generic_answer": "å›ç­”è¿‡äºç¬¼ç»Ÿï¼Œéœ€è¦æ·±å…¥äº†è§£å…·ä½“éœ€æ±‚",
        "option_only": "ä»…é€‰æ‹©äº†é¢„è®¾é€‰é¡¹ï¼Œéœ€è¦äº†è§£å…·ä½“åœºæ™¯å’Œè€ƒé‡",
        "no_quantification": "ç¼ºå°‘é‡åŒ–æŒ‡æ ‡ï¼Œéœ€è¦æ˜ç¡®å…·ä½“æ•°æ®è¦æ±‚",
        "single_selection": "åªé€‰æ‹©äº†å•ä¸€é€‰é¡¹ï¼Œéœ€è¦äº†è§£æ˜¯å¦è¿˜æœ‰å…¶ä»–éœ€æ±‚",
        "contradiction_detected": "å›ç­”ä¸­å­˜åœ¨å‰åå†²çªï¼Œéœ€è¦æ¾„æ¸…çœŸå®çº¦æŸ",
    }
    reasons = [reason_map.get(s, "") for s in signals if s in reason_map]
    return reasons[0] if reasons else "éœ€è¦è¿›ä¸€æ­¥äº†è§£è¯¦ç»†éœ€æ±‚"


def build_interview_prompt(session: dict, dimension: str, all_dim_logs: list,
                           session_id: str = None) -> tuple[str, list, dict]:
    """æ„å»ºè®¿è°ˆ promptï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£ + æ‘˜è¦å‹ç¼© + æ™ºèƒ½è¿½é—®ï¼‰

    Args:
        session: ä¼šè¯æ•°æ®
        dimension: å½“å‰ç»´åº¦
        all_dim_logs: å½“å‰ç»´åº¦çš„æ‰€æœ‰è®¿è°ˆè®°å½•
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºæ›´æ–°æ€è€ƒè¿›åº¦çŠ¶æ€ï¼‰

    Returns:
        tuple[str, list, dict]: (promptå­—ç¬¦ä¸², è¢«æˆªæ–­çš„æ–‡æ¡£åˆ—è¡¨, å†³ç­–å…ƒæ•°æ®)
    """
    topic = session.get("topic", "æœªçŸ¥é¡¹ç›®")
    description = session.get("description")
    # å…¼å®¹æ—§æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ reference_materialsï¼Œå¦åˆ™åˆå¹¶æ—§å­—æ®µ
    reference_materials = session.get("reference_materials", [])
    if not reference_materials:
        reference_materials = session.get("reference_docs", []) + session.get("research_docs", [])
    interview_log = session.get("interview_log", [])
    session_dim_info = get_dimension_info_for_session(session)
    dim_info = session_dim_info.get(dimension, {})

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = [f"å½“å‰è®¿è°ˆä¸»é¢˜ï¼š{topic}"]

    # å¦‚æœæœ‰ä¸»é¢˜æè¿°ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
    if description:
        context_parts.append(f"\nä¸»é¢˜æè¿°ï¼š{description[:500]}")

    # æ·»åŠ å‚è€ƒèµ„æ–™å†…å®¹ï¼ˆä½¿ç”¨æ€»é•¿åº¦é™åˆ¶ + æ™ºèƒ½æ‘˜è¦ï¼‰
    total_doc_length = 0
    truncated_docs = []  # è®°å½•è¢«å¤„ç†çš„æ–‡æ¡£ï¼ˆæ‘˜è¦æˆ–æˆªæ–­ï¼‰
    summarized_docs = []  # è®°å½•ä½¿ç”¨æ™ºèƒ½æ‘˜è¦çš„æ–‡æ¡£
    if reference_materials:
        context_parts.append("\n## å‚è€ƒèµ„æ–™ï¼š")
        for doc in reference_materials:
            if doc.get("content") and total_doc_length < MAX_TOTAL_DOCS:
                remaining = MAX_TOTAL_DOCS - total_doc_length
                original_length = len(doc["content"])

                # ä½¿ç”¨æ™ºèƒ½æ‘˜è¦å¤„ç†æ–‡æ¡£
                doc_name, processed_content, used_length, was_processed = process_document_for_context(
                    doc, remaining, topic
                )

                if processed_content:
                    # æ ¹æ® source æ·»åŠ æ ‡è®°
                    source_marker = "ğŸ”„ " if doc.get("source") == "auto" else ""
                    context_parts.append(f"### {source_marker}{doc_name}")
                    context_parts.append(processed_content)
                    total_doc_length += used_length

                    # è®°å½•å¤„ç†æƒ…å†µ
                    if was_processed:
                        if used_length < original_length * 0.6:  # å¦‚æœå†…å®¹å‡å°‘è¶…è¿‡40%ï¼Œå¯èƒ½æ˜¯æ‘˜è¦
                            summarized_docs.append(f"{doc_name}ï¼ˆåŸ{original_length}å­—ç¬¦ï¼Œæ‘˜è¦è‡³{used_length}å­—ç¬¦ï¼‰")
                        else:
                            truncated_docs.append(f"{doc_name}ï¼ˆåŸ{original_length}å­—ç¬¦ï¼Œæˆªå–{used_length}å­—ç¬¦ï¼‰")

    # æ·»åŠ å¤„ç†æç¤ºï¼ˆè®© AI çŸ¥é“æ–‡æ¡£ä¿¡æ¯ç»è¿‡å¤„ç†ï¼‰
    if summarized_docs:
        context_parts.append(f"\nğŸ“ æ³¨æ„ï¼šä»¥ä¸‹æ–‡æ¡£å·²é€šè¿‡AIç”Ÿæˆæ‘˜è¦ä»¥ä¿ç•™å…³é”®ä¿¡æ¯ï¼š{', '.join(summarized_docs)}")
    if truncated_docs:
        context_parts.append(f"\nâš ï¸ æ³¨æ„ï¼šä»¥ä¸‹æ–‡æ¡£å› é•¿åº¦é™åˆ¶å·²è¢«æˆªæ–­ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œæé—®ï¼š{', '.join(truncated_docs)}")

    # ========== æ™ºèƒ½è”ç½‘æœç´¢ï¼ˆè§„åˆ™é¢„åˆ¤ + AIå†³ç­–ï¼‰ ==========
    # è·å–æœ€è¿‘çš„é—®ç­”è®°å½•ç”¨äº AI åˆ¤æ–­
    recent_qa = interview_log[-3:] if interview_log else []
    will_search, search_query, search_reason = smart_search_decision(topic, dimension, session, recent_qa)

    if will_search and search_query:
        # æ›´æ–°æ€è€ƒçŠ¶æ€åˆ°"æœç´¢"é˜¶æ®µ
        if session_id:
            update_thinking_status(session_id, "searching", has_search=True)

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ” æ‰§è¡Œæœç´¢: {search_query} (åŸå› : {search_reason})")

        search_results = web_search(search_query)

        if search_results:
            context_parts.append("\n## è¡Œä¸šçŸ¥è¯†å‚è€ƒï¼ˆè”ç½‘æœç´¢ï¼‰ï¼š")
            for idx, result in enumerate(search_results[:2], 1):
                if result["type"] == "intent":
                    context_parts.append(f"**{result['content'][:150]}**")
                else:
                    context_parts.append(f"{idx}. **{result.get('title', 'å‚è€ƒä¿¡æ¯')[:40]}**")
                    context_parts.append(f"   {result['content'][:150]}")

    # ========== æ»‘åŠ¨çª—å£ + æ‘˜è¦å‹ç¼© ==========
    if interview_log:
        context_parts.append("\n## å·²æ”¶é›†çš„ä¿¡æ¯ï¼š")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨æ‘˜è¦
        if len(interview_log) > CONTEXT_WINDOW_SIZE:
            # è·å–æˆ–ç”Ÿæˆå†å²æ‘˜è¦
            history_summary = generate_history_summary(session, exclude_recent=CONTEXT_WINDOW_SIZE)
            if history_summary:
                context_parts.append(f"\n### å†å²è®¿è°ˆæ‘˜è¦ï¼ˆå…±{len(interview_log) - CONTEXT_WINDOW_SIZE}æ¡ï¼‰ï¼š")
                context_parts.append(history_summary)
                context_parts.append("\n### æœ€è¿‘é—®ç­”è®°å½•ï¼š")

            # åªä¿ç•™æœ€è¿‘çš„å®Œæ•´è®°å½•
            recent_logs = interview_log[-CONTEXT_WINDOW_SIZE:]
        else:
            recent_logs = interview_log

        # æ·»åŠ å®Œæ•´çš„æœ€è¿‘é—®ç­”è®°å½•
        base_index = len(interview_log) - len(recent_logs)
        for offset, log in enumerate(recent_logs, 1):
            follow_up_mark = " [è¿½é—®]" if log.get("is_follow_up") else ""
            q_number = base_index + offset
            context_parts.append(f"- Q{q_number}: {log['question']}{follow_up_mark}")
            context_parts.append(f"  A: {log['answer']}")
            dim_name = session_dim_info.get(log.get("dimension", ""), {}).get("name", "")
            if dim_name:
                context_parts.append(f"  (ç»´åº¦: {dim_name})")

    # è®¡ç®—æ­£å¼é—®é¢˜æ•°é‡ï¼ˆæ’é™¤è¿½é—®ï¼‰
    formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])
    mode_config = get_interview_mode_config(session)

    # ========== æ™ºèƒ½è¿½é—®åˆ¤æ–­ï¼ˆç»¼åˆé¢„ç®—+é¥±å’Œåº¦+ç–²åŠ³åº¦+è§„åˆ™è¯„ä¼°ï¼‰ ==========
    last_log = None
    should_follow_up = False
    suggest_ai_eval = False
    follow_up_reason = ""
    eval_signals = []
    comprehensive_decision = None
    hard_triggered = False
    missing_aspects = []
    follow_up_round = get_follow_up_round_for_dimension_logs(all_dim_logs)
    remaining_question_follow_up_budget = max(0, mode_config.get("max_questions_per_formal", 1) - follow_up_round)

    if all_dim_logs:
        last_log = all_dim_logs[-1]
        last_answer = last_log.get("answer", "")
        last_question = last_log.get("question", "")
        last_options = last_log.get("options", [])
        last_is_follow_up = last_log.get("is_follow_up", False)

        # ä½¿ç”¨å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°ï¼ˆè§„åˆ™å±‚ï¼‰
        eval_result = evaluate_answer_depth(
            question=last_question,
            answer=last_answer,
            dimension=dimension,
            options=last_options,
            is_follow_up=last_is_follow_up,
        )

        eval_signals = eval_result["signals"]
        hard_triggered = bool(eval_result.get("hard_triggered", False))

        # ä½¿ç”¨ç»¼åˆå†³ç­–å‡½æ•°ï¼ˆæ•´åˆé¢„ç®—ã€é¥±å’Œåº¦ã€ç–²åŠ³åº¦ï¼‰
        comprehensive_decision = should_follow_up_comprehensive(
            session=session,
            dimension=dimension,
            rule_based_result=eval_result
        )

        should_follow_up = comprehensive_decision["should_follow_up"]
        follow_up_reason = comprehensive_decision["reason"] or ""

        # åªæœ‰åœ¨è§„åˆ™å±‚å»ºè®® AI è¯„ä¼°ä¸”ç»¼åˆå†³ç­–å…è®¸è¿½é—®æ—¶ï¼Œæ‰å»ºè®® AI è¯„ä¼°
        suggest_ai_eval = eval_result["suggest_ai_eval"] and comprehensive_decision["should_follow_up"]

        missing_aspects = get_dimension_missing_aspects(session, dimension)

        if ENABLE_DEBUG_LOG:
            budget = comprehensive_decision.get("budget_status", {})
            saturation = comprehensive_decision.get("saturation", {})
            fatigue = comprehensive_decision.get("fatigue", {})
            print(f"ğŸ” è¿½é—®å†³ç­–: should_follow_up={should_follow_up}, reason={follow_up_reason}")
            print(f"   é¢„ç®—: {budget.get('total_used', 0)}/{budget.get('total_budget', 0)} (ç»´åº¦: {budget.get('dimension_used', 0)}/{budget.get('dimension_budget', 0)})")
            if saturation:
                print(f"   é¥±å’Œåº¦: {saturation.get('saturation_score', 0):.0%} ({saturation.get('level', 'unknown')})")
            if fatigue:
                print(f"   ç–²åŠ³åº¦: {fatigue.get('fatigue_score', 0):.0%}, ä¿¡å·: {fatigue.get('detected_signals', [])}")

    # æ„å»º AI è¯„ä¼°æç¤ºï¼ˆå½“è§„åˆ™æœªæ˜ç¡®è§¦å‘ä½†å»ºè®®AIåˆ¤æ–­æ—¶ï¼‰
    ai_eval_guidance = ""
    if suggest_ai_eval and last_log:
        ai_eval_guidance = f"""
## å›ç­”æ·±åº¦è¯„ä¼°

è¯·å…ˆè¯„ä¼°ç”¨æˆ·çš„ä¸Šä¸€ä¸ªå›ç­”æ˜¯å¦éœ€è¦è¿½é—®ï¼š

**ä¸Šä¸€ä¸ªé—®é¢˜**: {last_log.get('question', '')[:100]}
**ç”¨æˆ·å›ç­”**: {last_log.get('answer', '')}
**æ£€æµ‹ä¿¡å·**: {', '.join(eval_signals) if eval_signals else 'æ— æ˜æ˜¾é—®é¢˜'}

åˆ¤æ–­æ ‡å‡†ï¼ˆæ»¡è¶³ä»»ä¸€æ¡å³åº”è¿½é—®ï¼‰ï¼š
1. å›ç­”åªæ˜¯é€‰æ‹©äº†é€‰é¡¹ï¼Œæ²¡æœ‰è¯´æ˜å…·ä½“åœºæ™¯æˆ–åŸå› 
2. ç¼ºå°‘é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚æ—¶é—´ã€æ•°é‡ã€é¢‘ç‡ç­‰ï¼‰
3. å›ç­”æ¯”è¾ƒç¬¼ç»Ÿï¼Œæ²¡æœ‰é’ˆå¯¹æ€§ç»†èŠ‚
4. å¯èƒ½éšè—äº†æ›´æ·±å±‚çš„éœ€æ±‚æˆ–é¡¾è™‘

å¦‚æœåˆ¤æ–­éœ€è¦è¿½é—®ï¼Œè¯·ï¼š
- è®¾ç½® is_follow_up: true
- é’ˆå¯¹ä¸Šä¸€ä¸ªå›ç­”è¿›è¡Œæ·±å…¥æé—®
- é—®é¢˜è¦æ›´å…·ä½“ï¼Œå¼•å¯¼ç”¨æˆ·ç»™å‡ºæ˜ç¡®ç­”æ¡ˆ

å¦‚æœåˆ¤æ–­ä¸éœ€è¦è¿½é—®ï¼Œè¯·ç”Ÿæˆæ–°é—®é¢˜ç»§ç»­è®¿è°ˆã€‚
"""

    blindspot_guidance = ""
    if not should_follow_up:
        min_formal = mode_config.get("formal_questions_per_dim", 3)
        if formal_questions_count >= min_formal and missing_aspects:
            blindspot_guidance = f"""
## ç›²åŒºè¡¥é—®ä¼˜å…ˆï¼ˆå¿…é¡»æ‰§è¡Œï¼‰

å½“å‰ç»´åº¦ä»æœ‰æœªè¦†ç›–å…³é”®æ–¹é¢ï¼š{', '.join(missing_aspects)}

ç”Ÿæˆæ–°é—®é¢˜æ—¶è¯·æ»¡è¶³ï¼š
1. é—®é¢˜å¿…é¡»ç›´æ¥ç‚¹åè‡³å°‘ 1 ä¸ªæœªè¦†ç›–æ–¹é¢
2. ä¸è¦é‡å¤å·²å……åˆ†è¦†ç›–çš„ä¿¡æ¯
3. è‹¥æœ‰å¤šä¸ªç›²åŒºï¼Œä¼˜å…ˆæé—®å½±å“å†³ç­–æœ€å¤§çš„æ–¹é¢
"""

    # æ„å»ºè¿½é—®æ¨¡å¼çš„æç¤º
    follow_up_section = ""
    if should_follow_up:
        follow_up_section = f"""## è¿½é—®æ¨¡å¼ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰

ä¸Šä¸€ä¸ªç”¨æˆ·å›ç­”éœ€è¦è¿½é—®ã€‚åŸå› ï¼š{follow_up_reason}

**ä¸Šä¸€ä¸ªé—®é¢˜**: {last_log.get('question', '')[:100] if last_log else ''}
**ç”¨æˆ·å›ç­”**: {last_log.get('answer', '') if last_log else ''}

è¿½é—®è¦æ±‚ï¼š
1. å¿…é¡»è®¾ç½® is_follow_up: true
2. é’ˆå¯¹ä¸Šä¸€ä¸ªå›ç­”è¿›è¡Œæ·±å…¥æé—®ï¼Œä¸è¦è·³åˆ°æ–°è¯é¢˜
3. è¿½é—®é—®é¢˜è¦æ›´å…·ä½“ã€æ›´æœ‰é’ˆå¯¹æ€§
4. å¼•å¯¼ç”¨æˆ·ç»™å‡ºå…·ä½“çš„åœºæ™¯ã€æ•°æ®ã€æˆ–æ˜ç¡®çš„é€‰æ‹©
5. å¯ä»¥ä½¿ç”¨"æ‚¨æåˆ°çš„XXXï¼Œèƒ½å¦å…·ä½“è¯´æ˜..."è¿™æ ·çš„å¥å¼
"""
    else:
        follow_up_section = """## é—®é¢˜ç”Ÿæˆè¦æ±‚

1. ç”Ÿæˆ 1 ä¸ªé’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œç”¨äºæ”¶é›†è¯¥ç»´åº¦çš„å…³é”®ä¿¡æ¯
2. ä¸ºè¿™ä¸ªé—®é¢˜æä¾› 3-4 ä¸ªå…·ä½“çš„é€‰é¡¹
3. é€‰é¡¹è¦åŸºäºï¼š
   - è®¿è°ˆä¸»é¢˜çš„è¡Œä¸šç‰¹ç‚¹
   - å‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
   - è”ç½‘æœç´¢çš„è¡Œä¸šçŸ¥è¯†ï¼ˆå¦‚æœ‰ï¼‰
   - å·²æ”¶é›†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
4. æ ¹æ®é—®é¢˜æ€§è´¨åˆ¤æ–­æ˜¯å•é€‰è¿˜æ˜¯å¤šé€‰ï¼š
   - å•é€‰åœºæ™¯ï¼šäº’æ–¥é€‰é¡¹ï¼ˆæ˜¯/å¦ï¼‰ã€ä¼˜å…ˆçº§é€‰æ‹©ã€å”¯ä¸€é€‰æ‹©
   - å¤šé€‰åœºæ™¯ï¼šå¯å¹¶å­˜çš„åŠŸèƒ½éœ€æ±‚ã€å¤šä¸ªç—›ç‚¹ã€å¤šç§ç”¨æˆ·è§’è‰²
5. å¦‚æœç”¨æˆ·çš„å›ç­”ä¸å‚è€ƒæ–‡æ¡£å†…å®¹æœ‰å†²çªï¼Œè¦åœ¨é—®é¢˜ä¸­æŒ‡å‡ºå¹¶è¯·æ±‚æ¾„æ¸…
"""

    prompt = f"""**ä¸¥æ ¼è¾“å‡ºè¦æ±‚ï¼šä½ çš„å›å¤å¿…é¡»æ˜¯çº¯ JSON å¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€markdown ä»£ç å—æˆ–å…¶ä»–æ–‡æœ¬ã€‚ç¬¬ä¸€ä¸ªå­—ç¬¦å¿…é¡»æ˜¯ {{ï¼Œæœ€åä¸€ä¸ªå­—ç¬¦å¿…é¡»æ˜¯ }}**

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¿è°ˆå¸ˆï¼Œæ­£åœ¨è¿›è¡Œ"{topic}"çš„è®¿è°ˆã€‚
ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯**æ·±åº¦æŒ–æ˜ç”¨æˆ·çš„çœŸå®éœ€æ±‚**ï¼Œä¸æ»¡è¶³äºè¡¨é¢å›ç­”ã€‚

{chr(10).join(context_parts)}

## å½“å‰ä»»åŠ¡

ä½ ç°åœ¨éœ€è¦é’ˆå¯¹ã€Œ{dim_info.get('name', dimension)}ã€ç»´åº¦æ”¶é›†ä¿¡æ¯ã€‚
è¿™ä¸ªç»´åº¦å…³æ³¨ï¼š{dim_info.get('description', '')}

è¯¥ç»´åº¦å·²æ”¶é›†äº† {formal_questions_count} ä¸ªæ­£å¼é—®é¢˜çš„å›ç­”ï¼Œå…³é”®æ–¹é¢åŒ…æ‹¬ï¼š{', '.join(dim_info.get('key_aspects', []))}
{ai_eval_guidance}
{blindspot_guidance}
{follow_up_section}

å¦‚æœä¿¡æ¯è¶³å¤Ÿï¼Œè¯·åŸºäºå·²æ”¶é›†çš„å›ç­”ç»™å‡ºå¯¹å½“å‰é€‰é¡¹çš„ AI æ¨èï¼Œç”¨äºè¾…åŠ©ç”¨æˆ·å†³ç­–ã€‚è‹¥æ— æ³•æ¨èï¼Œè¯·å°† ai_recommendation è®¾ä¸º nullã€‚

## è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

    {{
        "question": "ä½ çš„é—®é¢˜",
        "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"],
        "multi_select": false,
        "is_follow_up": {'true' if should_follow_up else 'false'},
        "follow_up_reason": {json.dumps(follow_up_reason, ensure_ascii=False) if should_follow_up else 'null'},
        "conflict_detected": false,
        "conflict_description": null,
        "ai_recommendation": {{
            "recommended_options": ["é€‰é¡¹1"],
            "summary": "ä¸€å¥è¯æ¨èç†ç”±",
            "reasons": [
                {{"text": "ç†ç”±1", "evidence": ["Q1", "Q3"]}},
                {{"text": "ç†ç”±2", "evidence": ["Q2"]}}
            ],
            "confidence": "high"
        }}
    }}

å­—æ®µè¯´æ˜ï¼š
- question: å­—ç¬¦ä¸²ï¼Œä½ è¦é—®çš„é—®é¢˜
- options: å­—ç¬¦ä¸²æ•°ç»„ï¼Œ3-4 ä¸ªé€‰é¡¹
- multi_select: å¸ƒå°”å€¼ï¼Œtrue=å¯å¤šé€‰ï¼Œfalse=å•é€‰
- is_follow_up: å¸ƒå°”å€¼ï¼Œtrue=è¿½é—®ï¼ˆé’ˆå¯¹ä¸Šä¸€å›ç­”æ·±å…¥ï¼‰ï¼Œfalse=æ–°é—®é¢˜
- follow_up_reason: å­—ç¬¦ä¸²æˆ– nullï¼Œè¿½é—®æ—¶è¯´æ˜åŸå› 
- conflict_detected: å¸ƒå°”å€¼
- conflict_description: å­—ç¬¦ä¸²æˆ– null
- ai_recommendation: æ¨èå¯¹è±¡æˆ– null
  - recommended_options: æ•°ç»„ï¼ˆå•é€‰æ—¶åªæ”¾ 1 ä¸ªï¼Œå¤šé€‰æ—¶å¯æ”¾å¤šä¸ªï¼‰
  - summary: ä¸€å¥è¯æ¨èç†ç”±ï¼ˆä¸è¶…è¿‡ 25 å­—ï¼‰
  - reasons: 2-3 æ¡ç†ç”±ï¼Œéœ€é™„è¯æ®ç¼–å·ï¼ˆå¦‚ Q1ã€Q3ï¼‰
  - confidence: "high" | "medium" | "low"

å¦‚æœå½“å‰ä¿¡æ¯ä¸è¶³ä»¥åšæ¨èï¼Œè¯·å°† ai_recommendation è®¾ä¸º nullã€‚

**å…³é”®æé†’ï¼š**
- ä¸è¦ä½¿ç”¨ ```json ä»£ç å—æ ‡è®°
- ä¸è¦åœ¨ JSON å‰åæ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—
- ç¡®ä¿ JSON è¯­æ³•å®Œå…¨æ­£ç¡®ï¼ˆæ‰€æœ‰å­—ç¬¦ä¸²ç”¨åŒå¼•å·ï¼Œå¸ƒå°”å€¼ç”¨ true/falseï¼Œç©ºå€¼ç”¨ nullï¼‰
- ä½ çš„æ•´ä¸ªå›å¤å°±æ˜¯è¿™ä¸ª JSON å¯¹è±¡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹
- **é‡è¦**ï¼šis_follow_up çš„å€¼å·²ç”±ç³»ç»Ÿæ ¹æ®é¢„ç®—å’Œé¥±å’Œåº¦é¢„å…ˆå†³å®šï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿è®¾ç½®"""

    decision_meta = {
        "mode": get_mode_identifier(session),
        "follow_up_round": follow_up_round,
        "remaining_question_follow_up_budget": remaining_question_follow_up_budget,
        "hard_triggered": hard_triggered,
        "missing_aspects": missing_aspects,
    }

    return prompt, truncated_docs, decision_meta


def build_assessment_report_prompt(session: dict) -> str:
    """æ„å»ºé¢è¯•è¯„ä¼°æŠ¥å‘Š prompt"""
    topic = session.get("topic", "å€™é€‰äººè¯„ä¼°")
    description = session.get("description", "")
    interview_log = session.get("interview_log", [])
    dimensions = session.get("dimensions", {})
    scenario_config = session.get("scenario_config", {})
    assessment_config = scenario_config.get("assessment", {})

    # è·å–ç»´åº¦é…ç½®
    dim_configs = {d["id"]: d for d in scenario_config.get("dimensions", [])}

    # è®¡ç®—ç»¼åˆè¯„åˆ†
    total_score = 0.0
    total_weight = 0.0
    dim_scores_info = []
    for dim_id, dim_data in dimensions.items():
        dim_config = dim_configs.get(dim_id, {})
        weight = dim_config.get("weight", 0.25)
        score = dim_data.get("score")
        if score is not None:
            total_score += score * weight
            total_weight += weight
            dim_scores_info.append({
                "id": dim_id,
                "name": dim_config.get("name", dim_id),
                "score": score,
                "weight": weight,
                "criteria": dim_config.get("scoring_criteria", {})
            })

    final_score = round(total_score / total_weight, 2) if total_weight > 0 else 0

    # ç¡®å®šæ¨èç­‰çº§
    recommendation_levels = assessment_config.get("recommendation_levels", [])
    recommendation = {"level": "D", "name": "ä¸æ¨è", "color": "#ef4444"}
    for level in sorted(recommendation_levels, key=lambda x: x.get("threshold", 0), reverse=True):
        if final_score >= level.get("threshold", 0):
            recommendation = level
            break

    # æ„å»ºè¯„åˆ†è¡¨æ ¼æ–‡æœ¬
    score_table = "| ç»´åº¦ | å¾—åˆ† | æƒé‡ | åŠ æƒå¾—åˆ† |\n|:---|:---:|:---:|:---:|\n"
    for info in dim_scores_info:
        weighted = round(info["score"] * info["weight"], 2)
        score_table += f"| {info['name']} | {info['score']:.1f} | {info['weight']*100:.0f}% | {weighted:.2f} |\n"
    score_table += f"| **ç»¼åˆå¾—åˆ†** | **{final_score:.2f}** | 100% | **{final_score:.2f}** |"

    # æŒ‰ç»´åº¦æ•´ç†é—®ç­”å’Œè¯„åˆ†
    qa_sections = ""
    for dim_info in dim_scores_info:
        dim_id = dim_info["id"]
        qa_list = [log for log in interview_log if log.get("dimension") == dim_id]
        qa_sections += f"\n### {dim_info['name']}ï¼ˆå¾—åˆ†: {dim_info['score']:.1f}/5.0ï¼‰\n"
        for qa in qa_list:
            qa_sections += f"**Q**: {qa['question']}\n"
            qa_sections += f"**A**: {qa['answer']}\n"
            if qa.get("score"):
                qa_sections += f"*å•é¢˜è¯„åˆ†: {qa['score']:.1f}*\n"
            qa_sections += "\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é¢è¯•å®˜å’Œäººæ‰è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦åŸºäºä»¥ä¸‹è®¿è°ˆè®°å½•ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„é¢è¯•è¯„ä¼°æŠ¥å‘Šã€‚

## è¯„ä¼°ä¸»é¢˜
{topic}
"""

    if description:
        prompt += f"""
## èƒŒæ™¯è¯´æ˜
{description}
"""

    prompt += f"""
## å„ç»´åº¦å¾—åˆ†

{score_table}

## è®¿è°ˆè®°å½•ä¸è¯„åˆ†
{qa_sections}

## æŠ¥å‘Šè¦æ±‚

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„é¢è¯•è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

### 1. å€™é€‰äººæ¦‚è§ˆ
- è¯„ä¼°ä¸»é¢˜
- è¯„ä¼°æ—¶é—´
- ç»¼åˆå¾—åˆ†ï¼š**{final_score:.2f}/5.0**
- æ¨èç­‰çº§ï¼š**{recommendation.get('name', 'å¾…å®š')}** ({recommendation.get('level', 'C')})

### 2. èƒ½åŠ›é›·è¾¾å›¾
ä½¿ç”¨ Mermaid é›·è¾¾å›¾å±•ç¤ºå„ç»´åº¦å¾—åˆ†ï¼ˆå¦‚æœ Mermaid ä¸æ”¯æŒé›·è¾¾å›¾ï¼Œå¯ç”¨å…¶ä»–å¯è§†åŒ–æ–¹å¼æ›¿ä»£ï¼‰ï¼š

**æ³¨æ„**ï¼šç”±äº Mermaid ä¸åŸç”Ÿæ”¯æŒé›·è¾¾å›¾ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆï¼š

```mermaid
xychart-beta
    title "èƒ½åŠ›è¯„ä¼°é›·è¾¾"
    x-axis [{', '.join([f'"{d["name"]}"' for d in dim_scores_info])}]
    y-axis "å¾—åˆ†" 0 --> 5
    bar [{', '.join([str(d["score"]) for d in dim_scores_info])}]
```

### 3. å„ç»´åº¦è¯¦ç»†åˆ†æ
å¯¹æ¯ä¸ªè¯„ä¼°ç»´åº¦è¿›è¡Œè¯¦ç»†åˆ†æï¼š
- è¯¥ç»´åº¦çš„å¾—åˆ†å’Œè¡¨ç°
- å…·ä½“çš„ä¼˜åŠ¿ä½“ç°
- å­˜åœ¨çš„ä¸è¶³æˆ–å¾…æå‡ç‚¹
- å…³é”®è¯æ®ï¼ˆå¼•ç”¨è®¿è°ˆå†…å®¹ï¼‰

### 4. æ ¸å¿ƒä¼˜åŠ¿
æ€»ç»“å€™é€‰äººçš„ 2-3 ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼Œç”¨å…·ä½“äº‹ä¾‹æ”¯æ’‘

### 5. å¾…æå‡é¢†åŸŸ
æŒ‡å‡º 1-2 ä¸ªéœ€è¦æå‡çš„æ–¹é¢ï¼Œç»™å‡ºå…·ä½“å»ºè®®

### 6. æ¨èæ„è§
åŸºäºç»¼åˆè¯„åˆ† **{final_score:.2f}** ç»™å‡ºï¼š
- æ¨èç­‰çº§ï¼š**{recommendation.get('name', 'å¾…å®š')}**
- ç­‰çº§è¯´æ˜ï¼š{recommendation.get('description', '')}
- å½•ç”¨å»ºè®®ï¼ˆè¯¦ç»†è¯´æ˜å½•ç”¨/ä¸å½•ç”¨çš„ç†ç”±ï¼Œä»¥åŠå¦‚æœå½•ç”¨çš„æ³¨æ„äº‹é¡¹ï¼‰

### 7. åç»­å»ºè®®
- å¦‚éœ€è¿›ä¸€æ­¥è¯„ä¼°çš„é—®é¢˜
- å…¥èŒåçš„åŸ¹å…»å»ºè®®ï¼ˆå¦‚æœæ¨èå½•ç”¨ï¼‰

## é‡è¦æé†’
- æ‰€æœ‰åˆ†æå¿…é¡»ä¸¥æ ¼åŸºäºè®¿è°ˆè®°å½•ä¸­çš„å®é™…å†…å®¹
- è¯„åˆ†å·²ç”± AI åœ¨è®¿è°ˆè¿‡ç¨‹ä¸­é€é¢˜æ‰“åˆ†ï¼Œè¯·åŸºäºè¿™äº›è¯„åˆ†è¿›è¡Œåˆ†æ
- å®¢è§‚å…¬æ­£ï¼Œæ—¢è¦æŒ‡å‡ºä¼˜åŠ¿ä¹Ÿè¦æŒ‡å‡ºä¸è¶³
- æŠ¥å‘Šè¦ä¸“ä¸šã€ç»“æ„æ¸…æ™°ã€æœ‰ç†æœ‰æ®
- ä½¿ç”¨ Markdown æ ¼å¼
- æŠ¥å‘Šæœ«å°¾ä½¿ç”¨ç½²åï¼š*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³ç”Ÿæˆ*

è¯·ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šï¼š"""

    return prompt


def build_report_prompt(session: dict) -> str:
    """æ„å»ºæŠ¥å‘Šç”Ÿæˆ prompt"""
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¯„ä¼°ç±»å‹æŠ¥å‘Š
    report_type = session.get("scenario_config", {}).get("report", {}).get("type", "standard")
    if report_type == "assessment":
        return build_assessment_report_prompt(session)

    topic = session.get("topic", "æœªçŸ¥é¡¹ç›®")
    description = session.get("description")  # è·å–ä¸»é¢˜æè¿°
    interview_log = session.get("interview_log", [])
    dimensions = session.get("dimensions", {})
    # å…¼å®¹æ—§æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ reference_materialsï¼Œå¦åˆ™åˆå¹¶æ—§å­—æ®µ
    reference_materials = session.get("reference_materials", [])
    if not reference_materials:
        reference_materials = session.get("reference_docs", []) + session.get("research_docs", [])

    # è·å–ä¼šè¯çš„åŠ¨æ€ç»´åº¦ä¿¡æ¯
    report_dim_info = get_dimension_info_for_session(session)

    # æŒ‰ç»´åº¦æ•´ç†é—®ç­”
    qa_by_dim = {}
    for dim_key in report_dim_info:
        qa_by_dim[dim_key] = [log for log in interview_log if log.get("dimension") == dim_key]

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆï¼Œéœ€è¦åŸºäºä»¥ä¸‹è®¿è°ˆè®°å½•ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„è®¿è°ˆæŠ¥å‘Šã€‚

## è®¿è°ˆä¸»é¢˜
{topic}
"""

    # å¦‚æœæœ‰ä¸»é¢˜æè¿°ï¼Œæ·»åŠ åˆ° prompt ä¸­
    if description:
        prompt += f"""
## ä¸»é¢˜æè¿°
{description}
"""

    prompt += """
## å‚è€ƒèµ„æ–™
"""

    if reference_materials:
        prompt += "ä»¥ä¸‹æ˜¯ç”¨æˆ·æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œè¯·åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶å‚è€ƒè¿™äº›å†…å®¹ï¼š\n\n"
        for doc in reference_materials:
            doc_name = doc.get('name', 'æ–‡æ¡£')
            # æ ¹æ® source æ·»åŠ æ ‡è®°
            source_marker = "ğŸ”„ " if doc.get("source") == "auto" else ""
            prompt += f"### {source_marker}{doc_name}\n"
            if doc.get("content"):
                content = doc["content"]
                original_length = len(content)

                # ä½¿ç”¨æ™ºèƒ½æ‘˜è¦å¤„ç†é•¿æ–‡æ¡£
                if original_length > SMART_SUMMARY_THRESHOLD and ENABLE_SMART_SUMMARY:
                    processed_content, is_summarized = summarize_document(content, doc_name, topic)
                    if is_summarized:
                        prompt += f"{processed_content}\n"
                        prompt += f"*[åŸæ–‡æ¡£ {original_length} å­—ç¬¦ï¼Œå·²é€šè¿‡AIç”Ÿæˆæ‘˜è¦ä¿ç•™å…³é”®ä¿¡æ¯]*\n\n"
                    elif len(processed_content) > MAX_DOC_LENGTH:
                        prompt += f"{processed_content[:MAX_DOC_LENGTH]}\n"
                        prompt += f"*[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ {MAX_DOC_LENGTH} å­—ç¬¦]*\n\n"
                    else:
                        prompt += f"{processed_content}\n\n"
                elif original_length > MAX_DOC_LENGTH:
                    prompt += f"{content[:MAX_DOC_LENGTH]}\n"
                    prompt += f"*[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ {MAX_DOC_LENGTH} å­—ç¬¦]*\n\n"
                else:
                    prompt += f"{content}\n\n"
            else:
                prompt += "*[æ–‡æ¡£å†…å®¹ä¸ºç©º]*\n\n"
    else:
        prompt += "æ— å‚è€ƒèµ„æ–™\n"

    prompt += "\n## è®¿è°ˆè®°å½•\n"

    for dim_key, dim_info in report_dim_info.items():
        prompt += f"\n### {dim_info['name']}\n"
        qa_list = qa_by_dim.get(dim_key, [])
        if qa_list:
            for qa in qa_list:
                prompt += f"**Q**: {qa['question']}\n"
                prompt += f"**A**: {qa['answer']}\n\n"
        else:
            prompt += "*è¯¥ç»´åº¦æš‚æ— æ”¶é›†æ•°æ®*\n"

    prompt += """
## æŠ¥å‘Šè¦æ±‚

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„è®¿è°ˆæŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

1. **è®¿è°ˆæ¦‚è¿°** - åŸºæœ¬ä¿¡æ¯ã€è®¿è°ˆèƒŒæ™¯
2. **éœ€æ±‚æ‘˜è¦** - æ ¸å¿ƒéœ€æ±‚åˆ—è¡¨ã€ä¼˜å…ˆçº§çŸ©é˜µ
3. **è¯¦ç»†éœ€æ±‚åˆ†æ**
   - å®¢æˆ·/ç”¨æˆ·éœ€æ±‚ï¼ˆç—›ç‚¹ã€æœŸæœ›ã€åœºæ™¯ã€è§’è‰²ï¼‰
   - ä¸šåŠ¡æµç¨‹ï¼ˆå…³é”®æµç¨‹ã€å†³ç­–èŠ‚ç‚¹ï¼‰
   - æŠ€æœ¯çº¦æŸï¼ˆéƒ¨ç½²ã€é›†æˆã€å®‰å…¨ï¼‰
   - é¡¹ç›®çº¦æŸï¼ˆé¢„ç®—ã€æ—¶é—´ã€èµ„æºï¼‰
4. **å¯è§†åŒ–åˆ†æ** - ä½¿ç”¨ Mermaid å›¾è¡¨å±•ç¤ºå…³é”®ä¿¡æ¯
5. **æ–¹æ¡ˆå»ºè®®** - åŸºäºéœ€æ±‚çš„å¯è¡Œå»ºè®®
6. **é£é™©è¯„ä¼°** - æ½œåœ¨é£é™©å’Œåº”å¯¹ç­–ç•¥
7. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨** - å…·ä½“çš„è¡ŒåŠ¨é¡¹

**æ³¨æ„**ï¼šä¸éœ€è¦åŒ…å«"é™„å½•"ç« èŠ‚ï¼Œå®Œæ•´çš„è®¿è°ˆè®°å½•ä¼šåœ¨æŠ¥å‘Šç”Ÿæˆåè‡ªåŠ¨è¿½åŠ ã€‚

## Mermaid å›¾è¡¨è§„èŒƒ

è¯·åœ¨æŠ¥å‘Šä¸­åŒ…å«ä»¥ä¸‹ç±»å‹çš„ Mermaid å›¾è¡¨ã€‚**é™¤ quadrantChart å¤–ï¼Œæ‰€æœ‰å›¾è¡¨éƒ½åº”ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ã€‚

### 1. ä¼˜å…ˆçº§çŸ©é˜µï¼ˆå¿…é¡»ï¼Œä¸¤ç§å½¢å¼éƒ½è¦ï¼‰

#### 1.1 è±¡é™å›¾ï¼ˆMermaidï¼‰
ä½¿ç”¨ quadrantChart å±•ç¤ºéœ€æ±‚åœ¨é‡è¦æ€§-ç´§æ€¥æ€§åæ ‡ä¸­çš„ä½ç½®ï¼š

```mermaid
quadrantChart
    title Priority Matrix
    x-axis Low Urgency --> High Urgency
    y-axis Low Importance --> High Importance
    quadrant-1 Do First
    quadrant-2 Schedule
    quadrant-3 Delegate
    quadrant-4 Eliminate

    Requirement1: [0.8, 0.9]
    Requirement2: [0.3, 0.7]
```

**è±¡é™å›¾ä¸­æ–‡å›¾ä¾‹è¯´æ˜**ï¼ˆå¿…é¡»åœ¨è±¡é™å›¾ä¸‹æ–¹æ·»åŠ ï¼‰ï¼š
- **æ¨ªè½´**ï¼šç´§æ€¥ç¨‹åº¦ï¼ˆå·¦ä½å³é«˜ï¼‰
- **çºµè½´**ï¼šé‡è¦ç¨‹åº¦ï¼ˆä¸‹ä½ä¸Šé«˜ï¼‰
- **Do Firstï¼ˆç«‹å³æ‰§è¡Œï¼‰**ï¼šå³ä¸Šè±¡é™ï¼Œé‡è¦ä¸”ç´§æ€¥
- **Scheduleï¼ˆè®¡åˆ’æ‰§è¡Œï¼‰**ï¼šå·¦ä¸Šè±¡é™ï¼Œé‡è¦ä½†ä¸ç´§æ€¥
- **Delegateï¼ˆå¯å§”æ´¾ï¼‰**ï¼šå³ä¸‹è±¡é™ï¼Œç´§æ€¥ä½†ä¸é‡è¦
- **Eliminateï¼ˆä½ä¼˜å…ˆçº§ï¼‰**ï¼šå·¦ä¸‹è±¡é™ï¼Œä¸é‡è¦ä¸ç´§æ€¥
- ç„¶ååˆ—å‡ºæ¯ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„ä¸­æ–‡éœ€æ±‚åç§°ï¼Œå¦‚ï¼š`Requirement1 = éœ€æ±‚åç§°1`

**quadrantChart è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
- titleã€x-axisã€y-axisã€quadrant æ ‡ç­¾**å¿…é¡»ç”¨è‹±æ–‡**ï¼ˆæŠ€æœ¯é™åˆ¶ï¼‰
- æ•°æ®ç‚¹åç§°ç”¨è‹±æ–‡æˆ–æ‹¼éŸ³ï¼Œæ ¼å¼ï¼š`Name: [x, y]`ï¼Œxå’ŒyèŒƒå›´0-1
- ä¸è¦ä½¿ç”¨ç‰¹æ®Šç¬¦å·
- **å¿…é¡»åœ¨å›¾è¡¨ä¸‹æ–¹æ·»åŠ ä¸­æ–‡å›¾ä¾‹è¯´æ˜**

#### 1.2 ä¼˜å…ˆçº§æ¸…å•ï¼ˆMarkdownè¡¨æ ¼ï¼‰
ç´§æ¥ç€å›¾ä¾‹è¯´æ˜ï¼Œç”¨ä¸­æ–‡è¡¨æ ¼è¯¦ç»†è¯´æ˜æ¯ä¸ªéœ€æ±‚çš„ä¼˜å…ˆçº§ï¼š

| ä¼˜å…ˆçº§ | éœ€æ±‚é¡¹ | è¯´æ˜ |
|:---:|:---|:---|
| ğŸ”´ P0 ç«‹å³æ‰§è¡Œ | éœ€æ±‚1ã€éœ€æ±‚2 | é‡è¦ä¸”ç´§æ€¥ï¼Œå¿…é¡»ä¼˜å…ˆå¤„ç† |
| ğŸŸ¡ P1 è®¡åˆ’æ‰§è¡Œ | éœ€æ±‚3 | é‡è¦ä½†ä¸ç´§æ€¥ï¼Œéœ€è¦è§„åˆ’ |
| ğŸŸ¢ P2 å¯å§”æ´¾ | éœ€æ±‚4 | ç´§æ€¥ä½†ä¸é‡è¦ï¼Œå¯åˆ†é…ä»–äºº |
| âšª P3 ä½ä¼˜å…ˆçº§ | éœ€æ±‚5 | ä¸é‡è¦ä¸ç´§æ€¥ï¼Œå¯å»¶å |

**ä¸¤ç§å½¢å¼é…åˆä½¿ç”¨**ï¼šè±¡é™å›¾ç›´è§‚å±•ç¤ºä½ç½®åˆ†å¸ƒï¼Œä¸­æ–‡å›¾ä¾‹è§£é‡Šè‹±æ–‡æ ‡ç­¾ï¼Œè¡¨æ ¼è¯¦ç»†è¯´æ˜ä¼˜å…ˆçº§å’Œç†ç”±ã€‚

### 2. ä¸šåŠ¡æµç¨‹å›¾ï¼ˆæ¨èï¼‰
ä½¿ç”¨ flowchart å±•ç¤ºå…³é”®ä¸šåŠ¡æµç¨‹ï¼Œ**ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ï¼š

```mermaid
flowchart TD
    A[å¼€å§‹] --> B{åˆ¤æ–­æ¡ä»¶}
    B -->|æ¡ä»¶æ»¡è¶³| C[å¤„ç†æµç¨‹1]
    B -->|æ¡ä»¶ä¸æ»¡è¶³| D[å¤„ç†æµç¨‹2]
    C --> E[ç»“æŸ]
    D --> E
```

**æ³¨æ„**ï¼šå¸¦æ ‡ç­¾çš„è¿æ¥çº¿æ ¼å¼ä¸º `-->|æ ‡ç­¾|`ï¼Œæ ‡ç­¾å†™åœ¨ç®­å¤´åé¢çš„ç«–çº¿ä¹‹é—´ã€‚

**flowchart è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
- èŠ‚ç‚¹IDä½¿ç”¨è‹±æ–‡å­—æ¯ï¼ˆå¦‚ Aã€Bã€Cï¼‰ï¼ŒèŠ‚ç‚¹æ ‡ç­¾ä½¿ç”¨ä¸­æ–‡ï¼ˆå¦‚ `A[ä¸­æ–‡æ ‡ç­¾]`ï¼‰
- subgraph æ ‡é¢˜ä½¿ç”¨ä¸­æ–‡ï¼ˆå¦‚ `subgraph å­æµç¨‹åç§°`ï¼‰
- **æ¯ä¸ª subgraph å¿…é¡»æœ‰å¯¹åº”çš„ end å…³é—­**
- èŠ‚ç‚¹æ ‡ç­¾ä¸­**ä¸¥ç¦ä½¿ç”¨ä»¥ä¸‹ç‰¹æ®Šå­—ç¬¦**ï¼š
  - åŠè§’å†’å· `:` - ç”¨çŸ­æ¨ªçº¿ `-` æˆ–ç©ºæ ¼æ›¿ä»£
  - åŠè§’å¼•å· `"` - ç”¨å…¨è§’å¼•å· "" æˆ–ä¹¦åå· ã€Šã€‹ æ›¿ä»£
  - åŠè§’æ‹¬å· `()` - ç”¨å…¨è§’æ‹¬å· ï¼ˆï¼‰ æ›¿ä»£
  - HTML æ ‡ç­¾å¦‚ `<br>` - ç”¨ç©ºæ ¼æˆ–æ¢è¡Œæ›¿ä»£
- è±å½¢åˆ¤æ–­èŠ‚ç‚¹ä½¿ç”¨ `{ä¸­æ–‡}` æ ¼å¼
- **ä¸è¦åœ¨åŒä¸€ä¸ª flowchart ä¸­åµŒå¥—è¿‡å¤šå±‚çº§ï¼ˆæœ€å¤š2å±‚ subgraphï¼‰**
- **è¿æ¥çº¿è¯­æ³•è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰**ï¼š
  - æ— æ ‡ç­¾è¿æ¥ï¼š`A --> B`
  - å¸¦æ ‡ç­¾è¿æ¥ï¼š`A -->|æ ‡ç­¾æ–‡å­—| B`ï¼ˆæ ‡ç­¾åœ¨ç®­å¤´åé¢ï¼Œç«–çº¿åŒ…å›´ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A --|æ ‡ç­¾|--> B`ï¼ˆè¿™æ˜¯é”™è¯¯è¯­æ³•ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A -->|æ ‡ç­¾|--> B`ï¼ˆä¸èƒ½æœ‰åŒç®­å¤´ï¼‰
  - **ç¦æ­¢ä½¿ç”¨**ï¼š`A --- B`ï¼ˆè™šçº¿æ— ç®­å¤´ï¼‰

### 3. éœ€æ±‚åˆ†ç±»é¥¼å›¾ï¼ˆå¯é€‰ï¼‰
ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾ï¼š
```mermaid
pie title éœ€æ±‚åˆ†å¸ƒ
    "åŠŸèƒ½éœ€æ±‚" : 45
    "æ€§èƒ½éœ€æ±‚" : 25
    "å®‰å…¨éœ€æ±‚" : 20
    "æ˜“ç”¨æ€§" : 10
```

### 4. éƒ¨ç½²æ¶æ„å›¾ï¼ˆå¦‚æ¶‰åŠæŠ€æœ¯çº¦æŸï¼‰
å¦‚æœè®¿è°ˆä¸­æ¶‰åŠéƒ¨ç½²æ¨¡å¼ã€ç³»ç»Ÿæ¶æ„ç­‰æŠ€æœ¯è¯é¢˜ï¼Œå¯ä½¿ç”¨ flowchart å±•ç¤ºéƒ¨ç½²æ¶æ„ï¼š

```mermaid
flowchart LR
    subgraph å‰ç«¯
        A[å®¢æˆ·ç«¯]
    end
    subgraph åç«¯
        B[è´Ÿè½½å‡è¡¡]
        C[åº”ç”¨æœåŠ¡å™¨]
    end
    subgraph å­˜å‚¨
        D[(æ•°æ®åº“)]
    end
    A -->|è¯·æ±‚| B
    B --> C
    C -->|è¯»å†™æ•°æ®| D
```

**éƒ¨ç½²æ¶æ„å›¾è§„åˆ™ï¼š**
- ä½¿ç”¨ flowchart LRï¼ˆä»å·¦åˆ°å³ï¼‰æˆ– flowchart TDï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰
- èŠ‚ç‚¹IDä½¿ç”¨è‹±æ–‡å­—æ¯ï¼Œæ ‡ç­¾ä½¿ç”¨ä¸­æ–‡
- ä¿æŒç»“æ„ç®€æ´ï¼Œé¿å…è¿‡åº¦å¤æ‚çš„åµŒå¥—
- å¸¦æ ‡ç­¾çš„è¿æ¥çº¿ä½¿ç”¨ `-->|æ ‡ç­¾æ–‡å­—|` æ ¼å¼ï¼ˆæ ‡ç­¾åœ¨ç®­å¤´åé¢ï¼‰

## é‡è¦æé†’
- æ‰€æœ‰å†…å®¹å¿…é¡»ä¸¥æ ¼åŸºäºè®¿è°ˆè®°å½•ï¼Œä¸å¾—ç¼–é€ 
- ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒMermaid ä»£ç å—ä½¿ç”¨ ```mermaid æ ‡è®°
- **ä¼˜å…ˆçº§çŸ©é˜µå¿…é¡»åŒæ—¶åŒ…å«ï¼šquadrantChartè±¡é™å›¾ + Markdownè¡¨æ ¼**
- **flowchartã€pie ç­‰å›¾è¡¨ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾**ï¼ŒquadrantChart å› æŠ€æœ¯é™åˆ¶å¿…é¡»ç”¨è‹±æ–‡
- æŠ¥å‘Šè¦ä¸“ä¸šã€ç»“æ„æ¸…æ™°ã€å¯æ“ä½œ
- **Mermaid è¯­æ³•è¦æ±‚ä¸¥æ ¼ï¼Œè¯·ä»”ç»†æ£€æŸ¥æ¯ä¸ªå›¾è¡¨çš„è¯­æ³•æ­£ç¡®æ€§**
- **flowchart è¿æ¥çº¿å¸¦æ ‡ç­¾è¯­æ³•å¿…é¡»æ˜¯ `A -->|æ ‡ç­¾| B`ï¼Œç¦æ­¢ä½¿ç”¨ `A --|æ ‡ç­¾|--> B`**
- æŠ¥å‘Šæœ«å°¾ä½¿ç”¨ç½²åï¼š*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³ç”Ÿæˆ*

è¯·ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Šï¼š"""

    return prompt


def _extract_first_json_object(raw_text: str) -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡ã€‚"""
    if not raw_text:
        return None

    json_start = raw_text.find('{')
    if json_start < 0:
        return None

    brace_count = 0
    in_string = False
    escape_next = False

    for i in range(json_start, len(raw_text)):
        char = raw_text[i]

        if escape_next:
            escape_next = False
            continue
        if char == '\\':
            escape_next = True
            continue
        if char == '"' and not escape_next:
            in_string = not in_string
            continue

        if in_string:
            continue

        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0:
                return raw_text[json_start:i + 1]

    return None


def parse_structured_json_response(raw_text: str, required_keys: Optional[list] = None) -> Optional[dict]:
    """è§£æç»“æ„åŒ– JSON å“åº”ï¼ˆå…¼å®¹ä»£ç å—ä¸æ··æ‚æ–‡æœ¬ï¼‰ã€‚"""
    if not raw_text:
        return None

    text = raw_text.strip()
    candidates = []

    if text.startswith('{') and text.endswith('}'):
        candidates.append(text)

    if "```json" in text:
        try:
            json_start = text.find("```json") + 7
            json_end = text.find("```", json_start)
            if json_end > json_start:
                candidates.append(text[json_start:json_end].strip())
        except Exception:
            pass

    if "```" in text:
        try:
            blocks = re.findall(r"```(?:json)?\s*([\s\S]*?)```", text)
            candidates.extend([block.strip() for block in blocks if block.strip()])
        except Exception:
            pass

    extracted = _extract_first_json_object(text)
    if extracted:
        candidates.append(extracted)

    for candidate in candidates:
        try:
            parsed = json.loads(candidate)
            if not isinstance(parsed, dict):
                continue
            if not required_keys:
                return parsed
            if all(key in parsed for key in required_keys):
                return parsed
            if any(key in parsed for key in required_keys):
                return parsed
        except Exception:
            continue

    return None


def _normalize_evidence_refs(raw_refs) -> list:
    """æ ‡å‡†åŒ–è¯æ®å¼•ç”¨ï¼Œç»Ÿä¸€ä¸º Qæ•°å­— æ ¼å¼ã€‚"""
    refs = []
    if isinstance(raw_refs, str):
        refs.extend(re.findall(r"Q\d+", raw_refs.upper()))
    elif isinstance(raw_refs, list):
        for item in raw_refs:
            if isinstance(item, str):
                refs.extend(re.findall(r"Q\d+", item.upper()))

    dedup = sorted(set(refs), key=lambda ref: int(ref[1:]) if ref[1:].isdigit() else 10**9)
    return dedup


def build_report_evidence_pack(session: dict) -> dict:
    """æ„å»ºæŠ¥å‘Š V3 è¯æ®åŒ…ã€‚"""
    interview_log = session.get("interview_log", [])
    dim_info = get_dimension_info_for_session(session)
    mode_config = get_interview_mode_config(session)

    vague_terms = ["çœ‹æƒ…å†µ", "ä¸ç¡®å®š", "éƒ½å¯ä»¥", "ä¸çŸ¥é“", "å¯èƒ½", "æš‚æ—¶ä¸æ¸…æ¥š", "ä»¥åå†è¯´", "å·®ä¸å¤š"]
    unknown_signals = {"vague_expression", "generic_answer", "option_only"}

    contradiction_patterns = [
        ("need", "éœ€è¦", "ä¸éœ€è¦", "éœ€æ±‚å–å‘å†²çª"),
        ("support", "æ”¯æŒ", "ä¸æ”¯æŒ", "æ”¯æŒç«‹åœºå†²çª"),
        ("available", "æœ‰", "æ²¡æœ‰", "èµ„æºç°çŠ¶å†²çª"),
        ("must", "å¿…é¡»", "å¯é€‰", "çº¦æŸä¼˜å…ˆçº§å†²çª"),
        ("ready", "å·²ç»", "è¿˜æ²¡", "å‡†å¤‡çŠ¶æ€å†²çª"),
    ]

    facts = []
    unknowns = []
    contradictions = []
    blindspots = []
    contradiction_state = {}
    contradiction_keys = set()
    unknown_keys = set()

    for idx, log in enumerate(interview_log, 1):
        q_id = f"Q{idx}"
        dimension_key = log.get("dimension", "")
        dim_name = dim_info.get(dimension_key, {}).get("name", dimension_key or "æœªåˆ†ç±»")
        question = str(log.get("question", "")).strip()
        answer = str(log.get("answer", "")).strip()
        signals = log.get("follow_up_signals") if isinstance(log.get("follow_up_signals"), list) else []
        try:
            quality_score = float(log.get("quality_score", 0) or 0)
        except Exception:
            quality_score = 0.0
        quality_score = max(0.0, min(1.0, quality_score))

        fact = {
            "q_id": q_id,
            "dimension": dimension_key,
            "dimension_name": dim_name,
            "question": question,
            "answer": answer,
            "is_follow_up": bool(log.get("is_follow_up", False)),
            "follow_up_round": int(log.get("follow_up_round", 0) or 0),
            "quality_score": quality_score,
            "quality_signals": log.get("quality_signals", []) if isinstance(log.get("quality_signals"), list) else [],
            "follow_up_signals": signals,
            "hard_triggered": bool(log.get("hard_triggered", False)),
        }
        facts.append(fact)

        unknown_reasons = []
        if any(signal in unknown_signals for signal in signals):
            unknown_reasons.append("å‘½ä¸­æ¨¡ç³Šå›ç­”ä¿¡å·")
        if any(term in answer for term in vague_terms):
            unknown_reasons.append("å›ç­”å­˜åœ¨æ¨¡ç³Šè¡¨è¿°")
        if quality_score > 0 and quality_score < 0.45:
            unknown_reasons.append("å›ç­”è´¨é‡åä½")
        if unknown_reasons:
            unknown_key = f"{q_id}:{'|'.join(sorted(set(unknown_reasons)))}"
            if unknown_key not in unknown_keys:
                unknown_keys.add(unknown_key)
                unknowns.append({
                    "q_id": q_id,
                    "dimension": dim_name,
                    "reason": "ï¼›".join(sorted(set(unknown_reasons))),
                    "answer_excerpt": answer[:120],
                })

        for pair_id, positive, negative, description in contradiction_patterns:
            has_positive = positive in answer
            has_negative = negative in answer

            if has_positive and has_negative:
                key = f"self:{pair_id}:{q_id}"
                if key not in contradiction_keys:
                    contradiction_keys.add(key)
                    contradictions.append({
                        "type": "same_answer_conflict",
                        "pair_id": pair_id,
                        "description": description,
                        "dimension": dim_name,
                        "evidence_refs": [q_id],
                        "detail": f"{q_id} åŒæ—¶å‡ºç°ã€Œ{positive}ã€ä¸ã€Œ{negative}ã€",
                    })

            if not (has_positive or has_negative):
                continue

            state = "positive" if has_positive else "negative"
            state_key = (dimension_key, pair_id)
            previous = contradiction_state.get(state_key)
            if previous and previous.get("state") != state:
                key = f"cross:{pair_id}:{previous.get('q_id')}:{q_id}:{dimension_key}"
                if key not in contradiction_keys:
                    contradiction_keys.add(key)
                    contradictions.append({
                        "type": "cross_answer_conflict",
                        "pair_id": pair_id,
                        "description": description,
                        "dimension": dim_name,
                        "evidence_refs": [previous.get("q_id"), q_id],
                        "detail": f"{previous.get('q_id')} ä¸ {q_id} å¯¹ã€Œ{positive}/{negative}ã€å­˜åœ¨å†²çª",
                    })
            contradiction_state[state_key] = {"state": state, "q_id": q_id}

    dimension_coverage = {}
    coverage_values = []
    for dim_key, info in dim_info.items():
        dim_logs = [log for log in interview_log if log.get("dimension") == dim_key]
        formal_count = len([log for log in dim_logs if not log.get("is_follow_up", False)])
        follow_up_count = len([log for log in dim_logs if log.get("is_follow_up", False)])
        dim_state = session.get("dimensions", {}).get(dim_key, {})
        coverage_percent = int(dim_state.get("coverage", 0) or 0)
        coverage_values.append(max(0, min(100, coverage_percent)) / 100.0)
        missing_aspects = get_dimension_missing_aspects(session, dim_key)

        for aspect in missing_aspects:
            blindspots.append({
                "dimension": info.get("name", dim_key),
                "aspect": aspect,
            })

        dimension_coverage[dim_key] = {
            "name": info.get("name", dim_key),
            "coverage_percent": max(0, min(100, coverage_percent)),
            "coverage_ratio": round(max(0, min(100, coverage_percent)) / 100.0, 2),
            "formal_count": formal_count,
            "follow_up_count": follow_up_count,
            "minimum_formal": mode_config.get("formal_questions_per_dim", 3),
            "maximum_formal": mode_config.get("max_formal_questions_per_dim", mode_config.get("formal_questions_per_dim", 3)),
            "missing_aspects": missing_aspects,
            "key_aspects": info.get("key_aspects", []),
        }

    valid_quality_scores = [fact["quality_score"] for fact in facts if fact.get("quality_score", 0) > 0]
    average_quality = sum(valid_quality_scores) / len(valid_quality_scores) if valid_quality_scores else 0.0
    overall_coverage = sum(coverage_values) / len(coverage_values) if coverage_values else 0.0

    total_formal = len([fact for fact in facts if not fact.get("is_follow_up", False)])
    total_follow_up = len([fact for fact in facts if fact.get("is_follow_up", False)])

    return {
        "topic": session.get("topic", "æœªçŸ¥ä¸»é¢˜"),
        "report_type": session.get("scenario_config", {}).get("report", {}).get("type", "standard"),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "facts": facts,
        "contradictions": contradictions,
        "unknowns": unknowns,
        "blindspots": blindspots,
        "dimension_coverage": dimension_coverage,
        "overall_coverage": round(overall_coverage, 3),
        "quality_snapshot": {
            "average_quality_score": round(average_quality, 3),
            "hard_triggered_count": len([fact for fact in facts if fact.get("hard_triggered")]),
            "total_questions": len(facts),
            "total_formal_questions": total_formal,
            "total_follow_up_questions": total_follow_up,
            "follow_up_ratio": round(total_follow_up / len(facts), 3) if facts else 0.0,
        },
    }


def summarize_evidence_pack_for_debug(evidence_pack: dict) -> dict:
    """æç‚¼è¯æ®åŒ…æ‘˜è¦ï¼Œç”¨äºä¼šè¯å†…è°ƒè¯•ç•™ç—•ã€‚"""
    if not isinstance(evidence_pack, dict):
        return {}

    facts = evidence_pack.get("facts", [])
    contradictions = evidence_pack.get("contradictions", [])
    unknowns = evidence_pack.get("unknowns", [])
    blindspots = evidence_pack.get("blindspots", [])

    dimension_summary = {}
    raw_dimension_coverage = evidence_pack.get("dimension_coverage", {})
    if isinstance(raw_dimension_coverage, dict):
        for dim_key, dim_meta in raw_dimension_coverage.items():
            if not isinstance(dim_meta, dict):
                continue
            dimension_summary[dim_key] = {
                "name": dim_meta.get("name", dim_key),
                "coverage_percent": int(dim_meta.get("coverage_percent", 0) or 0),
                "formal_count": int(dim_meta.get("formal_count", 0) or 0),
                "follow_up_count": int(dim_meta.get("follow_up_count", 0) or 0),
                "missing_aspects": (dim_meta.get("missing_aspects", []) if isinstance(dim_meta.get("missing_aspects", []), list) else [])[:8],
            }

    quality_snapshot = evidence_pack.get("quality_snapshot", {})
    if not isinstance(quality_snapshot, dict):
        quality_snapshot = {}

    return {
        "overall_coverage": float(evidence_pack.get("overall_coverage", 0) or 0),
        "facts_count": len(facts) if isinstance(facts, list) else 0,
        "contradictions_count": len(contradictions) if isinstance(contradictions, list) else 0,
        "unknowns_count": len(unknowns) if isinstance(unknowns, list) else 0,
        "blindspots_count": len(blindspots) if isinstance(blindspots, list) else 0,
        "quality_snapshot": {
            "average_quality_score": float(quality_snapshot.get("average_quality_score", 0) or 0),
            "hard_triggered_count": int(quality_snapshot.get("hard_triggered_count", 0) or 0),
            "total_questions": int(quality_snapshot.get("total_questions", 0) or 0),
            "total_formal_questions": int(quality_snapshot.get("total_formal_questions", 0) or 0),
            "total_follow_up_questions": int(quality_snapshot.get("total_follow_up_questions", 0) or 0),
            "follow_up_ratio": float(quality_snapshot.get("follow_up_ratio", 0) or 0),
        },
        "dimension_coverage": dimension_summary,
    }


def build_report_draft_prompt_v3(session: dict, evidence_pack: dict) -> str:
    """æ„å»º V3 æŠ¥å‘Šè‰æ¡ˆç”Ÿæˆ Promptï¼ˆç»“æ„åŒ– JSONï¼‰ã€‚"""
    topic = session.get("topic", "æœªçŸ¥ä¸»é¢˜")
    description = session.get("description", "")
    report_type = evidence_pack.get("report_type", "standard")
    report_type_label = "é¢è¯•è¯„ä¼°" if report_type == "assessment" else "éœ€æ±‚è®¿è°ˆ"

    dimension_lines = []
    for dim_key, dim_meta in evidence_pack.get("dimension_coverage", {}).items():
        missing = "ã€".join(dim_meta.get("missing_aspects", [])[:4]) if dim_meta.get("missing_aspects") else "æ— "
        dimension_lines.append(
            f"- {dim_meta.get('name', dim_key)}: è¦†ç›–{dim_meta.get('coverage_percent', 0)}%ï¼Œ"
            f"æ­£å¼é¢˜ {dim_meta.get('formal_count', 0)}ï¼Œè¿½é—® {dim_meta.get('follow_up_count', 0)}ï¼Œæœªè¦†ç›–æ–¹é¢ï¼š{missing}"
        )
    dimension_text = "\n".join(dimension_lines) if dimension_lines else "- æš‚æ— ç»´åº¦è¦†ç›–æ•°æ®"

    facts_lines = []
    for fact in evidence_pack.get("facts", [])[:60]:
        question_text = (fact.get("question", "") or "").replace("\n", " ").strip()[:90]
        answer_text = (fact.get("answer", "") or "").replace("\n", " ").strip()[:150]
        facts_lines.append(
            f"- {fact.get('q_id')} [{fact.get('dimension_name', 'æœªåˆ†ç±»')}] "
            f"Q: {question_text} | A: {answer_text} | quality={fact.get('quality_score', 0):.2f}"
        )
    facts_text = "\n".join(facts_lines) if facts_lines else "- æ— æœ‰æ•ˆé—®ç­”è¯æ®"

    contradictions = evidence_pack.get("contradictions", [])
    contradiction_lines = [
        f"- {item.get('detail')}ï¼ˆè¯æ®: {', '.join(item.get('evidence_refs', []))}ï¼‰"
        for item in contradictions[:20]
    ]
    contradiction_text = "\n".join(contradiction_lines) if contradiction_lines else "- æœªå‘ç°æ˜æ˜¾å†²çª"

    unknowns = evidence_pack.get("unknowns", [])
    unknown_lines = [
        f"- {item.get('q_id')} [{item.get('dimension')}] {item.get('reason')}"
        for item in unknowns[:20]
    ]
    unknown_text = "\n".join(unknown_lines) if unknown_lines else "- æœªå‘ç°æ˜æ˜¾æ¨¡ç³Šå›ç­”"

    blindspots = evidence_pack.get("blindspots", [])
    blindspot_lines = [
        f"- {item.get('dimension')}: {item.get('aspect')}"
        for item in blindspots[:20]
    ]
    blindspot_text = "\n".join(blindspot_lines) if blindspot_lines else "- æš‚æ— ç›²åŒº"

    schema_example = {
        "overview": "è®¿è°ˆæ¦‚è¿°ï¼ˆ2-4æ®µï¼‰",
        "needs": [
            {
                "name": "æ ¸å¿ƒéœ€æ±‚åç§°",
                "priority": "P0",
                "description": "éœ€æ±‚æè¿°",
                "evidence_refs": ["Q1", "Q3"]
            }
        ],
        "analysis": {
            "customer_needs": "å®¢æˆ·/ç”¨æˆ·éœ€æ±‚åˆ†æ",
            "business_flow": "ä¸šåŠ¡æµç¨‹åˆ†æ",
            "tech_constraints": "æŠ€æœ¯çº¦æŸåˆ†æ",
            "project_constraints": "é¡¹ç›®çº¦æŸåˆ†æ"
        },
        "visualizations": {
            "priority_quadrant_mermaid": "quadrantChart ...",
            "business_flow_mermaid": "flowchart TD ...",
            "demand_pie_mermaid": "pie title ...",
            "architecture_mermaid": "flowchart LR ..."
        },
        "solutions": [
            {
                "title": "æ–¹æ¡ˆå»ºè®®æ ‡é¢˜",
                "description": "æ–¹æ¡ˆè¯´æ˜",
                "owner": "è´Ÿè´£è§’è‰²",
                "timeline": "æ—¶é—´è®¡åˆ’",
                "metric": "éªŒæ”¶æŒ‡æ ‡",
                "evidence_refs": ["Q2", "Q8"]
            }
        ],
        "risks": [
            {
                "risk": "é£é™©é¡¹",
                "impact": "å½±å“",
                "mitigation": "ç¼“è§£æªæ–½",
                "evidence_refs": ["Q6"]
            }
        ],
        "actions": [
            {
                "action": "ä¸‹ä¸€æ­¥è¡ŒåŠ¨",
                "owner": "è´Ÿè´£äººè§’è‰²",
                "timeline": "é¢„è®¡æ—¶é—´",
                "metric": "å®Œæˆæ ‡å‡†",
                "evidence_refs": ["Q4"]
            }
        ],
        "open_questions": [
            {
                "question": "æœªå†³é—®é¢˜",
                "reason": "ä¸ºä½•æœªå†³",
                "impact": "å½±å“èŒƒå›´",
                "suggested_follow_up": "å»ºè®®è¡¥å……è¿½é—®",
                "evidence_refs": ["Q7"]
            }
        ],
        "evidence_index": [
            {
                "claim": "å…³é”®ç»“è®º",
                "confidence": "high",
                "evidence_refs": ["Q1", "Q5"]
            }
        ]
    }

    return f"""ä½ æ˜¯ä¸€åèµ„æ·±åˆ†æé¡¾é—®ã€‚è¯·åŸºäºç»™å®šè¯æ®åŒ…ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–æŠ¥å‘Šè‰æ¡ˆ JSONï¼Œç¦æ­¢è¾“å‡ºä»»ä½• JSON ä¹‹å¤–çš„æ–‡å­—ã€‚

## ä»»åŠ¡ç±»å‹
- æŠ¥å‘Šç±»å‹ï¼š{report_type_label}
- ä¸»é¢˜ï¼š{topic}
{f"- ä¸»é¢˜æè¿°ï¼š{description}" if description else ""}

## ç»´åº¦è¦†ç›–å¿«ç…§
{dimension_text}

## å…³é”®è¯æ®ï¼ˆæŒ‰é—®ç­”ç¼–å·ï¼‰
{facts_text}

## å†²çªä¿¡å·
{contradiction_text}

## æ¨¡ç³Šä¸ä¸ç¡®å®šä¿¡å·
{unknown_text}

## ç›²åŒºæ¸…å•ï¼ˆå¿…é¡»ä¼˜å…ˆè¡¥é½ï¼‰
{blindspot_text}

## è¾“å‡ºç¡¬æ€§çº¦æŸ
1. è¾“å‡ºå¿…é¡»æ˜¯åˆæ³• JSON å¯¹è±¡ï¼Œé¦–å­—ç¬¦æ˜¯ {{ï¼Œæœ«å­—ç¬¦æ˜¯ }}ã€‚
2. æ‰€æœ‰å…³é”®ç»“è®ºéƒ½è¦æºå¸¦ evidence_refsï¼ˆæ ¼å¼å¿…é¡»æ˜¯ Qæ•°å­—ï¼‰ã€‚
3. solutions/actions æ¯ä¸€é¡¹å¿…é¡»åŒ…å« ownerã€timelineã€metricã€‚
4. è‹¥å­˜åœ¨å†²çªä¿¡å·ï¼Œå¿…é¡»åœ¨ risks æˆ– open_questions ä¸­æ˜¾å¼å¤„ç†ã€‚
5. è‹¥å­˜åœ¨ç›²åŒºï¼Œå¿…é¡»åœ¨ open_questions ä¸­ä½“ç°å¯¹åº”è¡¥é—®ã€‚
6. visualizations å­—æ®µä¸­è¯·è¾“å‡º Mermaid è¯­æ³•æ­£æ–‡ï¼ˆä¸è¦åŒ…è£¹```mermaidä»£ç å—ï¼‰ã€‚
7. ä¸å¾—ç¼–é€ è¯æ®ç¼–å·ï¼Œä¸å¾—å¼•ç”¨ä¸å­˜åœ¨çš„ Q ç¼–å·ã€‚

## JSON æ¨¡æ¿ï¼ˆå­—æ®µå¿…é¡»å®Œæ•´ï¼‰
{json.dumps(schema_example, ensure_ascii=False, indent=2)}
"""


def validate_report_draft_v3(draft: dict, evidence_pack: dict) -> tuple[dict, list]:
    """æ ¡éªŒå¹¶æ ‡å‡†åŒ– V3 æŠ¥å‘Šè‰æ¡ˆã€‚"""
    issues = []

    def add_issue(issue_type: str, severity: str, message: str, target: str):
        issues.append({
            "type": issue_type,
            "severity": severity,
            "message": message,
            "target": target,
        })

    if not isinstance(draft, dict):
        add_issue("structure_error", "high", "è‰æ¡ˆä¸æ˜¯ JSON å¯¹è±¡", "root")
        return {}, issues

    normalized = {
        "overview": str(draft.get("overview", "")).strip(),
        "needs": [],
        "analysis": {
            "customer_needs": "",
            "business_flow": "",
            "tech_constraints": "",
            "project_constraints": "",
        },
        "visualizations": {
            "priority_quadrant_mermaid": "",
            "business_flow_mermaid": "",
            "demand_pie_mermaid": "",
            "architecture_mermaid": "",
        },
        "solutions": [],
        "risks": [],
        "actions": [],
        "open_questions": [],
        "evidence_index": [],
    }

    valid_q_refs = {
        str(item.get("q_id", "")).upper().strip()
        for item in evidence_pack.get("facts", [])
        if isinstance(item, dict) and re.fullmatch(r"Q\d+", str(item.get("q_id", "")).upper().strip())
    }

    def normalize_evidence_refs_for_target(raw_refs, target: str) -> list:
        refs = _normalize_evidence_refs(raw_refs)
        if not refs:
            return []
        if not valid_q_refs:
            add_issue("invalid_evidence_ref", "high", "è¯æ®åŒ…ç¼ºå°‘å¯ç”¨é—®ç­”ç¼–å·ï¼Œæ— æ³•éªŒè¯è¯æ®å¼•ç”¨", target)
            return []

        invalid_refs = [ref for ref in refs if ref not in valid_q_refs]
        if invalid_refs:
            add_issue(
                "invalid_evidence_ref",
                "high",
                f"åŒ…å«æ— æ•ˆè¯æ®å¼•ç”¨ï¼š{', '.join(invalid_refs)}",
                target
            )

        return [ref for ref in refs if ref in valid_q_refs]

    analysis = draft.get("analysis", {})
    if isinstance(analysis, dict):
        for key in normalized["analysis"]:
            normalized["analysis"][key] = str(analysis.get(key, "")).strip()

    visualizations = draft.get("visualizations", {})
    if isinstance(visualizations, dict):
        for key in normalized["visualizations"]:
            normalized["visualizations"][key] = str(visualizations.get(key, "")).strip()

    if not normalized["overview"]:
        add_issue("structure_error", "high", "overview ä¸èƒ½ä¸ºç©º", "overview")

    needs = draft.get("needs", [])
    if isinstance(needs, list):
        for idx, item in enumerate(needs):
            if not isinstance(item, dict):
                add_issue("structure_error", "medium", "needs é¡¹å¿…é¡»æ˜¯å¯¹è±¡", f"needs[{idx}]")
                continue
            refs = normalize_evidence_refs_for_target(item.get("evidence_refs", []), f"needs[{idx}].evidence_refs")
            normalized_item = {
                "name": str(item.get("name", "")).strip(),
                "priority": str(item.get("priority", "P1")).strip().upper() or "P1",
                "description": str(item.get("description", "")).strip(),
                "evidence_refs": refs,
            }
            if not normalized_item["name"]:
                add_issue("structure_error", "medium", "needs.name ä¸èƒ½ä¸ºç©º", f"needs[{idx}].name")
            if not refs:
                add_issue("no_evidence", "high", "æ ¸å¿ƒéœ€æ±‚ç¼ºå°‘è¯æ®å¼•ç”¨", f"needs[{idx}]")
            normalized["needs"].append(normalized_item)

    for field, id_field in [("solutions", "title"), ("risks", "risk"), ("actions", "action"), ("open_questions", "question"), ("evidence_index", "claim")]:
        values = draft.get(field, [])
        if not isinstance(values, list):
            add_issue("structure_error", "medium", f"{field} å¿…é¡»æ˜¯æ•°ç»„", field)
            continue
        for idx, item in enumerate(values):
            if not isinstance(item, dict):
                add_issue("structure_error", "medium", f"{field} é¡¹å¿…é¡»æ˜¯å¯¹è±¡", f"{field}[{idx}]")
                continue
            refs = normalize_evidence_refs_for_target(item.get("evidence_refs", []), f"{field}[{idx}].evidence_refs")
            normalized_item = dict(item)
            normalized_item[id_field] = str(item.get(id_field, "")).strip()
            normalized_item["evidence_refs"] = refs

            if field in {"solutions", "actions"}:
                normalized_item["owner"] = str(item.get("owner", "")).strip()
                normalized_item["timeline"] = str(item.get("timeline", "")).strip()
                normalized_item["metric"] = str(item.get("metric", "")).strip()
                if not (normalized_item["owner"] and normalized_item["timeline"] and normalized_item["metric"]):
                    add_issue("not_actionable", "medium", f"{field} ç¼ºå°‘ owner/timeline/metric", f"{field}[{idx}]")

            if field == "risks":
                normalized_item["impact"] = str(item.get("impact", "")).strip()
                normalized_item["mitigation"] = str(item.get("mitigation", "")).strip()

            if field == "open_questions":
                normalized_item["reason"] = str(item.get("reason", "")).strip()
                normalized_item["impact"] = str(item.get("impact", "")).strip()
                normalized_item["suggested_follow_up"] = str(item.get("suggested_follow_up", "")).strip()

            if field == "evidence_index":
                confidence = str(item.get("confidence", "medium")).strip().lower()
                if confidence not in {"high", "medium", "low"}:
                    confidence = "medium"
                normalized_item["confidence"] = confidence

            if not normalized_item.get(id_field):
                add_issue("structure_error", "medium", f"{field}.{id_field} ä¸èƒ½ä¸ºç©º", f"{field}[{idx}].{id_field}")
            if not refs:
                add_issue("no_evidence", "high", f"{field} ç¼ºå°‘è¯æ®å¼•ç”¨", f"{field}[{idx}]")

            normalized[field].append(normalized_item)

    combined_text = json.dumps(normalized, ensure_ascii=False)

    contradictions = evidence_pack.get("contradictions", [])
    unresolved_conflicts = 0
    for item in contradictions:
        refs = item.get("evidence_refs", [])
        if refs and not any(ref in combined_text for ref in refs):
            unresolved_conflicts += 1
    if unresolved_conflicts > 0:
        add_issue(
            "unresolved_contradiction",
            "high",
            f"å­˜åœ¨ {unresolved_conflicts} æ¡å†²çªè¯æ®æœªåœ¨è‰æ¡ˆä¸­å¤„ç†",
            "risks/open_questions"
        )

    blindspots = evidence_pack.get("blindspots", [])
    unresolved_blindspots = []
    for item in blindspots:
        aspect = str(item.get("aspect", "")).strip()
        if aspect and aspect not in combined_text:
            unresolved_blindspots.append(aspect)
    if unresolved_blindspots:
        sample = "ã€".join(unresolved_blindspots[:5])
        add_issue(
            "blindspot",
            "medium",
            f"ä»æœ‰æœªè¦†ç›–ç›²åŒºæœªè¿›å…¥è‰æ¡ˆï¼š{sample}",
            "open_questions"
        )

    return normalized, issues


def build_report_review_prompt_v3(session: dict, evidence_pack: dict, draft: dict, issues: list) -> str:
    """æ„å»º V3 å®¡ç¨¿ä¸ä¿®å¤ Promptã€‚"""
    topic = session.get("topic", "æœªçŸ¥ä¸»é¢˜")
    contradiction_text = "\n".join(
        [f"- {item.get('detail')}ï¼ˆè¯æ®: {', '.join(item.get('evidence_refs', []))}ï¼‰" for item in evidence_pack.get("contradictions", [])[:20]]
    ) or "- æ— "
    blindspot_text = "\n".join(
        [f"- {item.get('dimension')}: {item.get('aspect')}" for item in evidence_pack.get("blindspots", [])[:20]]
    ) or "- æ— "

    issue_text = "\n".join(
        [f"- [{item.get('severity', 'medium')}] {item.get('type')}: {item.get('message')} @ {item.get('target', 'unknown')}" for item in issues[:30]]
    ) or "- æ— å·²çŸ¥é—®é¢˜"

    response_schema = {
        "passed": True,
        "issues": [
            {
                "type": "no_evidence",
                "severity": "high",
                "message": "é—®é¢˜æè¿°",
                "target": "å­—æ®µè·¯å¾„"
            }
        ],
        "revised_draft": {
            "overview": "ä¿®è®¢åçš„ overview",
            "needs": [],
            "analysis": {},
            "visualizations": {},
            "solutions": [],
            "risks": [],
            "actions": [],
            "open_questions": [],
            "evidence_index": []
        }
    }

    return f"""ä½ æ˜¯æŠ¥å‘Šè´¨é‡å®¡ç¨¿ä¸“å®¶ã€‚è¯·å¯¹è‰æ¡ˆæ‰§è¡Œä¸€è‡´æ€§å®¡ç¨¿å¹¶ç›´æ¥ä¿®å¤ï¼Œè¾“å‡º JSONã€‚

## è®¿è°ˆä¸»é¢˜
{topic}

## é‡ç‚¹æ ¸æŸ¥è§„åˆ™
1. no_evidenceï¼šå…³é”®ç»“è®ºæˆ–è¡ŒåŠ¨ç¼ºå°‘ evidence_refsã€‚
2. unresolved_contradictionï¼šå†²çªè¯æ®æ²¡æœ‰è¢«è§£é‡Šæˆ–å¤„ç†ã€‚
3. not_actionableï¼šè¡ŒåŠ¨å»ºè®®ç¼ºå°‘ owner/timeline/metricã€‚
4. blindspotï¼šç›²åŒºæ²¡æœ‰è¿›å…¥ open_questions æˆ–è¡ŒåŠ¨è®¡åˆ’ã€‚

## å†²çªè¯æ®
{contradiction_text}

## ç›²åŒºè¯æ®
{blindspot_text}

## å½“å‰å·²çŸ¥é—®é¢˜
{issue_text}

## å¾…å®¡ç¨¿è‰æ¡ˆ JSON
{json.dumps(draft, ensure_ascii=False)}

## è¾“å‡ºè¦æ±‚
- ä»…è¾“å‡ºåˆæ³• JSONï¼Œç¦æ­¢é™„åŠ è§£é‡Šæ–‡å­—ã€‚
- å¿…é¡»åŒ…å« passedã€issuesã€revised_draft ä¸‰ä¸ªå­—æ®µã€‚
- revised_draft å¿…é¡»ä¿ç•™åŸæœ‰ç»“æ„ï¼Œå¹¶ä¿®å¤å¯ä¿®å¤é—®é¢˜ã€‚
- è‹¥ä»æœ‰é—®é¢˜ï¼Œissues éœ€å®Œæ•´åˆ—å‡ºã€‚

## è¾“å‡ºæ¨¡æ¿
{json.dumps(response_schema, ensure_ascii=False, indent=2)}
"""


def parse_report_review_response_v3(raw_text: str) -> Optional[dict]:
    """è§£æ V3 å®¡ç¨¿å“åº”ã€‚"""
    parsed = parse_structured_json_response(raw_text, required_keys=["passed", "issues"])
    if not parsed:
        return None

    issues = []
    raw_issues = parsed.get("issues", [])
    if isinstance(raw_issues, list):
        for item in raw_issues:
            if not isinstance(item, dict):
                continue
            issues.append({
                "type": str(item.get("type", "unknown")).strip(),
                "severity": str(item.get("severity", "medium")).strip().lower() or "medium",
                "message": str(item.get("message", "")).strip(),
                "target": str(item.get("target", "unknown")).strip(),
            })

    revised_draft = parsed.get("revised_draft", {})
    if not isinstance(revised_draft, dict):
        revised_draft = {}

    return {
        "passed": bool(parsed.get("passed", False)),
        "issues": issues,
        "revised_draft": revised_draft,
    }


def _collect_claim_entries_for_quality(draft: dict) -> list:
    """æ”¶é›†è‰æ¡ˆä¸­çš„å¯è®¡é‡ç»“è®ºæ¡ç›®ã€‚"""
    claim_entries = []
    for field in ["needs", "solutions", "risks", "actions", "open_questions", "evidence_index"]:
        values = draft.get(field, [])
        if not isinstance(values, list):
            continue
        for item in values:
            if not isinstance(item, dict):
                continue
            claim_entries.append({
                "field": field,
                "evidence_refs": _normalize_evidence_refs(item.get("evidence_refs", [])),
                "owner": str(item.get("owner", "")).strip(),
                "timeline": str(item.get("timeline", "")).strip(),
                "metric": str(item.get("metric", "")).strip(),
            })
    return claim_entries


REPORT_V3_QUALITY_THRESHOLDS = {
    "evidence_coverage": 0.9,
    "consistency": 0.8,
    "actionability": 0.8,
}


def build_quality_gate_issues_v3(quality_meta: dict, thresholds: Optional[dict] = None) -> list:
    """æ ¹æ®è´¨é‡é˜ˆå€¼æ„å»ºé—¨ç¦é—®é¢˜åˆ—è¡¨ã€‚"""
    limit = thresholds or REPORT_V3_QUALITY_THRESHOLDS
    if not isinstance(quality_meta, dict):
        return [{
            "type": "quality_gate_missing",
            "severity": "high",
            "message": "ç¼ºå°‘è´¨é‡è¯„åˆ†ç»“æœï¼Œæ— æ³•é€šè¿‡è´¨é‡é—¨ç¦",
            "target": "quality_meta",
        }]

    checks = [
        ("evidence_coverage", "quality_gate_evidence", "è¯æ®è¦†ç›–ç‡", "needs/solutions/actions/risks/open_questions/evidence_index"),
        ("consistency", "quality_gate_consistency", "å†²çªè§£é‡Šå®Œæˆåº¦", "risks/open_questions"),
        ("actionability", "quality_gate_actionability", "å¯æ‰§è¡Œå»ºè®®å æ¯”", "solutions/actions"),
    ]

    issues = []
    for key, issue_type, label, target in checks:
        current = float(quality_meta.get(key, 0) or 0)
        required = float(limit.get(key, 0) or 0)
        if current + 1e-9 < required:
            issues.append({
                "type": issue_type,
                "severity": "high",
                "message": f"{label}ä½äºé—¨æ§›ï¼ˆå½“å‰{current:.1%}ï¼Œè¦æ±‚â‰¥{required:.1%}ï¼‰",
                "target": target,
            })
    return issues


def compute_report_quality_meta_v3(draft: dict, evidence_pack: dict, issues: list) -> dict:
    """è®¡ç®— V3 æŠ¥å‘Šè´¨é‡å…ƒæ•°æ®ã€‚"""
    claim_entries = _collect_claim_entries_for_quality(draft)
    claim_total = len(claim_entries)
    evidence_covered = len([entry for entry in claim_entries if entry.get("evidence_refs")])
    evidence_coverage = (evidence_covered / claim_total) if claim_total > 0 else 0.0

    contradiction_total = len(evidence_pack.get("contradictions", []))
    unresolved_contradictions = len([item for item in issues if item.get("type") == "unresolved_contradiction"])
    if contradiction_total <= 0:
        consistency = 1.0
    else:
        consistency = max(0.0, 1.0 - unresolved_contradictions / contradiction_total)

    action_entries = [entry for entry in claim_entries if entry.get("field") in {"solutions", "actions"}]
    actionable_total = len(action_entries)
    actionable_count = len([
        entry for entry in action_entries
        if entry.get("owner") and entry.get("timeline") and entry.get("metric")
    ])
    actionability = (actionable_count / actionable_total) if actionable_total > 0 else 0.0

    overall = 0.4 * evidence_coverage + 0.3 * consistency + 0.3 * actionability

    return {
        "mode": "v3_structured_reviewed",
        "evidence_coverage": round(evidence_coverage, 3),
        "consistency": round(consistency, 3),
        "actionability": round(actionability, 3),
        "overall": round(overall, 3),
        "claim_total": claim_total,
        "claim_with_evidence": evidence_covered,
        "review_issue_count": len(issues),
    }


def build_report_quality_meta_fallback(session: dict, mode: str) -> dict:
    """å›é€€æµç¨‹çš„è´¨é‡å…ƒæ•°æ®ä¼°ç®—ã€‚"""
    evidence_pack = build_report_evidence_pack(session)
    evidence_coverage = float(evidence_pack.get("overall_coverage", 0.0))
    contradiction_total = len(evidence_pack.get("contradictions", []))
    consistency = 1.0 if contradiction_total == 0 else 0.6
    actionability = 0.4
    overall = 0.4 * evidence_coverage + 0.3 * consistency + 0.3 * actionability

    return {
        "mode": mode,
        "evidence_coverage": round(evidence_coverage, 3),
        "consistency": round(consistency, 3),
        "actionability": round(actionability, 3),
        "overall": round(overall, 3),
        "claim_total": 0,
        "claim_with_evidence": 0,
        "review_issue_count": 0,
    }


def render_report_from_draft_v3(session: dict, draft: dict, quality_meta: dict) -> str:
    """å°† V3 ç»“æ„åŒ–è‰æ¡ˆæ¸²æŸ“ä¸º Markdown æŠ¥å‘Šã€‚"""
    topic = session.get("topic", "æœªå‘½åé¡¹ç›®")
    now = datetime.now()
    report_id = f"deep-vision-{now.strftime('%Y%m%d')}"

    needs = draft.get("needs", []) if isinstance(draft.get("needs", []), list) else []
    solutions = draft.get("solutions", []) if isinstance(draft.get("solutions", []), list) else []
    risks = draft.get("risks", []) if isinstance(draft.get("risks", []), list) else []
    actions = draft.get("actions", []) if isinstance(draft.get("actions", []), list) else []
    open_questions = draft.get("open_questions", []) if isinstance(draft.get("open_questions", []), list) else []
    analysis = draft.get("analysis", {}) if isinstance(draft.get("analysis", {}), dict) else {}
    visuals = draft.get("visualizations", {}) if isinstance(draft.get("visualizations", {}), dict) else {}

    def clean_mermaid(raw_value: str, fallback: str) -> str:
        value = str(raw_value or "").replace("```mermaid", "").replace("```", "").strip()
        return value or fallback

    def clamp_score(value: float) -> float:
        return max(0.05, min(0.95, value))

    def build_priority_matrix_for_needs(needs_items: list) -> tuple[str, list]:
        """æ„å»ºä¼˜å…ˆçº§çŸ©é˜µ Mermaid åŠå›¾ä¾‹ä¸­çš„æ•°æ®ç‚¹è¯´æ˜ã€‚"""
        if not needs_items:
            fallback_matrix = """quadrantChart
    title ä¼˜å…ˆçº§çŸ©é˜µ
    x-axis ç´§æ€¥ç¨‹åº¦ï¼ˆå·¦ä½ï¼‰ --> ç´§æ€¥ç¨‹åº¦ï¼ˆå³é«˜ï¼‰
    y-axis é‡è¦ç¨‹åº¦ï¼ˆä¸‹ä½ï¼‰ --> é‡è¦ç¨‹åº¦ï¼ˆä¸Šé«˜ï¼‰
    quadrant-1 ç«‹å³æ‰§è¡Œ
    quadrant-2 è®¡åˆ’æ‰§è¡Œ
    quadrant-3 ä½ä¼˜å…ˆçº§
    quadrant-4 å¯å§”æ´¾
    Req1: [0.78, 0.82]
    Req2: [0.62, 0.72]
    Req3: [0.36, 0.32]"""
            return fallback_matrix, []

        priority_anchor = {
            "P0": (0.84, 0.88),  # å³ä¸Šï¼šç«‹å³æ‰§è¡Œ
            "P1": (0.66, 0.74),  # åä¸Šï¼šè®¡åˆ’æ‰§è¡Œ
            "P2": (0.72, 0.40),  # å³ä¸‹ï¼šå¯å§”æ´¾
            "P3": (0.34, 0.30),  # å·¦ä¸‹ï¼šä½ä¼˜å…ˆçº§
        }

        point_lines = []
        point_legend = []
        for index, item in enumerate(needs_items[:10], 1):
            priority = str(item.get("priority", "P1")).upper()
            if priority not in priority_anchor:
                priority = "P1"
            base_x, base_y = priority_anchor[priority]
            spread = ((index % 4) - 1.5) * 0.03
            x = clamp_score(base_x + spread)
            y = clamp_score(base_y - spread * 0.7)
            point_lines.append(f"    Req{index}: [{x:.2f}, {y:.2f}]")

            name = str(item.get("name", "")).strip() or f"éœ€æ±‚{index}"
            if len(name) > 28:
                name = name[:28] + "..."
            point_legend.append(f"- `Req{index}` = {name}")

        matrix = "\n".join([
            "quadrantChart",
            "    title ä¼˜å…ˆçº§çŸ©é˜µ",
            "    x-axis ç´§æ€¥ç¨‹åº¦ï¼ˆå·¦ä½ï¼‰ --> ç´§æ€¥ç¨‹åº¦ï¼ˆå³é«˜ï¼‰",
            "    y-axis é‡è¦ç¨‹åº¦ï¼ˆä¸‹ä½ï¼‰ --> é‡è¦ç¨‹åº¦ï¼ˆä¸Šé«˜ï¼‰",
            "    quadrant-1 ç«‹å³æ‰§è¡Œ",
            "    quadrant-2 è®¡åˆ’æ‰§è¡Œ",
            "    quadrant-3 ä½ä¼˜å…ˆçº§",
            "    quadrant-4 å¯å§”æ´¾",
            *point_lines,
        ])
        return matrix, point_legend

    priority_quadrant_mermaid_from_draft = clean_mermaid(
        visuals.get("priority_quadrant_mermaid", ""),
        "",
    )
    generated_priority_quadrant_mermaid, priority_point_legend = build_priority_matrix_for_needs(needs)
    priority_quadrant_mermaid = priority_quadrant_mermaid_from_draft or generated_priority_quadrant_mermaid

    flow_mermaid = clean_mermaid(
        visuals.get("business_flow_mermaid", ""),
        """flowchart TD
    A[å¼€å§‹] --> B[éœ€æ±‚æ¾„æ¸…]
    B --> C[æ–¹æ¡ˆè¯„ä¼°]
    C --> D[æ‰§è¡Œè®¡åˆ’]
    D --> E[ç»“æŸ]"""
    )

    pie_mermaid = clean_mermaid(
        visuals.get("demand_pie_mermaid", ""),
        """pie title éœ€æ±‚åˆ†å¸ƒ
    "æ ¸å¿ƒåŠŸèƒ½" : 40
    "æµç¨‹ä¼˜åŒ–" : 30
    "æŠ€æœ¯çº¦æŸ" : 20
    "é£é™©æ²»ç†" : 10"""
    )

    architecture_mermaid = clean_mermaid(
        visuals.get("architecture_mermaid", ""),
        """flowchart LR
    A[å‰ç«¯å…¥å£] --> B[ä¸šåŠ¡æœåŠ¡]
    B --> C[(æ•°æ®å­˜å‚¨)]
    B --> D[ç¬¬ä¸‰æ–¹æœåŠ¡]"""
    )

    needs_table = []
    if needs:
        needs_table.append("| ä¼˜å…ˆçº§ | éœ€æ±‚é¡¹ | æè¿° |")
        needs_table.append("|:---:|:---|:---|")
        for item in needs:
            needs_table.append(
                f"| {item.get('priority', 'P1')} | {item.get('name', '')} | {item.get('description', '')} |"
            )
    else:
        needs_table.append("æš‚æ— ç»“æ„åŒ–æ ¸å¿ƒéœ€æ±‚ã€‚")

    priority_group = {"P0": [], "P1": [], "P2": [], "P3": []}
    for item in needs:
        priority = str(item.get("priority", "P1")).upper()
        priority = priority if priority in priority_group else "P1"
        priority_group[priority].append(item.get("name", "æœªå‘½åéœ€æ±‚"))

    priority_table = [
        "| ä¼˜å…ˆçº§ | éœ€æ±‚é¡¹ | è¯´æ˜ |",
        "|:---:|:---|:---|",
        f"| ğŸ”´ P0 ç«‹å³æ‰§è¡Œ | {'ã€'.join(priority_group['P0']) if priority_group['P0'] else '-'} | é‡è¦ä¸”ç´§æ€¥ï¼Œéœ€ä¼˜å…ˆæŠ•å…¥ |",
        f"| ğŸŸ¡ P1 è®¡åˆ’æ‰§è¡Œ | {'ã€'.join(priority_group['P1']) if priority_group['P1'] else '-'} | é‡è¦ä½†å¯åˆ†é˜¶æ®µæ¨è¿› |",
        f"| ğŸŸ¢ P2 å¯å§”æ´¾ | {'ã€'.join(priority_group['P2']) if priority_group['P2'] else '-'} | å½±å“æœ‰é™ï¼Œå¯å¹¶è¡Œå®‰æ’ |",
        f"| âšª P3 ä½ä¼˜å…ˆçº§ | {'ã€'.join(priority_group['P3']) if priority_group['P3'] else '-'} | å¯å»¶åå¤„ç†å¹¶æŒç»­è§‚å¯Ÿ |",
    ]

    lines = [
        f"# {topic} è®¿è°ˆæŠ¥å‘Š",
        "",
        f"**è®¿è°ˆæ—¥æœŸ**: {now.strftime('%Y-%m-%d')}",
        f"**æŠ¥å‘Šç¼–å·**: {report_id}",
        "",
        "---",
        "",
        "## 1. è®¿è°ˆæ¦‚è¿°",
        "",
        draft.get("overview", "æš‚æ— æ¦‚è¿°ä¿¡æ¯ã€‚"),
        "",
        "## 2. éœ€æ±‚æ‘˜è¦",
        "",
        "### æ ¸å¿ƒéœ€æ±‚åˆ—è¡¨",
        "",
        *needs_table,
        "",
        "### ä¼˜å…ˆçº§çŸ©é˜µï¼ˆMermaidï¼‰",
        "",
        "```mermaid",
        priority_quadrant_mermaid,
        "```",
        "",
        "### å›¾ä¾‹è¯´æ˜",
        "",
        "- æ¨ªè½´ï¼šç´§æ€¥ç¨‹åº¦ï¼ˆå·¦ä½å³é«˜ï¼‰",
        "- çºµè½´ï¼šé‡è¦ç¨‹åº¦ï¼ˆä¸‹ä½ä¸Šé«˜ï¼‰",
        "- å³ä¸Šï¼šç«‹å³æ‰§è¡Œï¼ˆé‡è¦ä¸”ç´§æ€¥ï¼‰",
        "- å·¦ä¸Šï¼šè®¡åˆ’æ‰§è¡Œï¼ˆé‡è¦ä½†ä¸ç´§æ€¥ï¼‰",
        "- å³ä¸‹ï¼šå¯å§”æ´¾ï¼ˆç´§æ€¥ä½†ä¸é‡è¦ï¼‰",
        "- å·¦ä¸‹ï¼šä½ä¼˜å…ˆçº§ï¼ˆä¸é‡è¦ä¸ç´§æ€¥ï¼‰",
        "- æ•°æ®ç‚¹å¯¹åº”å…³ç³»ï¼š",
        *(priority_point_legend if priority_point_legend else ["- æœ¬æ¬¡æ— å¯æ˜ å°„çš„éœ€æ±‚ç‚¹"]),
        "",
        "### ä¼˜å…ˆçº§æ¸…å•",
        "",
        *priority_table,
        "",
        "## 3. è¯¦ç»†éœ€æ±‚åˆ†æ",
        "",
        "### å®¢æˆ·/ç”¨æˆ·éœ€æ±‚",
        analysis.get("customer_needs", "æš‚æ— åˆ†æã€‚"),
        "",
        "### ä¸šåŠ¡æµç¨‹",
        analysis.get("business_flow", "æš‚æ— åˆ†æã€‚"),
        "",
        "### æŠ€æœ¯çº¦æŸ",
        analysis.get("tech_constraints", "æš‚æ— åˆ†æã€‚"),
        "",
        "### é¡¹ç›®çº¦æŸ",
        analysis.get("project_constraints", "æš‚æ— åˆ†æã€‚"),
        "",
        "## 4. å¯è§†åŒ–åˆ†æ",
        "",
        "### ä¸šåŠ¡æµç¨‹å›¾",
        "```mermaid",
        flow_mermaid,
        "```",
        "",
        "### éœ€æ±‚åˆ†ç±»é¥¼å›¾",
        "```mermaid",
        pie_mermaid,
        "```",
        "",
        "### éƒ¨ç½²æ¶æ„å›¾",
        "```mermaid",
        architecture_mermaid,
        "```",
        "",
        "## 5. æ–¹æ¡ˆå»ºè®®",
        "",
    ]

    if solutions:
        for idx, item in enumerate(solutions, 1):
            lines.extend([
                f"### å»ºè®® {idx}: {item.get('title', 'æœªå‘½åå»ºè®®')}",
                f"- è¯´æ˜ï¼š{item.get('description', '')}",
                f"- Ownerï¼š{item.get('owner', '') or 'å¾…å®š'}",
                f"- æ—¶é—´ï¼š{item.get('timeline', '') or 'å¾…å®š'}",
                f"- æŒ‡æ ‡ï¼š{item.get('metric', '') or 'å¾…å®š'}",
                "",
            ])
    else:
        lines.append("æš‚æ— ç»“æ„åŒ–æ–¹æ¡ˆå»ºè®®ã€‚")
        lines.append("")

    lines.extend([
        "## 6. é£é™©è¯„ä¼°",
        "",
    ])
    if risks:
        for idx, item in enumerate(risks, 1):
            lines.extend([
                f"### é£é™© {idx}: {item.get('risk', 'æœªå‘½åé£é™©')}",
                f"- å½±å“ï¼š{item.get('impact', '')}",
                f"- åº”å¯¹ï¼š{item.get('mitigation', '')}",
                "",
            ])
    else:
        lines.append("æš‚æ— ç»“æ„åŒ–é£é™©é¡¹ã€‚")
        lines.append("")

    lines.extend([
        "## 7. ä¸‹ä¸€æ­¥è¡ŒåŠ¨",
        "",
    ])
    if actions:
        for idx, item in enumerate(actions, 1):
            lines.extend([
                f"### è¡ŒåŠ¨ {idx}: {item.get('action', 'æœªå‘½åè¡ŒåŠ¨')}",
                f"- Ownerï¼š{item.get('owner', '') or 'å¾…å®š'}",
                f"- æ—¶é—´ï¼š{item.get('timeline', '') or 'å¾…å®š'}",
                f"- æŒ‡æ ‡ï¼š{item.get('metric', '') or 'å¾…å®š'}",
                "",
            ])
    else:
        lines.append("æš‚æ— ç»“æ„åŒ–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚")
        lines.append("")

    lines.append("### æœªå†³é—®é¢˜æ¸…å•")
    lines.append("")
    if open_questions:
        for idx, item in enumerate(open_questions, 1):
            lines.extend([
                f"{idx}. **é—®é¢˜**ï¼š{item.get('question', '')}",
                f"   - åŸå› ï¼š{item.get('reason', '')}",
                f"   - å½±å“ï¼š{item.get('impact', '')}",
                f"   - å»ºè®®è¡¥é—®ï¼š{item.get('suggested_follow_up', '')}",
                "",
            ])
    else:
        lines.append("æš‚æ— æœªå†³é—®é¢˜ã€‚")
        lines.append("")

    lines.extend([
        "*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³ç”Ÿæˆ*",
        "",
    ])

    return "\n".join(lines)


def generate_report_v3_pipeline(session: dict, session_id: Optional[str] = None) -> Optional[dict]:
    """æ‰§è¡Œ V3 æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿ã€‚å¤±è´¥æ—¶è¿”å›åŒ…å« reason çš„è°ƒè¯•ç»“æ„ã€‚"""
    try:
        report_draft_max_tokens = min(MAX_TOKENS_REPORT, 7000)
        report_review_max_tokens = min(MAX_TOKENS_REPORT, 6000)

        evidence_pack = build_report_evidence_pack(session)

        if session_id:
            update_report_generation_status(session_id, "building_prompt", message="æ­£åœ¨æ„å»ºè¯æ®åŒ…å¹¶ç”Ÿæˆç»“æ„åŒ–è‰æ¡ˆ...")

        draft_prompt = build_report_draft_prompt_v3(session, evidence_pack)
        draft_raw = call_claude(
            draft_prompt,
            max_tokens=report_draft_max_tokens,
            call_type="report_v3_draft",
            timeout=REPORT_API_TIMEOUT,
        )
        if not draft_raw:
            return {
                "status": "failed",
                "reason": "draft_generation_failed",
                "evidence_pack": evidence_pack,
                "review_issues": [],
            }

        draft_parsed = parse_structured_json_response(draft_raw, required_keys=["overview", "needs", "analysis"])
        if not draft_parsed:
            return {
                "status": "failed",
                "reason": "draft_parse_failed",
                "evidence_pack": evidence_pack,
                "review_issues": [],
            }

        current_draft, local_issues = validate_report_draft_v3(draft_parsed, evidence_pack)
        review_issues = list(local_issues)
        base_review_rounds = 3  # åˆå®¡ + æœ€å¤š2è½®å®šå‘ä¿®å¤
        quality_fix_rounds = 1  # è´¨é‡é—¨ç¦ä¸è¾¾æ ‡æ—¶é¢å¤–è¡¥ä¿®1è½®
        total_round_budget = base_review_rounds + quality_fix_rounds
        remaining_quality_fix_rounds = quality_fix_rounds
        final_issues = list(local_issues)

        for review_round in range(total_round_budget):
            if session_id:
                update_report_generation_status(
                    session_id,
                    "generating",
                    message=f"æ­£åœ¨æ‰§è¡ŒæŠ¥å‘Šä¸€è‡´æ€§å®¡ç¨¿ï¼ˆç¬¬{review_round + 1}/{total_round_budget}è½®ï¼‰..."
                )

            review_prompt = build_report_review_prompt_v3(session, evidence_pack, current_draft, review_issues)
            review_raw = call_claude(
                review_prompt,
                max_tokens=report_review_max_tokens,
                call_type=f"report_v3_review_round_{review_round + 1}",
                timeout=REPORT_API_TIMEOUT,
            )
            if not review_raw:
                return {
                    "status": "failed",
                    "reason": "review_generation_failed",
                    "evidence_pack": evidence_pack,
                    "review_issues": final_issues,
                }

            review_parsed = parse_report_review_response_v3(review_raw)
            if not review_parsed:
                return {
                    "status": "failed",
                    "reason": "review_parse_failed",
                    "evidence_pack": evidence_pack,
                    "review_issues": final_issues,
                }

            if review_parsed.get("revised_draft"):
                current_draft = review_parsed["revised_draft"]

            current_draft, local_issues = validate_report_draft_v3(current_draft, evidence_pack)
            merged_issues = []
            seen_keys = set()
            for item in (review_parsed.get("issues", []) + local_issues):
                key = f"{item.get('type')}|{item.get('target')}|{item.get('message')}"
                if key in seen_keys:
                    continue
                seen_keys.add(key)
                merged_issues.append(item)
            final_issues = merged_issues

            passed = bool(review_parsed.get("passed", False)) and len(merged_issues) == 0
            if passed:
                quality_meta = compute_report_quality_meta_v3(current_draft, evidence_pack, final_issues)
                quality_gate_issues = build_quality_gate_issues_v3(quality_meta)
                if quality_gate_issues:
                    final_issues = quality_gate_issues
                    if remaining_quality_fix_rounds <= 0:
                        break
                    remaining_quality_fix_rounds -= 1
                    review_issues = quality_gate_issues
                    continue

                report_content = render_report_from_draft_v3(session, current_draft, quality_meta)
                return {
                    "status": "success",
                    "report_content": report_content,
                    "quality_meta": quality_meta,
                    "evidence_pack": evidence_pack,
                    "review_issues": final_issues,
                }

            review_issues = merged_issues

        return {
            "status": "failed",
            "reason": "review_not_passed_or_quality_gate_failed",
            "evidence_pack": evidence_pack,
            "review_issues": final_issues,
        }
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ V3 æŠ¥å‘Šæµç¨‹å¤±è´¥: {e}")
        return {
            "status": "failed",
            "reason": "exception",
            "error": str(e)[:200],
            "evidence_pack": {},
            "review_issues": [],
        }


async def call_claude_async(prompt: str, max_tokens: int = None,
                            call_type: str = "async", model_name: str = "") -> Optional[str]:
    """å¼‚æ­¥è°ƒç”¨ Claude APIï¼Œå¸¦è¶…æ—¶æ§åˆ¶"""
    if not claude_client:
        return None

    if max_tokens is None:
        max_tokens = MAX_TOKENS_DEFAULT
    effective_model = resolve_model_name(call_type=call_type, model_name=model_name)

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– å¼‚æ­¥è°ƒç”¨ Claude APIï¼Œmodel={effective_model}ï¼Œmax_tokens={max_tokens}ï¼Œtimeout={API_TIMEOUT}s")

        # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
        message = claude_client.messages.create(
            model=effective_model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            timeout=API_TIMEOUT
        )

        response_text = extract_message_text(message)
        if not response_text:
            raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«å¯ç”¨æ–‡æœ¬å†…å®¹")

        if ENABLE_DEBUG_LOG:
            print(f"âœ… API å¼‚æ­¥å“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")

        return response_text
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Claude API å¼‚æ­¥è°ƒç”¨å¤±è´¥: {error_msg}")

        if "timeout" in error_msg.lower():
            print(f"   åŸå› : API è°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{API_TIMEOUT}ç§’ï¼‰")
        elif "rate" in error_msg.lower():
            print(f"   åŸå› : API è¯·æ±‚é¢‘ç‡é™åˆ¶")
        elif "authentication" in error_msg.lower() or "api key" in error_msg.lower():
            print(f"   åŸå› : API Key è®¤è¯å¤±è´¥")

        return None


def describe_image_with_vision(image_path: Path, filename: str) -> str:
    """
    ä½¿ç”¨æ™ºè°±è§†è§‰æ¨¡å‹æè¿°å›¾ç‰‡å†…å®¹

    Args:
        image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        filename: åŸå§‹æ–‡ä»¶å

    Returns:
        str: å›¾ç‰‡æè¿°æ–‡æœ¬
    """
    if not ENABLE_VISION:
        return f"[å›¾ç‰‡: {filename}] (è§†è§‰åŠŸèƒ½å·²ç¦ç”¨)"

    if not ZHIPU_API_KEY or ZHIPU_API_KEY == "your-zhipu-api-key-here":
        return f"[å›¾ç‰‡: {filename}] (è§†è§‰ API æœªé…ç½®)"

    try:
        # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
        with open(image_path, "rb") as f:
            image_data = f.read()

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size_mb = len(image_data) / (1024 * 1024)
        if size_mb > MAX_IMAGE_SIZE_MB:
            return f"[å›¾ç‰‡: {filename}] (æ–‡ä»¶è¿‡å¤§: {size_mb:.1f}MB > {MAX_IMAGE_SIZE_MB}MB)"

        # ç¡®å®š MIME ç±»å‹
        ext = Path(filename).suffix.lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        mime_type = mime_types.get(ext, 'image/jpeg')

        base64_image = base64.b64encode(image_data).decode('utf-8')

        # æ„å»ºè¯·æ±‚
        headers = {
            "Authorization": f"Bearer {ZHIPU_API_KEY}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": VISION_MODEL_NAME,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. å›¾ç‰‡ä¸­çš„å…³é”®å…ƒç´ ï¼ˆäººç‰©ã€ç‰©ä½“ã€æ–‡å­—ç­‰ï¼‰
3. å¦‚æœæ˜¯æµç¨‹å›¾/æ¶æ„å›¾/å›¾è¡¨ï¼Œè¯·è§£è¯»å…¶å«ä¹‰
4. å¦‚æœæœ‰æ–‡å­—ï¼Œè¯·æå–ä¸»è¦æ–‡å­—å†…å®¹

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹å°½é‡å®Œæ•´å‡†ç¡®ã€‚"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 1000
        }

        if ENABLE_DEBUG_LOG:
            print(f"ğŸ–¼ï¸ è°ƒç”¨è§†è§‰ API æè¿°å›¾ç‰‡: {filename}")

        response = requests.post(
            VISION_API_URL,
            headers=headers,
            json=payload,
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            description = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            if description:
                if ENABLE_DEBUG_LOG:
                    print(f"âœ… å›¾ç‰‡æè¿°ç”ŸæˆæˆåŠŸ: {len(description)} å­—ç¬¦")
                return f"[å›¾ç‰‡: {filename}]\n\n**AI å›¾ç‰‡æè¿°:**\n{description}"
            else:
                return f"[å›¾ç‰‡: {filename}] (æè¿°ç”Ÿæˆå¤±è´¥: ç©ºå“åº”)"
        else:
            error_msg = response.json().get("error", {}).get("message", response.text[:200])
            if ENABLE_DEBUG_LOG:
                print(f"âŒ è§†è§‰ API è°ƒç”¨å¤±è´¥: {error_msg}")
            return f"[å›¾ç‰‡: {filename}] (API é”™è¯¯: {error_msg[:100]})"

    except requests.exceptions.Timeout:
        return f"[å›¾ç‰‡: {filename}] (API è¶…æ—¶)"
    except Exception as e:
        if ENABLE_DEBUG_LOG:
            print(f"âŒ å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {e}")
        return f"[å›¾ç‰‡: {filename}] (å¤„ç†å¤±è´¥: {str(e)[:100]})"


def call_claude(prompt: str, max_tokens: int = None, retry_on_timeout: bool = True,
                call_type: str = "unknown", truncated_docs: list = None,
                timeout: float = None, model_name: str = "") -> Optional[str]:
    """åŒæ­¥è°ƒç”¨ Claude APIï¼Œå¸¦è¶…æ—¶æ§åˆ¶å’Œå®¹é”™æœºåˆ¶"""
    import time

    if not claude_client:
        return None

    if max_tokens is None:
        max_tokens = MAX_TOKENS_DEFAULT

    effective_timeout = timeout if timeout is not None else API_TIMEOUT
    effective_model = resolve_model_name(call_type=call_type, model_name=model_name)

    start_time = time.time()
    success = False
    timeout_occurred = False
    error_message = None
    response_text = None

    try:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¤– è°ƒç”¨ Claude APIï¼Œmodel={effective_model}ï¼Œmax_tokens={max_tokens}ï¼Œtimeout={effective_timeout}s")

        # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
        message = claude_client.messages.create(
            model=effective_model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            timeout=effective_timeout
        )

        response_text = extract_message_text(message)
        if not response_text:
            raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«å¯ç”¨æ–‡æœ¬å†…å®¹")
        success = True

        if ENABLE_DEBUG_LOG:
            print(f"âœ… API å“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")

    except Exception as e:
        error_message = str(e)
        print(f"âŒ Claude API è°ƒç”¨å¤±è´¥: {error_message}")

        # è¯¦ç»†çš„é”™è¯¯åˆ†ç±»å’Œå®¹é”™å¤„ç†
        if "timeout" in error_message.lower():
            timeout_occurred = True
            print(f"   åŸå› : API è°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{effective_timeout}ç§’ï¼‰")

            # è¶…æ—¶å®¹é”™ï¼šå¦‚æœå…è®¸é‡è¯•ï¼Œå°è¯•å‡å°‘ prompt é•¿åº¦
            if retry_on_timeout and len(prompt) > 5000:
                print(f"   ğŸ”„ å°è¯•å®¹é”™é‡è¯•ï¼šæˆªæ–­ prompt åé‡è¯•...")
                # æˆªæ–­ prompt åˆ°åŸæ¥çš„ 70%
                truncated_prompt = prompt[:int(len(prompt) * 0.7)]
                truncated_prompt += "\n\n[æ³¨æ„ï¼šç”±äºå†…å®¹è¿‡é•¿ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡å·²è¢«æˆªæ–­ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œå›ç­”]"

                # æŠ¥å‘Šç”Ÿæˆæ›´å®¹æ˜“è§¦å‘é•¿å“åº”è¶…æ—¶ï¼Œé‡è¯•æ—¶åŒæ­¥æ”¶æ•›è¾“å‡ºé•¿åº¦å¹¶æ”¾å®½è¶…æ—¶
                is_report_call = "report" in (call_type or "").lower()
                retry_max_tokens = max_tokens
                retry_timeout = effective_timeout
                if is_report_call:
                    retry_max_tokens = max(3000, int(max_tokens * 0.65))
                    retry_timeout = max(effective_timeout, REPORT_API_TIMEOUT)
                else:
                    retry_max_tokens = max(1000, int(max_tokens * 0.8))

                # é€’å½’é‡è¯•ï¼ˆç¦æ­¢å†æ¬¡é‡è¯•ï¼‰
                response_text = call_claude(
                    truncated_prompt, retry_max_tokens,
                    retry_on_timeout=False,
                    call_type=call_type + "_retry",
                    truncated_docs=truncated_docs,
                    timeout=retry_timeout,
                    model_name=effective_model
                )

                if response_text:
                    success = True

        elif "rate" in error_message.lower():
            print(f"   åŸå› : API è¯·æ±‚é¢‘ç‡é™åˆ¶")
        elif "authentication" in error_message.lower() or "api key" in error_message.lower():
            print(f"   åŸå› : API Key è®¤è¯å¤±è´¥")

    finally:
        # è®°å½•æŒ‡æ ‡
        response_time = time.time() - start_time
        metrics_collector.record_api_call(
            call_type=call_type,
            prompt_length=len(prompt),
            response_time=response_time,
            success=success,
            timeout=timeout_occurred,
            error_msg=error_message if not success else None,
            truncated_docs=truncated_docs,
            max_tokens=max_tokens
        )

    return response_text


# ============ é™æ€æ–‡ä»¶ ============

@app.route('/')
def index():
    return send_from_directory('.', 'index.html')


@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory('.', filename)


# ============ åœºæ™¯ API ============

@app.route('/api/scenarios', methods=['GET'])
def list_scenarios():
    """è·å–æ‰€æœ‰åœºæ™¯åˆ—è¡¨"""
    scenarios = scenario_loader.get_all_scenarios()
    # è¿”å›ç®€åŒ–çš„åœºæ™¯ä¿¡æ¯
    return jsonify([
        {
            "id": s.get("id"),
            "name": s.get("name"),
            "name_en": s.get("name_en"),
            "description": s.get("description"),
            "icon": s.get("icon"),
            "builtin": s.get("builtin", True),
            "custom": s.get("custom", False),
            "dimensions": [
                {
                    "id": d.get("id"),
                    "name": d.get("name"),
                    "description": d.get("description")
                }
                for d in s.get("dimensions", [])
            ],
            "report_type": s.get("report", {}).get("type", "standard")
        }
        for s in scenarios
    ])


@app.route('/api/scenarios/<scenario_id>', methods=['GET'])
def get_scenario(scenario_id):
    """è·å–åœºæ™¯è¯¦æƒ…"""
    scenario = scenario_loader.get_scenario(scenario_id)
    if not scenario:
        return jsonify({"error": "åœºæ™¯ä¸å­˜åœ¨"}), 404
    return jsonify(scenario)


@app.route('/api/scenarios/generate', methods=['POST'])
def generate_scenario_with_ai():
    """AI è‡ªåŠ¨ç”Ÿæˆåœºæ™¯é…ç½®"""
    if not claude_client:
        return jsonify({"error": "AI æœåŠ¡ä¸å¯ç”¨"}), 503

    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    user_description = data.get("user_description", "").strip()
    if not user_description:
        return jsonify({"error": "è¯·è¾“å…¥åœºæ™¯æè¿°"}), 400

    if len(user_description) < 10:
        return jsonify({"error": "æè¿°å¤ªçŸ­ï¼Œè¯·è‡³å°‘è¾“å…¥10ä¸ªå­—"}), 400

    if len(user_description) > 500:
        return jsonify({"error": "æè¿°ä¸èƒ½è¶…è¿‡500å­—"}), 400

    # æ„å»º Prompt
    prompt = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¿è°ˆåœºæ™¯è®¾è®¡å¸ˆã€‚ç”¨æˆ·å°†æè¿°ä»–ä»¬æƒ³è¦è¿›è¡Œçš„è®¿è°ˆæˆ–è°ƒç ”ç›®æ ‡ï¼Œä½ éœ€è¦è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„è®¿è°ˆåœºæ™¯é…ç½®ã€‚

## ç”¨æˆ·æè¿°
{user_description}

## è®¾è®¡è¦æ±‚
1. åœºæ™¯åç§°ï¼šç®€æ´æ˜äº†ï¼Œ4-10ä¸ªå­—
2. åœºæ™¯æè¿°ï¼šè¯´æ˜åœºæ™¯é€‚ç”¨èŒƒå›´ï¼Œ20-50å­—
3. å…³é”®è¯ï¼š5-10ä¸ªç”¨äºè‡ªåŠ¨åŒ¹é…çš„å…³é”®è¯
4. ç»´åº¦è®¾è®¡ï¼š3-5ä¸ªç»´åº¦ï¼ˆæ ¹æ®è®¿è°ˆå¤æ‚åº¦è°ƒæ•´ï¼‰
   - æ¯ä¸ªç»´åº¦éœ€è¦æœ‰æ¸…æ™°çš„åç§°ï¼ˆ2-6å­—ï¼‰
   - ç»´åº¦æè¿°è¯´æ˜è¯¥ç»´åº¦å…³æ³¨çš„å†…å®¹ï¼ˆ10-30å­—ï¼‰
   - æ¯ä¸ªç»´åº¦åŒ…å«3-5ä¸ªå…³é”®ç‚¹ï¼ˆkey_aspectsï¼‰
   - min_questions å›ºå®šä¸º 2ï¼Œmax_questions å›ºå®šä¸º 4

## è®¾è®¡åŸåˆ™
- ç»´åº¦ä¹‹é—´åº”è¯¥äº’è¡¥ï¼Œå…±åŒè¦†ç›–ç”¨æˆ·å…³å¿ƒçš„æ‰€æœ‰æ–¹é¢
- ç»´åº¦é¡ºåºåº”è¯¥ç¬¦åˆè®¤çŸ¥é€»è¾‘ï¼ˆå¦‚ä»å…·ä½“åˆ°æŠ½è±¡ï¼Œä»æ ¸å¿ƒåˆ°å¤–å›´ï¼‰
- å…³é”®ç‚¹åº”è¯¥å…·ä½“å¯é—®ï¼Œä¾¿äºAIç”Ÿæˆè®¿è°ˆé—®é¢˜
- å¦‚æœç”¨æˆ·æè¿°æ¶‰åŠè¯„ä¼°/è¯„åˆ†ç±»åœºæ™¯ï¼Œå¯è€ƒè™‘åœ¨ç»´åº¦ä¸­åŠ å…¥è¯„åˆ†ç›¸å…³çš„å…³é”®ç‚¹

## å‚è€ƒï¼šç°æœ‰åœºæ™¯ç¤ºä¾‹
- äº§å“éœ€æ±‚ï¼šå®¢æˆ·éœ€æ±‚ã€ä¸šåŠ¡æµç¨‹ã€æŠ€æœ¯çº¦æŸã€é¡¹ç›®çº¦æŸ
- ç”¨æˆ·ç ”ç©¶ï¼šç”¨æˆ·èƒŒæ™¯ã€ä½¿ç”¨åœºæ™¯ã€ç—›ç‚¹æœŸæœ›ã€è¡Œä¸ºæ¨¡å¼
- ç«å“åˆ†æï¼šå¸‚åœºå®šä½ã€åŠŸèƒ½å¯¹æ¯”ã€ç”¨æˆ·è¯„ä»·ã€å·®å¼‚åŒ–æœºä¼š

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼š
```json
{{
  "name": "åœºæ™¯åç§°",
  "description": "åœºæ™¯æè¿°",
  "dimensions": [
    {{
      "id": "dim_1",
      "name": "ç»´åº¦åç§°",
      "description": "ç»´åº¦æè¿°",
      "key_aspects": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3", "å…³é”®ç‚¹4"],
      "min_questions": 2,
      "max_questions": 4
    }}
  ],
  "explanation": "è®¾è®¡æ€è·¯è¯´æ˜ï¼ˆ1-2å¥è¯ï¼Œè§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ç»´åº¦ï¼‰"
}}
```'''

    try:
        response = claude_client.messages.create(
            model=resolve_model_name(call_type="scenario_generate"),
            max_tokens=1500,
            timeout=30.0,
            messages=[{"role": "user", "content": prompt}]
        )

        raw_text = extract_message_text(response)
        if not raw_text:
            raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«åœºæ™¯é…ç½®æ–‡æœ¬")

        # æå– JSONï¼ˆå…¼å®¹æ¨¡å‹è¿”å› markdown ä»£ç å—ï¼‰
        if "```json" in raw_text:
            raw_text = raw_text.split("```json")[1].split("```")[0].strip()
        elif "```" in raw_text:
            raw_text = raw_text.split("```")[1].split("```")[0].strip()

        generated = json.loads(raw_text)

        # éªŒè¯å¿…è¦å­—æ®µ
        if not generated.get("name"):
            return jsonify({"error": "ç”Ÿæˆçš„åœºæ™¯ç¼ºå°‘åç§°"}), 500

        if not generated.get("dimensions") or len(generated["dimensions"]) < 1:
            return jsonify({"error": "ç”Ÿæˆçš„åœºæ™¯ç¼ºå°‘ç»´åº¦"}), 500

        # æå– explanation å¹¶ç§»é™¤ï¼ˆä¸å­˜å…¥åœºæ™¯é…ç½®ï¼‰
        ai_explanation = generated.pop("explanation", "")

        # ç¡®ä¿ç»´åº¦æ ¼å¼æ­£ç¡®
        for i, dim in enumerate(generated["dimensions"]):
            dim["id"] = f"dim_{i + 1}"
            dim.setdefault("min_questions", 2)
            dim.setdefault("max_questions", 4)
            if not isinstance(dim.get("key_aspects"), list):
                dim["key_aspects"] = []

        # æ·»åŠ é»˜è®¤çš„ report é…ç½®
        generated["report"] = {"type": "standard"}

        return jsonify({
            "success": True,
            "generated_scenario": generated,
            "ai_explanation": ai_explanation
        })

    except json.JSONDecodeError as e:
        print(f"âš ï¸ AI ç”Ÿæˆåœºæ™¯ JSON è§£æå¤±è´¥: {e}")
        return jsonify({"error": "AI è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè¯·é‡è¯•"}), 500
    except Exception as e:
        print(f"âš ï¸ AI ç”Ÿæˆåœºæ™¯å¤±è´¥: {e}")
        return jsonify({"error": f"ç”Ÿæˆå¤±è´¥: {str(e)[:100]}"}), 500


@app.route('/api/scenarios/custom', methods=['POST'])
def create_custom_scenario():
    """åˆ›å»ºè‡ªå®šä¹‰åœºæ™¯"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    name = data.get("name", "").strip()
    if not name:
        return jsonify({"error": "åœºæ™¯åç§°ä¸èƒ½ä¸ºç©º"}), 400

    dimensions = data.get("dimensions", [])
    if not dimensions or len(dimensions) < 1:
        return jsonify({"error": "è‡³å°‘éœ€è¦ 1 ä¸ªç»´åº¦"}), 400
    if len(dimensions) > 8:
        return jsonify({"error": "æœ€å¤šæ”¯æŒ 8 ä¸ªç»´åº¦"}), 400

    # éªŒè¯ç»´åº¦æ•°æ®
    for i, dim in enumerate(dimensions):
        if not dim.get("name", "").strip():
            return jsonify({"error": f"ç¬¬ {i+1} ä¸ªç»´åº¦åç§°ä¸èƒ½ä¸ºç©º"}), 400
        # è‡ªåŠ¨ç”Ÿæˆç»´åº¦ ID
        if not dim.get("id"):
            dim["id"] = f"dim_{i+1}"
        # ç¡®ä¿æœ‰å¿…è¦å­—æ®µ
        dim.setdefault("description", "")
        dim.setdefault("key_aspects", [])
        dim.setdefault("min_questions", 2)
        dim.setdefault("max_questions", 4)

    scenario = {
        "name": name,
        "description": data.get("description", "").strip(),
        "icon": data.get("icon", "clipboard-list"),
        "dimensions": dimensions,
        "report": data.get("report", {"type": "standard"}),
    }

    scenario_id = scenario_loader.save_custom_scenario(scenario)

    return jsonify({
        "success": True,
        "scenario_id": scenario_id,
        "scenario": scenario_loader.get_scenario(scenario_id)
    })


@app.route('/api/scenarios/custom/<scenario_id>', methods=['DELETE'])
def delete_custom_scenario(scenario_id):
    """åˆ é™¤è‡ªå®šä¹‰åœºæ™¯"""
    if not scenario_id.startswith("custom-"):
        return jsonify({"error": "åªèƒ½åˆ é™¤è‡ªå®šä¹‰åœºæ™¯"}), 400

    success = scenario_loader.delete_custom_scenario(scenario_id)
    if not success:
        return jsonify({"error": "åœºæ™¯ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤"}), 404

    return jsonify({"success": True})


@app.route('/api/scenarios/recognize', methods=['POST'])
def recognize_scenario():
    """æ ¹æ®ä¸»é¢˜å’Œæè¿°æ™ºèƒ½è¯†åˆ«æœ€åŒ¹é…çš„è®¿è°ˆåœºæ™¯"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    topic = data.get("topic", "")
    description = data.get("description", "")
    if not topic:
        return jsonify({"error": "ä¸»é¢˜ä¸èƒ½ä¸ºç©º"}), 400

    # æ„å»ºåœºæ™¯æ‘˜è¦ä¾› AI åˆ¤æ–­
    all_scenarios = scenario_loader.get_all_scenarios()
    scenario_list_text = "\n".join(
        f"- id: {s['id']}, åç§°: {s['name']}, è¯´æ˜: {s.get('description', '')}"
        for s in all_scenarios
    )

    user_input = f"è®¿è°ˆä¸»é¢˜ï¼š{topic}"
    if description:
        user_input += f"\nä¸»é¢˜æè¿°ï¼š{description}"

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®¿è°ˆåœºæ™¯åˆ†ç±»å™¨ã€‚æ ¹æ®ç”¨æˆ·çš„è®¿è°ˆä¸»é¢˜å’Œæè¿°ï¼Œä»ä»¥ä¸‹åœºæ™¯ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªã€‚

å¯é€‰åœºæ™¯ï¼š
{scenario_list_text}

{user_input}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{"scenario_id": "æœ€åŒ¹é…çš„åœºæ™¯id", "confidence": 0.0åˆ°1.0çš„ç½®ä¿¡åº¦, "reason": "ä¸€å¥è¯ç†ç”±"}}"""

    valid_scenario_ids = {s["id"] for s in all_scenarios}

    # ä¼˜å…ˆä½¿ç”¨ AI è¯†åˆ«ï¼Œå¤±è´¥æ—¶å›é€€åˆ°å…³é”®è¯åŒ¹é…
    ai_result = None
    ai_last_error = None
    if claude_client:
        attempts = [
            {
                "max_tokens": 220,
                "timeout": 10.0,
                "extra_instruction": "",
            },
            {
                "max_tokens": 480,
                "timeout": 18.0,
                "extra_instruction": "åªè¾“å‡ºä¸€è¡Œ JSONï¼Œä¸è¦ markdown ä»£ç å—ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œä¸è¦æ¢è¡Œã€‚",
            },
        ]

        for idx, attempt in enumerate(attempts, start=1):
            try:
                attempt_prompt = prompt
                if attempt["extra_instruction"]:
                    attempt_prompt = f"{prompt}\n\nã€é¢å¤–è¦æ±‚ã€‘{attempt['extra_instruction']}"

                response = claude_client.messages.create(
                    model=resolve_model_name(call_type="scenario_recognize"),
                    max_tokens=attempt["max_tokens"],
                    timeout=attempt["timeout"],
                    messages=[{"role": "user", "content": attempt_prompt}]
                )

                raw = extract_message_text(response)
                if not raw:
                    raise ValueError("æ¨¡å‹å“åº”ä¸­æœªåŒ…å«åœºæ™¯è¯†åˆ«æ–‡æœ¬")

                parsed = parse_scenario_recognition_response(raw, valid_scenario_ids)
                if parsed:
                    ai_result = parsed
                    break

                raise ValueError(f"æ— æ³•ä»å“åº”æå–æœ‰æ•ˆåœºæ™¯ç»“æœï¼Œå“åº”å‰120å­—: {raw[:120]}")
            except Exception as e:
                ai_last_error = e
                if ENABLE_DEBUG_LOG:
                    print(f"âš ï¸  AI åœºæ™¯è¯†åˆ«ç¬¬{idx}æ¬¡å¤±è´¥: {e}")

        if not ai_result and ai_last_error:
            print(f"âš ï¸  AI åœºæ™¯è¯†åˆ«å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {ai_last_error}")

    if ai_result and ai_result.get("scenario_id") in valid_scenario_ids:
        best_id = ai_result["scenario_id"]
        confidence = min(1.0, max(0.0, float(ai_result.get("confidence", 0.8))))
        reason = ai_result.get("reason", "")
    else:
        # å›é€€ï¼šå…³é”®è¯åŒ¹é…
        kw_result = scenario_loader.match_by_keywords(topic)
        best_id = kw_result["scenario_id"]
        confidence = kw_result["confidence"]
        reason = ""

    recommended_scenario = scenario_loader.get_scenario(best_id)

    return jsonify({
        "recommended": {
            "id": best_id,
            "name": recommended_scenario.get("name") if recommended_scenario else best_id,
            "description": recommended_scenario.get("description") if recommended_scenario else "",
            "icon": recommended_scenario.get("icon") if recommended_scenario else "clipboard-list",
            "dimensions_count": len(recommended_scenario.get("dimensions", [])) if recommended_scenario else 4
        },
        "confidence": confidence,
        "reason": reason
    })


# ============ è®¤è¯ API ============

@app.route('/api/auth/register', methods=['POST'])
def auth_register():
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    account = str(data.get("account", "")).strip()
    password = str(data.get("password", ""))

    email, phone, account_error = normalize_account(account)
    if account_error:
        return jsonify({"error": account_error}), 400

    if len(password) < 8 or len(password) > 64:
        return jsonify({"error": "å¯†ç é•¿åº¦éœ€ä¸º 8-64 ä½"}), 400

    if query_user_by_account(email, phone):
        return jsonify({"error": "è¯¥è´¦å·å·²æ³¨å†Œ"}), 409

    now_iso = datetime.now(timezone.utc).isoformat()
    password_hash = generate_password_hash(password, method="pbkdf2:sha256")

    try:
        with get_auth_db_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO users (email, phone, password_hash, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?)
                """,
                (email, phone, password_hash, now_iso, now_iso)
            )
            conn.commit()
            user_id = cursor.lastrowid
    except sqlite3.IntegrityError:
        return jsonify({"error": "è¯¥è´¦å·å·²æ³¨å†Œ"}), 409

    user_row = query_user_by_id(user_id)
    if not user_row:
        return jsonify({"error": "æ³¨å†Œå¤±è´¥ï¼Œè¯·é‡è¯•"}), 500

    login_user(user_row)
    return jsonify({"user": build_user_payload(user_row)}), 201


@app.route('/api/auth/login', methods=['POST'])
def auth_login():
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    account = str(data.get("account", "")).strip()
    password = str(data.get("password", ""))

    if not account or not password:
        return jsonify({"error": "è´¦å·å’Œå¯†ç ä¸èƒ½ä¸ºç©º"}), 400

    email, phone, account_error = normalize_account(account)
    if account_error:
        return jsonify({"error": account_error}), 400

    user_row = query_user_by_account(email, phone)
    if not user_row or not check_password_hash(user_row["password_hash"], password):
        return jsonify({"error": "è´¦å·æˆ–å¯†ç é”™è¯¯"}), 401

    login_user(user_row)
    return jsonify({"user": build_user_payload(user_row)})


@app.route('/api/auth/logout', methods=['POST'])
def auth_logout():
    session.clear()
    return jsonify({"success": True})


@app.route('/api/auth/me', methods=['GET'])
def auth_me():
    user_row = get_current_user()
    if not user_row:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401
    return jsonify({"user": build_user_payload(user_row)})


# ============ ä¼šè¯ API ============

@app.route('/api/sessions', methods=['GET'])
def list_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    sessions = []
    for f in SESSIONS_DIR.glob("*.json"):
        try:
            data = json.loads(f.read_text(encoding="utf-8"))
            if not is_session_owned_by_user(data, user_id):
                continue
            session_id = data.get("session_id")
            report_generation = None
            if session_id:
                status_record = get_report_generation_record(session_id)
                if status_record and not bool(status_record.get("active")) and is_report_generation_worker_alive(session_id):
                    status_state = str(status_record.get("state") or "").strip()
                    if status_state not in {"completed", "failed", "cancelled"}:
                        update_report_generation_status(session_id, "queued", message="æŠ¥å‘Šä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­...")
                        status_record = get_report_generation_record(session_id)
                payload = build_report_generation_payload(status_record)
                if payload.get("active"):
                    report_generation = {
                        "active": True,
                        "state": payload.get("state", "queued"),
                        "progress": payload.get("progress", 0),
                        "message": payload.get("message", ""),
                        "updated_at": payload.get("updated_at"),
                        "action": payload.get("action", "generate"),
                    }
            sessions.append({
                "session_id": session_id,
                "topic": data.get("topic"),
                "status": data.get("status"),
                "created_at": data.get("created_at"),
                "updated_at": data.get("updated_at"),
                "dimensions": data.get("dimensions", {}),
                "interview_count": len(data.get("interview_log", [])),
                "scenario_id": data.get("scenario_id"),
                "scenario_config": data.get("scenario_config"),
                "report_generation": report_generation,
            })
        except Exception as e:
            print(f"è¯»å–ä¼šè¯å¤±è´¥ {f}: {e}")

    sessions.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
    return jsonify(sessions)


@app.route('/api/sessions', methods=['POST'])
def create_session():
    """åˆ›å»ºæ–°ä¼šè¯"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    topic = data.get("topic", "æœªå‘½åè®¿è°ˆ")
    description = data.get("description")  # è·å–å¯é€‰çš„ä¸»é¢˜æè¿°
    interview_mode = data.get("interview_mode", DEFAULT_INTERVIEW_MODE)  # è·å–è®¿è°ˆæ¨¡å¼
    scenario_id = data.get("scenario_id")  # è·å–åœºæ™¯ID

    # éªŒè¯ topic
    if not isinstance(topic, str) or not topic.strip():
        return jsonify({"error": "ä¸»é¢˜ä¸èƒ½ä¸ºç©º"}), 400
    if len(topic) > 200:
        return jsonify({"error": "ä¸»é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡200å­—ç¬¦"}), 400

    # éªŒè¯ description
    if description and (not isinstance(description, str) or len(description) > 2000):
        return jsonify({"error": "æè¿°é•¿åº¦ä¸èƒ½è¶…è¿‡2000å­—ç¬¦"}), 400

    # éªŒè¯è®¿è°ˆæ¨¡å¼
    if interview_mode not in INTERVIEW_MODES:
        interview_mode = DEFAULT_INTERVIEW_MODE

    # åŠ è½½åœºæ™¯é…ç½®ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯ï¼‰
    if not scenario_id:
        scenario_id = "product-requirement"
    scenario_config = scenario_loader.get_scenario(scenario_id)
    if not scenario_config:
        scenario_config = scenario_loader.get_default_scenario()
        scenario_id = scenario_config.get("id", "product-requirement")

    # æ ¹æ®åœºæ™¯é…ç½®åˆ›å»ºåŠ¨æ€ç»´åº¦
    dimensions = {}
    for dim in scenario_config.get("dimensions", []):
        dimensions[dim["id"]] = {
            "coverage": 0,
            "items": [],
            "score": None  # ç”¨äºè¯„ä¼°å‹åœºæ™¯
        }

    session_id = generate_session_id()
    now = get_utc_now()

    session = {
        "session_id": session_id,
        "owner_user_id": user_id,
        "topic": topic,
        "description": description,  # å­˜å‚¨ä¸»é¢˜æè¿°
        "interview_mode": interview_mode,  # å­˜å‚¨è®¿è°ˆæ¨¡å¼
        "created_at": now,
        "updated_at": now,
        "status": "in_progress",
        "scenario_id": scenario_id,  # å­˜å‚¨åœºæ™¯ID
        "scenario_config": scenario_config,  # å­˜å‚¨åœºæ™¯å®Œæ•´é…ç½®
        "dimensions": dimensions,  # åŠ¨æ€ç»´åº¦
        "reference_materials": [],  # å‚è€ƒèµ„æ–™ï¼ˆåˆå¹¶åŸ reference_docs å’Œ research_docsï¼‰
        "interview_log": [],
        "requirements": [],
        "summary": None
    }

    session["depth_v2"] = {
        "enabled": True,
        "mode": interview_mode,
        "skip_followup_confirm": DEEP_MODE_SKIP_FOLLOWUP_CONFIRM,
    }

    session_file = SESSIONS_DIR / f"{session_id}.json"
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # ========== æ­¥éª¤6: é¢„ç”Ÿæˆé¦–é¢˜ ==========
    try:
        prefetch_first_question(session_id)
    except Exception as e:
        # é¢„ç”Ÿæˆå¤±è´¥ä¸å½±å“ä¼šè¯åˆ›å»º
        print(f"âš ï¸ é¢„ç”Ÿæˆé¦–é¢˜å¤±è´¥: {e}")

    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """è·å–ä¼šè¯è¯¦æƒ…"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded

    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯æ ¼å¼
    session = migrate_session_docs(session)
    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['PUT'])
def update_session(session_id):
    """æ›´æ–°ä¼šè¯"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    updates = request.get_json()
    session_file, session = loaded

    if not isinstance(updates, dict):
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    # å®šä¹‰å…è®¸æ›´æ–°çš„å­—æ®µç™½åå•
    UPDATABLE_FIELDS = {"description", "topic", "status"}

    for key, value in updates.items():
        if key != "session_id" and key in UPDATABLE_FIELDS:
            session[key] = value

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify(session)


@app.route('/api/sessions/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id, include_missing=True)
    session_file, _session, state = loaded

    if state == "not_found":
        return jsonify({"error": "ä¼šè¯ä¸å­˜åœ¨"}), 404

    session_file.unlink()

    # ========== æ­¥éª¤7: æ¸…ç†ç¼“å­˜å’ŒçŠ¶æ€ ==========
    invalidate_prefetch(session_id)
    clear_thinking_status(session_id)
    clear_report_generation_status(session_id)

    return jsonify({"success": True})


@app.route('/api/sessions/batch-delete', methods=['POST'])
def batch_delete_sessions():
    """æ‰¹é‡åˆ é™¤ä¼šè¯ï¼ˆé»˜è®¤å…è®¸åˆ é™¤è¿›è¡Œä¸­çš„ä¼šè¯ï¼‰ã€‚"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    data = request.get_json(silent=True) or {}
    session_ids = unique_non_empty_strings(data.get("session_ids", []))
    delete_reports = bool(data.get("delete_reports", False))
    skip_in_progress = bool(data.get("skip_in_progress", False))

    if not session_ids:
        return jsonify({"error": "session_ids ä¸èƒ½ä¸ºç©º"}), 400

    deleted_sessions = []
    skipped_sessions = []
    missing_sessions = []
    deleted_reports = set()

    for session_id in session_ids:
        session_file = SESSIONS_DIR / f"{session_id}.json"
        if not session_file.exists():
            missing_sessions.append(session_id)
            continue

        session = safe_load_session(session_file)
        if session is None:
            skipped_sessions.append({"session_id": session_id, "reason": "ä¼šè¯æ•°æ®æŸå"})
            continue

        if not is_session_owned_by_user(session, user_id):
            missing_sessions.append(session_id)
            continue

        if skip_in_progress:
            effective_status = get_effective_session_status(session)
            if effective_status == "in_progress":
                skipped_sessions.append({"session_id": session_id, "reason": "ä¼šè¯è¿›è¡Œä¸­ï¼Œå·²æŒ‰è®¾ç½®è·³è¿‡"})
                continue

        if delete_reports:
            linked_reports = find_reports_by_session_topic(session)
            for report_name in linked_reports:
                report_file = REPORTS_DIR / report_name
                if report_file.exists():
                    mark_report_as_deleted(report_name)
                    deleted_reports.add(report_name)

        try:
            session_file.unlink()
            invalidate_prefetch(session_id)
            clear_thinking_status(session_id)
            deleted_sessions.append(session_id)
        except Exception as exc:
            skipped_sessions.append({"session_id": session_id, "reason": f"åˆ é™¤å¤±è´¥: {str(exc)}"})

    return jsonify({
        "success": True,
        "requested": len(session_ids),
        "deleted_sessions": deleted_sessions,
        "deleted_reports": sorted(deleted_reports),
        "skipped_sessions": skipped_sessions,
        "missing_sessions": missing_sessions
    })


# ============ AI é©±åŠ¨çš„è®¿è°ˆ API ============

def parse_question_response(response: str, debug: bool = False) -> Optional[dict]:
    """è§£æ AI è¿”å›çš„é—®é¢˜ JSON å“åº”

    ä½¿ç”¨5ç§é€’è¿›å¼è§£æç­–ç•¥ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦æå–æœ‰æ•ˆJSONã€‚

    Args:
        response: AI è¿”å›çš„åŸå§‹å“åº”æ–‡æœ¬
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•æ—¥å¿—

    Returns:
        è§£æåçš„ dictï¼ˆåŒ…å« question å’Œ optionsï¼‰ï¼Œå¤±è´¥è¿”å› None
    """
    import re

    result = None
    parse_error = None

    if debug:
        print(f"ğŸ“ AI åŸå§‹å“åº” (å‰500å­—): {response[:500]}")

    # æ–¹æ³•1: ç›´æ¥å°è¯•è§£æï¼ˆå¦‚æœAIä¸¥æ ¼éµå®ˆæŒ‡ä»¤ï¼‰
    try:
        cleaned = response.strip()
        if cleaned.startswith('{') and cleaned.endswith('}'):
            result = json.loads(cleaned)
            if debug:
                print(f"âœ… æ–¹æ³•1æˆåŠŸ: ç›´æ¥è§£æ")
    except json.JSONDecodeError as e:
        parse_error = e
        if debug:
            print(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")

    # æ–¹æ³•2: å°è¯•æå– ```json ä»£ç å—
    if result is None and "```json" in response:
        try:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end > json_start:
                json_str = response[json_start:json_end].strip()
                result = json.loads(json_str)
                if debug:
                    print(f"âœ… æ–¹æ³•2æˆåŠŸ: ä»ä»£ç å—æå–")
        except json.JSONDecodeError as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•2å¤±è´¥ (JSONé”™è¯¯): {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•2å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆèŠ±æ‹¬å·é…å¯¹ï¼‰
    if result is None:
        try:
            json_start = response.find('{')
            if json_start >= 0:
                brace_count = 0
                json_end = -1
                in_string = False
                escape_next = False

                for i in range(json_start, len(response)):
                    char = response[i]

                    if escape_next:
                        escape_next = False
                        continue

                    if char == '\\':
                        escape_next = True
                        continue

                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue

                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_end = i + 1
                                break

                if json_end > json_start:
                    try:
                        json_str = response[json_start:json_end]
                        result = json.loads(json_str)
                        if debug:
                            print(f"âœ… æ–¹æ³•3æˆåŠŸ: èŠ±æ‹¬å·é…å¯¹æå–")
                    except json.JSONDecodeError as e:
                        parse_error = e
                        if debug:
                            print(f"âš ï¸ æ–¹æ³•3å¤±è´¥ (JSONé”™è¯¯): {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•3å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•4: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– JSON å¯¹è±¡
    if result is None:
        try:
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.findall(json_pattern, response, re.DOTALL)
            for match in matches:
                try:
                    candidate = json.loads(match)
                    # éªŒè¯å¿…é¡»æœ‰ question å­—æ®µ
                    if isinstance(candidate, dict) and "question" in candidate:
                        result = candidate
                        if debug:
                            print(f"âœ… æ–¹æ³•4æˆåŠŸ: æ­£åˆ™è¡¨è¾¾å¼æå–")
                        break
                except json.JSONDecodeError:
                    continue
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•4å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # æ–¹æ³•5: å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSONï¼ˆè¡¥å…¨ç¼ºå¤±å­—æ®µï¼‰
    if result is None and '{' in response and '"question"' in response:
        try:
            if debug:
                print(f"ğŸ”§ å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSON...")

            # æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹ä½ç½®
            json_start = response.find('{')
            json_content = response[json_start:]

            # å°è¯•è¡¥å…¨ç¼ºå¤±çš„ç»“å°¾éƒ¨åˆ†
            if '"options"' in json_content and '"question"' in json_content:
                # å¦‚æœæœ‰optionsæ•°ç»„ä½†æ²¡æœ‰æ­£ç¡®ç»“æŸï¼Œå°è¯•è¡¥å…¨
                if json_content.count('[') > json_content.count(']'):
                    json_content += ']'
                if json_content.count('{') > json_content.count('}'):
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    if '"multi_select"' not in json_content:
                        json_content += ', "multi_select": false'
                    if '"is_follow_up"' not in json_content:
                        json_content += ', "is_follow_up": false'
                    json_content += '}'

                # å°è¯•è§£æä¿®å¤åçš„JSON
                try:
                    result = json.loads(json_content)
                    if isinstance(result, dict) and "question" in result:
                        if debug:
                            print(f"âœ… æ–¹æ³•5æˆåŠŸ: JSONä¿®å¤å®Œæˆ")
                except json.JSONDecodeError as e:
                    if debug:
                        print(f"âš ï¸ æ–¹æ³•5å¤±è´¥: ä¿®å¤åä»æ— æ³•è§£æ - {e}")
        except Exception as e:
            parse_error = e
            if debug:
                print(f"âš ï¸ æ–¹æ³•5å¤±è´¥ (å…¶ä»–é”™è¯¯): {e}")

    # éªŒè¯ç»“æœ
    if result is not None and isinstance(result, dict):
        if "question" in result and "options" in result:
            # è¡¥å…¨å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
            if "multi_select" not in result:
                result["multi_select"] = False
            if "is_follow_up" not in result:
                result["is_follow_up"] = False
            return result

    # æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥äº†
    if debug:
        print(f"âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥")
        print(f"ğŸ“„ AI å“åº”å‰500å­—ç¬¦:\n{response[:500] if response else 'None'}")
        print(f"ğŸ“„ æœ€åè§£æé”™è¯¯: {str(parse_error) if parse_error else 'æœªçŸ¥'}")

    return None

@app.route('/api/sessions/<session_id>/next-question', methods=['POST'])
def get_next_question(session_id):
    """è·å–ä¸‹ä¸€ä¸ªé—®é¢˜ï¼ˆAI ç”Ÿæˆï¼‰"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded
    data = request.get_json() or {}
    default_dim = get_dimension_order_for_session(session)[0] if get_dimension_order_for_session(session) else "customer_needs"
    dimension = data.get("dimension", default_dim)

    # ========== æ­¥éª¤5: æ£€æŸ¥é¢„ç”Ÿæˆç¼“å­˜ ==========
    prefetched = get_prefetch_result(session_id, dimension)
    if prefetched:
        if ENABLE_DEBUG_LOG:
            print(f"ğŸ¯ é¢„ç”Ÿæˆç¼“å­˜å‘½ä¸­: session={session_id}, dimension={dimension}")

        # å…ˆæ£€æŸ¥ç»´åº¦æ˜¯å¦å·²å®Œæˆï¼ˆå³ä½¿æœ‰ç¼“å­˜ä¹Ÿè¦æ£€æŸ¥ï¼‰
        dim_data = session.get("dimensions", {}).get(dimension, {})
        dim_coverage = dim_data.get("coverage", 0)
        user_completed = dim_data.get("user_completed", False)
        if dim_coverage >= 100 or user_completed:
            # ç»´åº¦å·²å®Œæˆï¼Œå¿½ç•¥ç¼“å­˜ï¼Œè¿”å›å®ŒæˆçŠ¶æ€
            all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
            formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])
            dim_follow_ups = len([log for log in all_dim_logs if log.get("is_follow_up", False)])
            completion_reason = dim_data.get("completion_reason") or ("user_completed" if user_completed else "auto_completed")
            quality_warning = bool(dim_data.get("quality_warning", False))
            return jsonify({
                "dimension": dimension,
                "completed": True,
                "completion_reason": completion_reason,
                "quality_warning": quality_warning,
                "decision_meta": {
                    "mode": get_mode_identifier(session),
                    "follow_up_round": get_follow_up_round_for_dimension_logs(all_dim_logs),
                    "remaining_question_follow_up_budget": max(
                        0,
                        get_interview_mode_config(session).get("max_questions_per_formal", 1)
                        - get_follow_up_round_for_dimension_logs(all_dim_logs)
                    ),
                    "hard_triggered": False,
                    "missing_aspects": get_dimension_missing_aspects(session, dimension),
                },
                "stats": {
                    "formal_questions": formal_questions_count,
                    "follow_ups": dim_follow_ups,
                    "saturation": 1.0
                }
            })

        # å¯¹é¢„ç”Ÿæˆç»“æœä¹Ÿåš is_follow_up æ ¡éªŒ
        all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
        if prefetched.get("is_follow_up", False) and all_dim_logs:
            last_log = all_dim_logs[-1]
            eval_result = evaluate_answer_depth(
                question=last_log.get("question", ""),
                answer=last_log.get("answer", ""),
                dimension=dimension,
                options=last_log.get("options", []),
                is_follow_up=last_log.get("is_follow_up", False),
            )
            comprehensive_check = should_follow_up_comprehensive(
                session=session,
                dimension=dimension,
                rule_based_result=eval_result
            )
            if not comprehensive_check["should_follow_up"]:
                if ENABLE_DEBUG_LOG:
                    print(f"âš ï¸ é¢„ç”Ÿæˆç¼“å­˜ is_follow_up=true ä½†åç«¯å†³ç­–ä¸å…è®¸ï¼Œå¼ºåˆ¶è¦†ç›–ä¸º false")
                prefetched["is_follow_up"] = False
                prefetched["follow_up_reason"] = None

        prefetched["decision_meta"] = {
            "mode": get_mode_identifier(session),
            "follow_up_round": get_follow_up_round_for_dimension_logs(all_dim_logs),
            "remaining_question_follow_up_budget": max(
                0,
                get_interview_mode_config(session).get("max_questions_per_formal", 1)
                - get_follow_up_round_for_dimension_logs(all_dim_logs)
            ),
            "hard_triggered": False,
            "missing_aspects": get_dimension_missing_aspects(session, dimension),
        }

        prefetched["prefetched"] = True
        return jsonify(prefetched)

    # æ£€æŸ¥æ˜¯å¦æœ‰ Claude API
    if not claude_client:
        return jsonify({
            "error": "AI æœåŠ¡æœªå¯ç”¨",
            "detail": "è¯·è”ç³»ç®¡ç†å‘˜é…ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡"
        }), 503

    # è·å–å½“å‰ç»´åº¦çš„æ‰€æœ‰è®°å½•
    all_dim_logs = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]

    # è®¡ç®—æ­£å¼é—®é¢˜æ•°é‡ï¼ˆæ’é™¤è¿½é—®ï¼‰
    formal_questions_count = len([log for log in all_dim_logs if not log.get("is_follow_up", False)])

    # è·å–è®¿è°ˆæ¨¡å¼é…ç½®
    mode_config = get_interview_mode_config(session)

    # è·å–å½“å‰ç»´åº¦çŠ¶æ€
    dim_data = session.get("dimensions", {}).get(dimension, {})
    dim_coverage = dim_data.get("coverage", 0)
    user_completed = dim_data.get("user_completed", False)

    # ç»´åº¦å·²å®Œæˆï¼ˆç”¨æˆ·æ‰‹åŠ¨å®Œæˆæˆ–è‡ªåŠ¨å®Œæˆï¼‰
    if dim_coverage >= 100 or user_completed:
        dim_follow_ups = len([log for log in all_dim_logs if log.get("is_follow_up", False)])
        completion_reason = dim_data.get("completion_reason") or ("user_completed" if user_completed else "auto_completed")
        quality_warning = bool(dim_data.get("quality_warning", False))
        return jsonify({
            "dimension": dimension,
            "completed": True,
            "completion_reason": completion_reason,
            "quality_warning": quality_warning,
            "decision_meta": {
                "mode": get_mode_identifier(session),
                "follow_up_round": get_follow_up_round_for_dimension_logs(all_dim_logs),
                "remaining_question_follow_up_budget": max(0, mode_config.get("max_questions_per_formal", 1) - get_follow_up_round_for_dimension_logs(all_dim_logs)),
                "hard_triggered": False,
                "missing_aspects": get_dimension_missing_aspects(session, dimension),
            },
            "stats": {
                "formal_questions": formal_questions_count,
                "follow_ups": dim_follow_ups,
                "saturation": 1.0
            }
        })

    completion = evaluate_dimension_completion_v2(session, dimension)
    if completion.get("can_complete"):
        # è‡ªåŠ¨å®Œæˆéœ€æŒä¹…åŒ–ï¼Œå¦åˆ™åç»­è¯»å–ä¼šå‡ºç°è¦†ç›–ç‡å›é€€
        dim_state = session.setdefault("dimensions", {}).setdefault(dimension, {})
        dim_state["coverage"] = 100
        dim_state["auto_completed"] = True
        dim_state["completion_reason"] = completion.get("reason") or "quality_gate_passed"
        dim_state["quality_warning"] = bool(completion.get("quality_warning", False))

        session["updated_at"] = get_utc_now()
        session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

        dim_follow_ups = len([log for log in all_dim_logs if log.get("is_follow_up", False)])
        snapshot = completion.get("snapshot", {})
        return jsonify({
            "dimension": dimension,
            "completed": True,
            "completion_reason": dim_state.get("completion_reason"),
            "quality_warning": bool(dim_state.get("quality_warning", False)),
            "decision_meta": {
                "mode": get_mode_identifier(session),
                "follow_up_round": snapshot.get("follow_up_round", get_follow_up_round_for_dimension_logs(all_dim_logs)),
                "remaining_question_follow_up_budget": max(0, mode_config.get("max_questions_per_formal", 1) - snapshot.get("follow_up_round", 0)),
                "hard_triggered": bool(snapshot.get("pending_forced_follow_up", False)),
                "missing_aspects": snapshot.get("missing_aspects", []),
            },
            "stats": {
                "formal_questions": formal_questions_count,
                "follow_ups": dim_follow_ups,
                "saturation": snapshot.get("saturation", 0)
            }
        })

    # è°ƒç”¨ Claude ç”Ÿæˆé—®é¢˜
    # åˆ¤æ–­æ˜¯å¦ä¼šæœ‰æœç´¢ï¼ˆç”¨äºè®¾ç½®æ­£ç¡®çš„é˜¶æ®µæ•°ï¼‰
    has_search = should_search(session.get("topic", ""), dimension, session)

    try:
        # é˜¶æ®µ1: åˆ†æå›ç­”
        update_thinking_status(session_id, "analyzing", has_search)

        prompt, truncated_docs, decision_meta = build_interview_prompt(session, dimension, all_dim_logs, session_id=session_id)

        # æ—¥å¿—ï¼šè®°å½• prompt é•¿åº¦ï¼ˆä¾¿äºç›‘æ§å’Œè°ƒä¼˜ï¼‰
        if ENABLE_DEBUG_LOG:
            ref_docs_count = len(session.get("reference_materials", session.get("reference_docs", []) + session.get("research_docs", [])))
            print(f"ğŸ“Š è®¿è°ˆ Prompt ç»Ÿè®¡ï¼šæ€»é•¿åº¦={len(prompt)}å­—ç¬¦ï¼Œå‚è€ƒèµ„æ–™={ref_docs_count}ä¸ª")
            if truncated_docs:
                print(f"âš ï¸  æ–‡æ¡£æˆªæ–­ï¼š{len(truncated_docs)}ä¸ªæ–‡æ¡£è¢«æˆªæ–­")

        # é˜¶æ®µ3: ç”Ÿæˆé—®é¢˜
        update_thinking_status(session_id, "generating", has_search)

        response = call_claude(
            prompt,
            max_tokens=MAX_TOKENS_QUESTION,
            call_type="question",
            truncated_docs=truncated_docs
        )

        if not response:
            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            clear_thinking_status(session_id)
            return jsonify({
                "error": "AI å“åº”å¤±è´¥",
                "detail": "æœªèƒ½ä» AI æœåŠ¡è·å–å“åº”ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
            }), 503

        # ä½¿ç”¨æŠ½å–çš„è§£æå‡½æ•°è§£æ JSON å“åº”
        result = parse_question_response(response, debug=ENABLE_DEBUG_LOG)

        if result:
            result["dimension"] = dimension
            result["ai_generated"] = True
            result["decision_meta"] = decision_meta
            # å…œåº•ï¼šé¿å…è¿ç»­é‡å¤é—®é¢˜ï¼ˆæœ€å¤šè‡ªåŠ¨é‡è¯•ä¸€æ¬¡ï¼‰
            last_log = all_dim_logs[-1] if all_dim_logs else None
            if last_log and last_log.get("question") == result.get("question"):
                if ENABLE_DEBUG_LOG:
                    print("âš ï¸ æ£€æµ‹åˆ°é‡å¤é—®é¢˜ï¼Œè‡ªåŠ¨é‡è¯•ä¸€æ¬¡")
                retry_response = call_claude(
                    prompt,
                    max_tokens=MAX_TOKENS_QUESTION,
                    call_type="question",
                    truncated_docs=truncated_docs
                )
                retry_result = parse_question_response(retry_response, debug=ENABLE_DEBUG_LOG) if retry_response else None
                if retry_result and retry_result.get("question") != last_log.get("question"):
                    retry_result["dimension"] = dimension
                    retry_result["ai_generated"] = True
                    retry_result["decision_meta"] = decision_meta
                    result = retry_result
                else:
                    # æ¸…é™¤æ€è€ƒçŠ¶æ€
                    clear_thinking_status(session_id)
                    return jsonify({
                        "error": "ç”Ÿæˆé‡å¤é—®é¢˜",
                        "detail": "æ£€æµ‹åˆ°é‡å¤é—®é¢˜ï¼Œè¯·é‡è¯•"
                    }), 503

            # ========== åç«¯å¼ºåˆ¶æ ¡éªŒ is_follow_up ==========
            # é˜²æ­¢ AI ç»•è¿‡è¿½é—®é¢„ç®—æ§åˆ¶ï¼Œè‡ªè¡Œå°†é—®é¢˜æ ‡è®°ä¸ºè¿½é—®
            if result.get("is_follow_up", False):
                # é‡æ–°è®¡ç®—è¿½é—®å†³ç­–
                last_log = all_dim_logs[-1] if all_dim_logs else None
                if last_log:
                    eval_result = evaluate_answer_depth(
                        question=last_log.get("question", ""),
                        answer=last_log.get("answer", ""),
                        dimension=dimension,
                        options=last_log.get("options", []),
                        is_follow_up=last_log.get("is_follow_up", False),
                    )
                    comprehensive_check = should_follow_up_comprehensive(
                        session=session,
                        dimension=dimension,
                        rule_based_result=eval_result
                    )
                    if not comprehensive_check["should_follow_up"]:
                        if ENABLE_DEBUG_LOG:
                            print(f"âš ï¸ AI è¿”å› is_follow_up=true ä½†åç«¯å†³ç­–ä¸å…è®¸è¿½é—®ï¼Œå¼ºåˆ¶è¦†ç›–ä¸º false (åŸå› : {comprehensive_check['reason']})")
                        result["is_follow_up"] = False
                        result["follow_up_reason"] = None

            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            clear_thinking_status(session_id)
            # ========== æ­¥éª¤5: è§¦å‘é¢„ç”Ÿæˆï¼ˆå¦‚æœéœ€è¦ï¼‰==========
            trigger_prefetch_if_needed(session, dimension)
            return jsonify(result)

        # è§£æå¤±è´¥
        # æ¸…é™¤æ€è€ƒçŠ¶æ€
        clear_thinking_status(session_id)
        return jsonify({
            "error": "AI å“åº”æ ¼å¼é”™è¯¯",
            "detail": "AI è¿”å›çš„å†…å®¹æ— æ³•è§£æä¸ºæœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚è¯·ç‚¹å‡»ã€Œé‡è¯•ã€æŒ‰é’®é‡æ–°ç”Ÿæˆé—®é¢˜ã€‚"
        }), 503

    except Exception as e:
        # æ¸…é™¤æ€è€ƒçŠ¶æ€
        clear_thinking_status(session_id)
        print(f"ç”Ÿæˆé—®é¢˜æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        error_msg = str(e)

        # æ ¹æ®å¼‚å¸¸ç±»å‹æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        if "connection" in error_msg.lower() or "network" in error_msg.lower():
            return jsonify({
                "error": "ç½‘ç»œè¿æ¥å¤±è´¥",
                "detail": "æ— æ³•è¿æ¥åˆ° AI æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            }), 503
        elif "timeout" in error_msg.lower():
            return jsonify({
                "error": "è¯·æ±‚è¶…æ—¶",
                "detail": "AI æœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
            }), 503
        elif "authentication" in error_msg.lower() or "api key" in error_msg.lower():
            return jsonify({
                "error": "API è®¤è¯å¤±è´¥",
                "detail": "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }), 503
        elif "rate limit" in error_msg.lower():
            return jsonify({
                "error": "è¯·æ±‚é¢‘ç‡è¶…é™",
                "detail": "AI æœåŠ¡è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"
            }), 503
        else:
            return jsonify({
                "error": "ç”Ÿæˆé—®é¢˜å¤±è´¥",
                "detail": f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"
            }), 503


def get_fallback_question(session: dict, dimension: str) -> dict:
    """è·å–å¤‡ç”¨é—®é¢˜ï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    fallback_questions = {
        "customer_needs": [
            {"question": "æ‚¨å¸Œæœ›é€šè¿‡è¿™ä¸ªé¡¹ç›®è§£å†³å“ªäº›æ ¸å¿ƒé—®é¢˜ï¼Ÿ", "options": ["æå‡å·¥ä½œæ•ˆç‡", "é™ä½è¿è¥æˆæœ¬", "æ”¹å–„ç”¨æˆ·ä½“éªŒ", "å¢å¼ºæ•°æ®åˆ†æèƒ½åŠ›"], "multi_select": True},
            {"question": "ä¸»è¦çš„ç”¨æˆ·ç¾¤ä½“æœ‰å“ªäº›ï¼Ÿ", "options": ["å†…éƒ¨å‘˜å·¥", "å¤–éƒ¨å®¢æˆ·", "åˆä½œä¼™ä¼´", "ç®¡ç†å±‚"], "multi_select": True},
            {"question": "ç”¨æˆ·æœ€æœŸæœ›è·å¾—çš„æ ¸å¿ƒä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["èŠ‚çœæ—¶é—´", "å‡å°‘é”™è¯¯", "è·å–æ´å¯Ÿ", "æå‡åä½œ"], "multi_select": False},
        ],
        "business_process": [
            {"question": "å½“å‰ä¸šåŠ¡æµç¨‹ä¸­éœ€è¦ä¼˜åŒ–çš„ç¯èŠ‚æœ‰å“ªäº›ï¼Ÿ", "options": ["æ•°æ®å½•å…¥", "å®¡æ‰¹æµç¨‹", "æŠ¥è¡¨ç”Ÿæˆ", "è·¨éƒ¨é—¨åä½œ"], "multi_select": True},
            {"question": "å…³é”®ä¸šåŠ¡æµç¨‹æ¶‰åŠå“ªäº›éƒ¨é—¨ï¼Ÿ", "options": ["é”€å”®éƒ¨é—¨", "æŠ€æœ¯éƒ¨é—¨", "è´¢åŠ¡éƒ¨é—¨", "è¿è¥éƒ¨é—¨"], "multi_select": True},
            {"question": "æµç¨‹ä¸­æœ€å…³é”®çš„å†³ç­–èŠ‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["å®¡æ‰¹èŠ‚ç‚¹", "åˆ†é…èŠ‚ç‚¹", "éªŒæ”¶èŠ‚ç‚¹", "ç»“ç®—èŠ‚ç‚¹"], "multi_select": False},
        ],
        "tech_constraints": [
            {"question": "æœŸæœ›çš„ç³»ç»Ÿéƒ¨ç½²æ–¹å¼æ˜¯ï¼Ÿ", "options": ["å…¬æœ‰äº‘éƒ¨ç½²", "ç§æœ‰äº‘éƒ¨ç½²", "æ··åˆäº‘éƒ¨ç½²", "æœ¬åœ°éƒ¨ç½²"], "multi_select": False},
            {"question": "éœ€è¦ä¸å“ªäº›ç°æœ‰ç³»ç»Ÿé›†æˆï¼Ÿ", "options": ["ERPç³»ç»Ÿ", "CRMç³»ç»Ÿ", "OAåŠå…¬ç³»ç»Ÿ", "è´¢åŠ¡ç³»ç»Ÿ"], "multi_select": True},
            {"question": "å¯¹ç³»ç»Ÿå®‰å…¨æ€§çš„è¦æ±‚æ˜¯ï¼Ÿ", "options": ["ç­‰ä¿äºŒçº§", "ç­‰ä¿ä¸‰çº§", "åŸºç¡€å®‰å…¨å³å¯", "éœ€è¦è¯¦ç»†è¯„ä¼°"], "multi_select": False},
        ],
        "project_constraints": [
            {"question": "é¡¹ç›®çš„é¢„æœŸé¢„ç®—èŒƒå›´æ˜¯ï¼Ÿ", "options": ["10ä¸‡ä»¥å†…", "10-50ä¸‡", "50-100ä¸‡", "100ä¸‡ä»¥ä¸Š"], "multi_select": False},
            {"question": "æœŸæœ›çš„ä¸Šçº¿æ—¶é—´æ˜¯ï¼Ÿ", "options": ["1ä¸ªæœˆå†…", "1-3ä¸ªæœˆ", "3-6ä¸ªæœˆ", "6ä¸ªæœˆä»¥ä¸Š"], "multi_select": False},
            {"question": "é¡¹ç›®å›¢é˜Ÿçš„èµ„æºæƒ…å†µå¦‚ä½•ï¼Ÿ", "options": ["æœ‰ä¸“èŒå›¢é˜Ÿ", "å…¼èŒå‚ä¸", "å®Œå…¨å¤–åŒ…", "éœ€è¦è¯„ä¼°"], "multi_select": False},
        ]
    }

    # è·å–è¯¥ç»´åº¦å·²å›ç­”çš„é—®é¢˜æ•°
    answered = len([log for log in session.get("interview_log", []) if log.get("dimension") == dimension])
    questions = fallback_questions.get(dimension, [])

    if answered < len(questions):
        q = questions[answered]
        return {
            "question": q["question"],
            "options": q["options"],
            "multi_select": q.get("multi_select", False),
            "dimension": dimension,
            "ai_generated": False,
            "is_follow_up": False,
            "ai_recommendation": None
        }

    # ç»´åº¦å·²å®Œæˆ
    return {
        "question": None,
        "dimension": dimension,
        "completed": True,
        "ai_recommendation": None
    }


@app.route('/api/sessions/<session_id>/submit-answer', methods=['POST'])
def submit_answer(session_id):
    """æäº¤å›ç­”"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded

    # éªŒè¯è¯·æ±‚æ•°æ®
    data = request.get_json()
    if not data:
        return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

    question = data.get("question")
    answer = data.get("answer")
    dimension = data.get("dimension")
    options = data.get("options", [])
    is_follow_up = data.get("is_follow_up", False)

    # éªŒè¯å¿…éœ€å‚æ•°
    if not question or not isinstance(question, str):
        return jsonify({"error": "é—®é¢˜ä¸èƒ½ä¸ºç©º"}), 400
    if not answer or not isinstance(answer, str):
        return jsonify({"error": "ç­”æ¡ˆä¸èƒ½ä¸ºç©º"}), 400
    if len(answer) > 5000:
        return jsonify({"error": "ç­”æ¡ˆé•¿åº¦ä¸èƒ½è¶…è¿‡5000å­—ç¬¦"}), 400
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400
    if not isinstance(options, list):
        return jsonify({"error": "é€‰é¡¹å¿…é¡»æ˜¯åˆ—è¡¨"}), 400
    if not isinstance(is_follow_up, bool):
        return jsonify({"error": "is_follow_upå¿…é¡»æ˜¯å¸ƒå°”å€¼"}), 400

    # ä½¿ç”¨å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°åˆ¤æ–­å›ç­”æ˜¯å¦éœ€è¦è¿½é—®
    eval_result = evaluate_answer_depth(
        question=question,
        answer=answer,
        dimension=dimension,
        options=options,
        is_follow_up=is_follow_up,
    )
    needs_follow_up = eval_result["needs_follow_up"]
    follow_up_signals = eval_result["signals"]

    dim_logs_before = [log for log in session.get("interview_log", []) if log.get("dimension") == dimension]
    follow_up_round = get_follow_up_round_for_dimension_logs(dim_logs_before)
    if not is_follow_up:
        follow_up_round = 0
    else:
        follow_up_round += 1

    quality_result = evaluate_answer_quality(
        eval_result=eval_result,
        answer=answer,
        is_follow_up=is_follow_up,
        follow_up_round=follow_up_round,
    )

    if ENABLE_DEBUG_LOG and (needs_follow_up or eval_result["suggest_ai_eval"]):
        print(f"ğŸ“ å›ç­”è¯„ä¼°: signals={follow_up_signals}, needs_follow_up={needs_follow_up}")

    # æ·»åŠ åˆ°è®¿è°ˆè®°å½•
    log_entry = {
        "timestamp": get_utc_now(),
        "question": question,
        "answer": answer,
        "dimension": dimension,
        "options": options,
        "is_follow_up": is_follow_up,
        "needs_follow_up": needs_follow_up,
        "follow_up_signals": follow_up_signals,  # è®°å½•æ£€æµ‹åˆ°çš„ä¿¡å·
        "quality_score": quality_result["quality_score"],
        "quality_signals": quality_result["quality_signals"],
        "hard_triggered": quality_result["hard_triggered"],
        "follow_up_round": follow_up_round,
    }
    session["interview_log"].append(log_entry)

    # æ›´æ–°ç»´åº¦æ•°æ®ï¼ˆåªæœ‰æ­£å¼é—®é¢˜æ‰æ·»åŠ åˆ°ç»´åº¦éœ€æ±‚åˆ—è¡¨ï¼‰
    if dimension and dimension in session["dimensions"] and not is_follow_up:
        session["dimensions"][dimension]["items"].append({
            "name": answer,
            "description": question,
            "priority": "ä¸­"
        })

    # è®¡ç®—è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼Œè¿½é—®ä¸è®¡å…¥ï¼‰
    if dimension and dimension in session["dimensions"]:
        session["dimensions"][dimension]["coverage"] = calculate_dimension_coverage(session, dimension)

    # è¯„ä¼°åœºæ™¯ï¼šä¸ºæ¯æ¬¡å›ç­”è¿›è¡Œ AI è¯„åˆ†
    is_assessment = session.get("scenario_config", {}).get("report", {}).get("type") == "assessment"
    if is_assessment and dimension and dimension in session["dimensions"]:
        score = score_assessment_answer(session, dimension, question, answer)
        if score is not None:
            # è®°å½•æœ¬æ¬¡å›ç­”çš„è¯„åˆ†
            log_entry["score"] = score
            # æ›´æ–°ç»´åº¦çš„ç»¼åˆè¯„åˆ†ï¼ˆå–è¯¥ç»´åº¦æ‰€æœ‰è¯„åˆ†çš„å¹³å‡å€¼ï¼‰
            dim_scores = [
                log.get("score") for log in session["interview_log"]
                if log.get("dimension") == dimension and log.get("score") is not None
            ]
            if dim_scores:
                session["dimensions"][dimension]["score"] = round(sum(dim_scores) / len(dim_scores), 2)
            if ENABLE_DEBUG_LOG:
                print(f"ğŸ“Š è¯„ä¼°è¯„åˆ†: {dimension} = {score}åˆ†ï¼Œç»´åº¦å‡åˆ† = {session['dimensions'][dimension].get('score')}")

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # å¼‚æ­¥æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆè¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
    import threading
    def async_update_summary():
        try:
            update_context_summary(session, session_file)
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æ›´æ–°æ‘˜è¦å¤±è´¥: {e}")
    threading.Thread(target=async_update_summary, daemon=True).start()

    return jsonify(session)


@app.route('/api/sessions/<session_id>/undo-answer', methods=['POST'])
def undo_answer(session_id):
    """æ’¤é”€æœ€åä¸€ä¸ªå›ç­”"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded

    # æ£€æŸ¥æ˜¯å¦æœ‰å›ç­”å¯ä»¥æ’¤é”€
    if not session.get("interview_log") or len(session["interview_log"]) == 0:
        return jsonify({"error": "æ²¡æœ‰å¯æ’¤é”€çš„å›ç­”"}), 400

    # åˆ é™¤æœ€åä¸€ä¸ªå›ç­”
    last_log = session["interview_log"].pop()
    dimension = last_log.get("dimension")
    was_follow_up = last_log.get("is_follow_up", False)

    # æ›´æ–°ç»´åº¦æ•°æ®ï¼ˆåªæœ‰æ­£å¼é—®é¢˜æ‰å½±å“ç»´åº¦ itemsï¼‰
    if dimension and dimension in session["dimensions"]:
        # åªæœ‰åˆ é™¤çš„æ˜¯æ­£å¼é—®é¢˜æ—¶ï¼Œæ‰ä» items ä¸­åˆ é™¤
        if not was_follow_up and session["dimensions"][dimension]["items"]:
            session["dimensions"][dimension]["items"].pop()

        # é‡æ–°è®¡ç®—è¦†ç›–åº¦ï¼ˆåªç»Ÿè®¡æ­£å¼é—®é¢˜ï¼‰
        session["dimensions"][dimension]["coverage"] = calculate_dimension_coverage(session, dimension)

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify(session)


@app.route('/api/sessions/<session_id>/skip-follow-up', methods=['POST'])
def skip_follow_up(session_id):
    """
    ç”¨æˆ·ä¸»åŠ¨è·³è¿‡å½“å‰é—®é¢˜çš„è¿½é—®
    æ ‡è®°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜çš„å›ç­”ä¸º"ä¸éœ€è¦è¿½é—®"
    """
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded
    data = request.get_json() or {}
    dimension = data.get("dimension")

    # éªŒè¯ dimension
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400

    interview_log = session.get("interview_log", [])
    if not interview_log:
        return jsonify({"error": "æ²¡æœ‰å¯è·³è¿‡çš„é—®é¢˜"}), 400

    # æ‰¾åˆ°å½“å‰ç»´åº¦çš„æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜
    dim_logs = [log for log in interview_log if log.get("dimension") == dimension]
    formal_logs = [log for log in dim_logs if not log.get("is_follow_up", False)]

    if not formal_logs:
        return jsonify({"error": "æ²¡æœ‰å¯è·³è¿‡çš„æ­£å¼é—®é¢˜"}), 400

    # æ ‡è®°æœ€åä¸€ä¸ªæ­£å¼é—®é¢˜ä¸éœ€è¦è¿½é—®
    last_formal = formal_logs[-1]
    last_formal["needs_follow_up"] = False
    last_formal["user_skip_follow_up"] = True  # æ ‡è®°ä¸ºç”¨æˆ·ä¸»åŠ¨è·³è¿‡

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    if ENABLE_DEBUG_LOG:
        print(f"â­ï¸ ç”¨æˆ·è·³è¿‡è¿½é—®: dimension={dimension}")

    return jsonify({"success": True, "message": "å·²è·³è¿‡è¿½é—®"})


@app.route('/api/sessions/<session_id>/complete-dimension', methods=['POST'])
def complete_dimension(session_id):
    """
    ç”¨æˆ·ä¸»åŠ¨å®Œæˆå½“å‰ç»´åº¦
    å°†å½“å‰ç»´åº¦æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆè¦†ç›–åº¦è®¾ä¸º100%ï¼‰
    """
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded
    data = request.get_json() or {}
    dimension = data.get("dimension")

    # éªŒè¯ç»´åº¦æœ‰æ•ˆæ€§
    if not dimension or dimension not in session.get("dimensions", {}):
        return jsonify({"error": "æ— æ•ˆçš„ç»´åº¦"}), 400

    # æ£€æŸ¥è¦†ç›–åº¦æ˜¯å¦å·²è¾¾åˆ°è‡³å°‘ 50%
    current_coverage = session["dimensions"][dimension]["coverage"]
    if current_coverage < 50:
        return jsonify({
            "error": "æ— æ³•å®Œæˆç»´åº¦",
            "detail": "å½“å‰ç»´åº¦è¦†ç›–åº¦ä¸è¶³50%ï¼Œå»ºè®®è‡³å°‘å›ç­”ä¸€åŠé—®é¢˜åå†è·³è¿‡"
        }), 400

    # æ ‡è®°æ‰€æœ‰è¯¥ç»´åº¦çš„å›ç­”ä¸ºä¸éœ€è¦è¿½é—®
    for log in session.get("interview_log", []):
        if log.get("dimension") == dimension and not log.get("is_follow_up", False):
            log["needs_follow_up"] = False

    # å°†ç»´åº¦è¦†ç›–åº¦è®¾ä¸º 100%
    session["dimensions"][dimension]["coverage"] = 100
    session["dimensions"][dimension]["user_completed"] = True  # æ ‡è®°ä¸ºç”¨æˆ·ä¸»åŠ¨å®Œæˆ
    session["dimensions"][dimension]["auto_completed"] = False
    session["dimensions"][dimension]["completion_reason"] = "user_completed"
    session["dimensions"][dimension]["quality_warning"] = False

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    if ENABLE_DEBUG_LOG:
        print(f"â­ï¸ ç”¨æˆ·å®Œæˆç»´åº¦: dimension={dimension}, coverage={current_coverage}%")

    session_dim_info = get_dimension_info_for_session(session)
    dim_name = session_dim_info.get(dimension, {}).get('name', dimension)
    return jsonify({"success": True, "message": f"{dim_name}ç»´åº¦å·²å®Œæˆ"})


# ============ æ–‡æ¡£ä¸Šä¼  API ============

@app.route('/api/sessions/<session_id>/documents', methods=['POST'])
def upload_document(session_id):
    """ä¸Šä¼ å‚è€ƒæ–‡æ¡£"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded

    if 'file' not in request.files:
        return jsonify({"error": "æœªæ‰¾åˆ°æ–‡ä»¶"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æ–‡ä»¶åä¸ºç©º"}), 400

    # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§10MBï¼‰
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
    file_size = file.tell()
    file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    if file_size > MAX_FILE_SIZE:
        return jsonify({"error": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§{MAX_FILE_SIZE // 1024 // 1024}MBï¼‰"}), 400

    filename = file.filename
    filepath = TEMP_DIR / filename
    file.save(filepath)

    # è¯»å–æ–‡ä»¶å†…å®¹
    ext = Path(filename).suffix.lower()
    content = ""

    try:
        # å›¾ç‰‡å¤„ç†
        if ext in SUPPORTED_IMAGE_TYPES:
            content = describe_image_with_vision(filepath, filename)
        elif ext in ['.md', '.txt']:
            content = filepath.read_text(encoding="utf-8")
            if not content or not content.strip():
                return jsonify({"error": "æ–‡ä»¶å†…å®¹ä¸ºç©º"}), 400
        elif ext in ['.pdf', '.docx', '.xlsx', '.pptx']:
            # è°ƒç”¨è½¬æ¢è„šæœ¬
            import subprocess
            convert_script = SKILL_DIR / "scripts" / "convert_doc.py"
            if convert_script.exists():
                try:
                    result = subprocess.run(
                        ["uv", "run", str(convert_script), "convert", str(filepath)],
                        capture_output=True, text=True, cwd=str(SKILL_DIR)
                    )
                    if result.returncode == 0:
                        converted_file = CONVERTED_DIR / f"{Path(filename).stem}.md"
                        if converted_file.exists():
                            content = converted_file.read_text(encoding="utf-8")
                        else:
                            content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: æœªæ‰¾åˆ°è½¬æ¢åçš„æ–‡ä»¶]"
                    else:
                        error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                        content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: {error_msg}]"
                except Exception as e:
                    print(f"è½¬æ¢æ–‡æ¡£å¤±è´¥: {e}")
                    content = f"[{ext.upper()[1:]} è§£æå¤±è´¥: {str(e)[:200]}]"
            else:
                content = f"[{ext.upper()[1:]} æ–‡ä»¶: {filename}] (è½¬æ¢è„šæœ¬ä¸å­˜åœ¨)"
    except UnicodeDecodeError as e:
        return jsonify({"error": f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}"}), 400
    except Exception as e:
        return jsonify({"error": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"}), 500

    if not content or not content.strip():
        return jsonify({"error": "æ–‡ä»¶è§£æåå†…å®¹ä¸ºç©º"}), 400

    # æ›´æ–°ä¼šè¯
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)
    session["reference_materials"].append({
        "name": filename,
        "type": ext,
        "content": content[:10000],  # é™åˆ¶é•¿åº¦
        "source": "upload",  # ç”¨æˆ·ä¸Šä¼ 
        "uploaded_at": get_utc_now()
    })
    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify({
        "success": True,
        "filename": filename,
        "content_length": len(content)
    })


@app.route('/api/sessions/<session_id>/documents/<path:doc_name>', methods=['DELETE'])
def delete_document(session_id, doc_name):
    """åˆ é™¤å‚è€ƒèµ„æ–™ï¼ˆè½¯åˆ é™¤ï¼‰"""
    # è·¯å¾„éå†é˜²æŠ¤
    if '..' in doc_name or doc_name.startswith('/'):
        return jsonify({"error": "æ— æ•ˆçš„æ–‡æ¡£å"}), 400

    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)

    # æŸ¥æ‰¾å¹¶åˆ é™¤æ–‡æ¡£
    original_count = len(session["reference_materials"])
    session["reference_materials"] = [
        doc for doc in session["reference_materials"]
        if doc["name"] != doc_name
    ]

    if len(session["reference_materials"]) == original_count:
        return jsonify({"error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404

    session["updated_at"] = get_utc_now()
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    # è½¯åˆ é™¤ï¼šè®°å½•åˆ°åˆ é™¤æ—¥å¿—ï¼Œæ–‡ä»¶ä¿ç•™åœ¨ temp/converted ç›®å½•
    mark_doc_as_deleted(session_id, doc_name)

    return jsonify({
        "success": True,
        "deleted": doc_name,
        "message": "æ–‡æ¡£å·²ä»åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆæ–‡ä»¶å·²å­˜æ¡£ï¼‰"
    })


# ============ é‡æ–°è®¿è°ˆ API ============

@app.route('/api/sessions/<session_id>/restart-interview', methods=['POST'])
def restart_interview(session_id):
    """é‡æ–°è®¿è°ˆï¼šå°†å½“å‰è®¿è°ˆè®°å½•ä¿å­˜ä¸ºå‚è€ƒèµ„æ–™ï¼Œç„¶åé‡ç½®è®¿è°ˆçŠ¶æ€"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    session_file, session = loaded
    # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
    session = migrate_session_docs(session)

    # æ•´ç†å½“å‰è®¿è°ˆè®°å½•ä¸º markdown æ ¼å¼
    interview_log = session.get("interview_log", [])
    if not interview_log:
        return jsonify({"error": "æ²¡æœ‰è®¿è°ˆè®°å½•å¯ä»¥ä¿å­˜"}), 400

    # ç”Ÿæˆè®¿è°ˆè®°å½•æ–‡æ¡£å†…å®¹
    research_content = f"""# è®¿è°ˆè®°å½• - {session.get('topic', 'æœªå‘½åè®¿è°ˆ')}

ç”Ÿæˆæ—¶é—´: {get_utc_now()}

"""

    if session.get("description"):
        # æ¸…ç†æè¿°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        desc = session['description'].replace('\n', ' ').replace('\r', '')
        research_content += f"ä¸»é¢˜æè¿°: {desc}\n\n"

    research_content += "## è®¿è°ˆè®°å½•\n\n"

    # æŒ‰ç»´åº¦æ•´ç†è®¿è°ˆè®°å½•
    restart_dim_info = get_dimension_info_for_session(session)
    for dim_key, dim_info in restart_dim_info.items():
        dim_logs = [log for log in interview_log if log.get("dimension") == dim_key]
        if dim_logs:
            research_content += f"### {dim_info['name']}\n\n"
            for log in dim_logs:
                # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…å½±å“ JSON è§£æ
                question = log.get('question', '').replace('**', '').replace('`', '')
                answer = log.get('answer', '').replace('**', '').replace('`', '')

                research_content += f"Q: {question}\n\n"
                research_content += f"A: {answer}\n\n"

                if log.get('follow_up_question'):
                    follow_q = log['follow_up_question'].replace('**', '').replace('`', '')
                    follow_a = log.get('follow_up_answer', '').replace('**', '').replace('`', '')
                    research_content += f"è¿½é—®: {follow_q}\n\n"
                    research_content += f"å›ç­”: {follow_a}\n\n"
                research_content += "---\n\n"

    # æ·»åŠ åˆ°å‚è€ƒèµ„æ–™åˆ—è¡¨
    doc_name = f"è®¿è°ˆè®°å½•-{get_utc_now().replace(':', '-').replace(' ', '_')}.md"

    # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿å¯¼è‡´ AI prompt é—®é¢˜
    max_length = 2000
    if len(research_content) > max_length:
        research_content = research_content[:max_length] + "\n\n...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"

    session["reference_materials"].append({
        "name": doc_name,
        "type": ".md",
        "content": research_content,
        "source": "auto",  # ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ
        "uploaded_at": get_utc_now()
    })

    # é‡ç½®è®¿è°ˆçŠ¶æ€ - ä»åœºæ™¯é…ç½®åŠ¨æ€åˆ›å»ºç»´åº¦
    session["interview_log"] = []
    reset_dim_info = get_dimension_info_for_session(session)
    session["dimensions"] = {
        dim_key: {"coverage": 0, "items": [], "score": None}
        for dim_key in reset_dim_info
    }
    session["status"] = "in_progress"  # é‡ç½®çŠ¶æ€ä¸ºè¿›è¡Œä¸­
    session["updated_at"] = get_utc_now()

    # ä¿å­˜ä¼šè¯
    session_file.write_text(json.dumps(session, ensure_ascii=False, indent=2), encoding="utf-8")

    return jsonify({
        "success": True,
        "message": "å·²ä¿å­˜å½“å‰è®¿è°ˆå†…å®¹å¹¶é‡ç½®è®¿è°ˆ",
        "research_doc_name": doc_name
    })


# ============ æŠ¥å‘Šç”Ÿæˆ API ============

def run_report_generation_job(session_id: str, user_id: int, request_id: str) -> None:
    """åå°ç”ŸæˆæŠ¥å‘Šä»»åŠ¡ã€‚"""
    worker_ref = threading.current_thread()
    try:
        loaded = load_session_for_user(session_id, user_id, include_missing=True)
        session_file, session, state = loaded
        if state != "ok" or session_file is None or session is None:
            error_msg = "ä¼šè¯ä¸å­˜åœ¨æˆ–æ— æƒé™"
            update_report_generation_status(session_id, "failed", message=f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{error_msg}", active=False)
            set_report_generation_metadata(session_id, {
                "request_id": request_id,
                "error": error_msg,
                "completed_at": get_utc_now(),
            })
            return

        # æ•°æ®è¿ç§»ï¼šå…¼å®¹æ—§ä¼šè¯
        session = migrate_session_docs(session)

        def persist_report(content: str, quality_meta: Optional[dict] = None) -> tuple[Path, str]:
            """ä¿å­˜æŠ¥å‘Šå¹¶æ›´æ–°ä¼šè¯çŠ¶æ€ã€‚"""
            topic_slug = session.get("topic", "report").replace(" ", "-")[:30]
            date_str = datetime.now().strftime("%Y%m%d")
            filename = f"deep-vision-{date_str}-{topic_slug}.md"
            report_file = REPORTS_DIR / filename
            report_file.write_text(content, encoding="utf-8")
            set_report_owner_id(filename, user_id)

            latest_session = safe_load_session(session_file)
            if isinstance(latest_session, dict) and ensure_session_owner(latest_session, user_id):
                latest_session["status"] = "completed"
                latest_session["updated_at"] = get_utc_now()
                if isinstance(quality_meta, dict):
                    latest_session["last_report_quality_meta"] = quality_meta
                if isinstance(session.get("last_report_v3_debug"), dict):
                    latest_session["last_report_v3_debug"] = session["last_report_v3_debug"]
                session_file.write_text(json.dumps(latest_session, ensure_ascii=False, indent=2), encoding="utf-8")

            return report_file, filename

        # æ£€æŸ¥æ˜¯å¦æœ‰ Claude API
        if claude_client:
            update_report_generation_status(session_id, "building_prompt", message="æ­£åœ¨æ‰§è¡Œ V3 è¯æ®åŒ…æ„å»ºä¸ç»“æ„åŒ–è‰æ¡ˆ...")
            v3_result = generate_report_v3_pipeline(session, session_id=session_id)

            if v3_result and v3_result.get("report_content"):
                report_content = v3_result["report_content"] + generate_interview_appendix(session)
                quality_meta = v3_result.get("quality_meta", {})
                session["last_report_quality_meta"] = quality_meta
                session["last_report_v3_debug"] = {
                    "generated_at": get_utc_now(),
                    "status": "success",
                    "reason": "v3_pipeline_passed",
                    "quality_meta": quality_meta,
                    "review_issues": (v3_result.get("review_issues", []) if isinstance(v3_result.get("review_issues", []), list) else [])[:60],
                    "evidence_pack_summary": summarize_evidence_pack_for_debug(v3_result.get("evidence_pack", {})),
                }

                update_report_generation_status(session_id, "saving", message="æ­£åœ¨ä¿å­˜ V3 å®¡ç¨¿å¢å¼ºæŠ¥å‘Š...")
                report_file, filename = persist_report(report_content, quality_meta=quality_meta)
                update_report_generation_status(session_id, "completed", active=False)
                set_report_generation_metadata(session_id, {
                    "request_id": request_id,
                    "report_name": filename,
                    "report_path": str(report_file),
                    "ai_generated": True,
                    "v3_enabled": True,
                    "report_quality_meta": quality_meta if isinstance(quality_meta, dict) else {},
                    "error": "",
                    "completed_at": get_utc_now(),
                })
                return

            session["last_report_v3_debug"] = {
                "generated_at": get_utc_now(),
                "status": "failed",
                "reason": (v3_result or {}).get("reason", "v3_pipeline_returned_empty") if isinstance(v3_result, dict) else "v3_pipeline_returned_empty",
                "error": (v3_result or {}).get("error", "") if isinstance(v3_result, dict) else "",
                "review_issues": ((v3_result or {}).get("review_issues", []) if isinstance((v3_result or {}).get("review_issues", []), list) else [])[:60] if isinstance(v3_result, dict) else [],
                "evidence_pack_summary": summarize_evidence_pack_for_debug((v3_result or {}).get("evidence_pack", {})) if isinstance(v3_result, dict) else {},
            }

            if ENABLE_DEBUG_LOG:
                print("âš ï¸ V3 æŠ¥å‘Šæµç¨‹æœªé€šè¿‡ï¼Œè‡ªåŠ¨å›é€€åˆ°æ ‡å‡†æŠ¥å‘Šç”Ÿæˆæµç¨‹")

            update_report_generation_status(session_id, "generating", message="V3 æµç¨‹å¤±è´¥ï¼Œæ­£åœ¨å›é€€æ ‡å‡†æŠ¥å‘Šç”Ÿæˆ...")
            prompt = build_report_prompt(session)

            if ENABLE_DEBUG_LOG:
                ref_docs_count = len(session.get("reference_materials", session.get("reference_docs", []) + session.get("research_docs", [])))
                interview_count = len(session.get("interview_log", []))
                print(f"ğŸ“Š å›é€€æŠ¥å‘Š Prompt ç»Ÿè®¡ï¼šæ€»é•¿åº¦={len(prompt)}å­—ç¬¦ï¼Œå‚è€ƒèµ„æ–™={ref_docs_count}ä¸ªï¼Œè®¿è°ˆè®°å½•={interview_count}æ¡")

            report_content = call_claude(
                prompt,
                max_tokens=min(MAX_TOKENS_REPORT, 7000),
                call_type="report_legacy_fallback",
                timeout=REPORT_API_TIMEOUT,
            )

            if report_content:
                report_content = report_content + generate_interview_appendix(session)
                quality_meta = build_report_quality_meta_fallback(session, mode="legacy_ai_fallback")
                session["last_report_quality_meta"] = quality_meta

                update_report_generation_status(session_id, "saving", message="æ­£åœ¨ä¿å­˜å›é€€ç”ŸæˆæŠ¥å‘Š...")
                report_file, filename = persist_report(report_content, quality_meta=quality_meta)
                update_report_generation_status(session_id, "completed", active=False)
                set_report_generation_metadata(session_id, {
                    "request_id": request_id,
                    "report_name": filename,
                    "report_path": str(report_file),
                    "ai_generated": True,
                    "v3_enabled": False,
                    "report_quality_meta": quality_meta if isinstance(quality_meta, dict) else {},
                    "error": "",
                    "completed_at": get_utc_now(),
                })
                return

        # å›é€€åˆ°ç®€å•æŠ¥å‘Šç”Ÿæˆ
        update_report_generation_status(session_id, "fallback", message="AI å›é€€å¤±è´¥ï¼Œæ­£åœ¨ä½¿ç”¨æ¨¡æ¿æŠ¥å‘Šå…œåº•...")
        report_content = generate_simple_report(session)
        quality_meta = build_report_quality_meta_fallback(session, mode="simple_template_fallback")
        session["last_report_quality_meta"] = quality_meta
        update_report_generation_status(session_id, "saving")
        report_file, filename = persist_report(report_content, quality_meta=quality_meta)

        update_report_generation_status(session_id, "completed", active=False)
        set_report_generation_metadata(session_id, {
            "request_id": request_id,
            "report_name": filename,
            "report_path": str(report_file),
            "ai_generated": False,
            "v3_enabled": False,
            "report_quality_meta": quality_meta if isinstance(quality_meta, dict) else {},
            "error": "",
            "completed_at": get_utc_now(),
        })
    except Exception as exc:
        error_detail = str(exc)[:200] or "æœªçŸ¥é”™è¯¯"
        update_report_generation_status(session_id, "failed", message=f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{error_detail}", active=False)
        set_report_generation_metadata(session_id, {
            "request_id": request_id,
            "error": error_detail,
            "completed_at": get_utc_now(),
        })
        if ENABLE_DEBUG_LOG:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {error_detail}")
    finally:
        cleanup_report_generation_worker(session_id, worker=worker_ref)


@app.route('/api/sessions/<session_id>/generate-report', methods=['POST'])
def generate_report(session_id):
    """å¼‚æ­¥ç”Ÿæˆè®¿è°ˆæŠ¥å‘Šã€‚"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        _file, error_msg, status_code = loaded
        return jsonify({"error": error_msg}), status_code

    data = request.get_json(silent=True) or {}
    action = "regenerate" if data.get("action") == "regenerate" else "generate"

    current_record = get_report_generation_record(session_id) or {}
    worker_alive = is_report_generation_worker_alive(session_id)
    if bool(current_record.get("active")) or worker_alive:
        if worker_alive and not bool(current_record.get("active")):
            current_state = str(current_record.get("state") or "").strip()
            if current_state not in {"completed", "failed", "cancelled"}:
                update_report_generation_status(session_id, "queued", message="æŠ¥å‘Šä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­...")
                current_record = get_report_generation_record(session_id) or {}
        payload = build_report_generation_payload(current_record)
        payload.update({
            "success": True,
            "already_running": True,
            "action": current_record.get("action", action),
        })
        return jsonify(payload), 202

    request_id = secrets.token_hex(12)
    set_report_generation_metadata(session_id, {
        "request_id": request_id,
        "action": action,
        "started_at": get_utc_now(),
        "completed_at": "",
        "report_name": "",
        "report_path": "",
        "ai_generated": None,
        "v3_enabled": None,
        "report_quality_meta": {},
        "error": "",
    })
    update_report_generation_status(session_id, "queued")

    worker = threading.Thread(
        target=run_report_generation_job,
        args=(session_id, user_id, request_id),
        daemon=True,
        name=f"report-generator-{session_id[:8]}"
    )
    register_report_generation_worker(session_id, worker)
    worker.start()

    payload = build_report_generation_payload(get_report_generation_record(session_id))
    payload.update({
        "success": True,
        "already_running": False,
        "action": action,
    })
    return jsonify(payload), 202


def generate_interview_appendix(session: dict) -> str:
    """ç”Ÿæˆå®Œæ•´çš„è®¿è°ˆè®°å½•é™„å½•"""
    interview_log = session.get("interview_log", [])
    if not interview_log:
        return ""

    appendix = "\n\n---\n\n## é™„å½•ï¼šå®Œæ•´è®¿è°ˆè®°å½•\n\n"
    appendix += "<details>\n"
    appendix += f"<summary>æœ¬æ¬¡è®¿è°ˆå…±æ”¶é›†äº† {len(interview_log)} ä¸ªé—®é¢˜çš„å›ç­”ï¼ˆç‚¹å‡»å±•å¼€/æ”¶èµ·ï¼‰</summary>\n\n"

    appendix_dim_info = get_dimension_info_for_session(session)
    for i, log in enumerate(interview_log, 1):
        dim_name = appendix_dim_info.get(log.get('dimension', ''), {}).get('name', 'æœªåˆ†ç±»')
        question = log.get('question', '')
        answer = log.get('answer', '')
        appendix += "<details>\n"
        appendix += f"<summary>Q{i}: {question}</summary>\n\n"
        appendix += f"**å›ç­”**: {answer}\n\n"
        appendix += f"**ç»´åº¦**: {dim_name}\n\n"
        if log.get('timestamp'):
            appendix += f"*è®°å½•æ—¶é—´: {log['timestamp']}*\n\n"
        appendix += "</details>\n\n"
    appendix += "</details>\n\n"

    return appendix


def generate_simple_report(session: dict) -> str:
    """ç”Ÿæˆç®€å•æŠ¥å‘Šï¼ˆæ—  AI æ—¶ä½¿ç”¨ï¼‰"""
    topic = session.get("topic", "æœªå‘½åé¡¹ç›®")
    interview_log = session.get("interview_log", [])
    now = datetime.now()

    content = f"""# {topic} è®¿è°ˆæŠ¥å‘Š

**è®¿è°ˆæ—¥æœŸ**: {now.strftime('%Y-%m-%d')}
**æŠ¥å‘Šç¼–å·**: deep-vision-{now.strftime('%Y%m%d')}

---

## 1. è®¿è°ˆæ¦‚è¿°

æœ¬æ¬¡è®¿è°ˆä¸»é¢˜ä¸ºã€Œ{topic}ã€ï¼Œå…±æ”¶é›†äº† {len(interview_log)} ä¸ªé—®é¢˜çš„å›ç­”ã€‚

## 2. éœ€æ±‚æ‘˜è¦

"""

    simple_dim_info = get_dimension_info_for_session(session)
    for dim_key, dim_info in simple_dim_info.items():
        content += f"### {dim_info['name']}\n\n"
        logs = [log for log in interview_log if log.get("dimension") == dim_key]
        if logs:
            for log in logs:
                content += f"- **{log['answer']}** - {log['question']}\n"
        else:
            content += "*æš‚æ— æ•°æ®*\n"
        content += "\n"

    # ä½¿ç”¨ç»Ÿä¸€çš„é™„å½•ç”Ÿæˆå‡½æ•°ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
    content += generate_interview_appendix(session)

    content += """
*æ­¤æŠ¥å‘Šç”± Deep Vision æ·±ç³ç”Ÿæˆ*
"""

    return content


# ============ Refly é›†æˆ ============

def is_refly_configured() -> bool:
    return bool(REFLY_API_URL and REFLY_API_KEY and REFLY_WORKFLOW_ID and REFLY_INPUT_FIELD)


def build_refly_payload(report_content: str, file_keys: Optional[list] = None) -> dict:
    variables = {REFLY_INPUT_FIELD: report_content}
    if file_keys:
        variables[REFLY_FILES_FIELD] = file_keys
    return {"variables": variables}


def get_refly_base_url() -> str:
    return (REFLY_API_URL or "").rstrip("/")


def get_refly_run_url() -> str:
    base = get_refly_base_url()
    if not base:
        return base
    return f"{base}/openapi/workflow/{REFLY_WORKFLOW_ID}/run"


def get_refly_upload_url() -> str:
    base = get_refly_base_url()
    if not base:
        return base
    return f"{base}/openapi/files/upload"


def get_refly_status_url(execution_id: str) -> str:
    base = get_refly_base_url()
    if not base:
        return base
    return f"{base}/openapi/workflow/{execution_id}/status"


def get_refly_output_url(execution_id: str) -> str:
    base = get_refly_base_url()
    if not base:
        return base
    return f"{base}/openapi/workflow/{execution_id}/output"


def get_refly_execution_status(status_response: dict) -> str:
    if not isinstance(status_response, dict):
        return ""
    payload = status_response.get("payload") or status_response.get("data") or status_response
    data = payload.get("data") if isinstance(payload, dict) else {}
    status = ""
    if isinstance(data, dict):
        status = data.get("status") or ""
    return status or ""


def get_refly_abort_url(execution_id: str) -> str:
    base = get_refly_base_url()
    if not base:
        return base
    return f"{base}/openapi/workflow/{execution_id}/abort"


def upload_refly_file(file_path: Path) -> dict:
    url = get_refly_upload_url()
    headers = {
        "Authorization": f"Bearer {REFLY_API_KEY}"
    }
    if ENABLE_DEBUG_LOG:
        print(f"ğŸ“¤ Refly ä¸Šä¼ æ–‡ä»¶: {url}")
    with open(file_path, "rb") as file_handle:
        files = {"files": (file_path.name, file_handle)}
        response = requests.post(url, headers=headers, files=files, timeout=REFLY_TIMEOUT)
    response.raise_for_status()
    try:
        return response.json()
    except ValueError:
        return {"raw": response.text}


def extract_refly_file_keys(upload_response: dict) -> list:
    if not isinstance(upload_response, dict):
        return []
    files = (
        upload_response.get("data", {}).get("files")
        or upload_response.get("files")
        or []
    )
    keys = []
    if isinstance(files, list):
        for item in files:
            if isinstance(item, dict):
                key = item.get("fileKey") or item.get("file_key") or item.get("key")
                if key:
                    keys.append(key)
    return keys


def run_refly_workflow(report_content: str, file_keys: Optional[list] = None) -> dict:
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {REFLY_API_KEY}"
    }
    url = get_refly_run_url()
    payload = build_refly_payload(report_content, file_keys=file_keys)

    if ENABLE_DEBUG_LOG:
        print(f"ğŸ“¤ Refly è¿è¡Œå·¥ä½œæµ: {url}")

    response = requests.post(url, json=payload, headers=headers, timeout=REFLY_TIMEOUT)
    response.raise_for_status()
    try:
        return response.json()
    except ValueError:
        return {"raw": response.text}


def poll_refly_execution(execution_id: str) -> dict:
    headers = {
        "Authorization": f"Bearer {REFLY_API_KEY}"
    }
    status_url = get_refly_status_url(execution_id)
    deadline = _time.time() + REFLY_POLL_TIMEOUT
    last_payload = {}
    status = "executing"

    if ENABLE_DEBUG_LOG:
        print(f"ğŸ” Refly è½®è¯¢çŠ¶æ€: {status_url}")

    while _time.time() < deadline and status == "executing":
        _time.sleep(REFLY_POLL_INTERVAL)
        response = requests.get(status_url, headers=headers, timeout=REFLY_TIMEOUT)
        response.raise_for_status()
        try:
            last_payload = response.json()
        except ValueError:
            last_payload = {"raw": response.text}
        status = (
            last_payload.get("data", {}).get("status")
            or last_payload.get("status")
            or status
        )

    return {
        "status": status,
        "payload": last_payload
    }


def fetch_refly_status_once(execution_id: str) -> dict:
    headers = {
        "Authorization": f"Bearer {REFLY_API_KEY}"
    }
    status_url = get_refly_status_url(execution_id)
    response = requests.get(status_url, headers=headers, timeout=REFLY_TIMEOUT)
    response.raise_for_status()
    try:
        payload = response.json()
    except ValueError:
        payload = {"raw": response.text}
    status = payload.get("data", {}).get("status") or payload.get("status") or "executing"
    return {
        "status": status,
        "payload": payload
    }


def fetch_refly_output(execution_id: str) -> dict:
    headers = {
        "Authorization": f"Bearer {REFLY_API_KEY}"
    }
    output_url = get_refly_output_url(execution_id)

    if ENABLE_DEBUG_LOG:
        print(f"ğŸ“¥ Refly è·å–ç»“æœ: {output_url}")

    response = requests.get(output_url, headers=headers, timeout=REFLY_TIMEOUT)
    response.raise_for_status()
    try:
        return response.json()
    except ValueError:
        return {"raw": response.text}


def has_pdf_or_result_file(payload) -> bool:
    if not isinstance(payload, dict):
        return False
    data = payload.get("data") if isinstance(payload.get("data"), dict) else {}
    files = data.get("files") if isinstance(data.get("files"), list) else []
    for item in files:
        if not isinstance(item, dict):
            continue
        name = (item.get("name") or "").lower()
        file_type = (item.get("type") or "").lower()
        url = (item.get("url") or "").lower()
        if name.endswith(".pdf") or name.endswith("result_info.json"):
            return True
        if url.endswith(".pdf") or url.endswith("result_info.json"):
            return True
        if "pdf" in file_type:
            return True
    return False


def wait_for_refly_output_ready(execution_id: str) -> dict:
    deadline = _time.time() + REFLY_POLL_TIMEOUT
    last_output = {}
    last_status = "executing"
    while _time.time() < deadline:
        try:
            status_payload = fetch_refly_status_once(execution_id)
            last_status = status_payload.get("status") or last_status
        except Exception:
            pass
        if last_status != "executing":
            break
        _time.sleep(REFLY_POLL_INTERVAL)
    try:
        last_output = fetch_refly_output(execution_id)
    except Exception:
        last_output = last_output or {}
    if has_pdf_or_result_file(last_output):
        return last_output
    return last_output


def extract_refly_url(payload) -> Optional[str]:
    if isinstance(payload, dict):
        for key in ("presentation_url", "url", "share_url", "shareUrl", "preview_url", "previewUrl"):
            value = payload.get(key)
            if isinstance(value, str) and value.startswith("http"):
                return value
        for value in payload.values():
            found = extract_refly_url(value)
            if found:
                return found
    elif isinstance(payload, list):
        for value in payload:
            found = extract_refly_url(value)
            if found:
                return found
    elif isinstance(payload, str) and payload.startswith("http"):
        return payload
    return None


def extract_pdf_filename(payload) -> Optional[str]:
    if isinstance(payload, dict):
        if isinstance(payload.get("pdf_filename"), str):
            return payload.get("pdf_filename")
        for value in payload.values():
            found = extract_pdf_filename(value)
            if found:
                return found
    elif isinstance(payload, list):
        for value in payload:
            found = extract_pdf_filename(value)
            if found:
                return found
    return None


def extract_refly_file_id(payload) -> Optional[str]:
    urls = collect_refly_urls(payload, [])
    for url in urls:
        match = re.search(r"/drive/file/(?:content|public)/([^/]+)/", url)
        if match:
            return match.group(1)
    return None


def build_pdf_url_from_result_info(payload) -> Optional[str]:
    pdf_filename = extract_pdf_filename(payload)
    file_id = extract_refly_file_id(payload)
    if not pdf_filename or not file_id:
        return None
    base = get_refly_base_url()
    if not base:
        return None
    return f"{base}/drive/file/content/{file_id}/{quote(pdf_filename)}"


def collect_refly_urls(payload, urls: Optional[list] = None) -> list:
    if urls is None:
        urls = []
    if not payload:
        return urls
    if isinstance(payload, str):
        if payload.startswith("http"):
            urls.append(payload)
        return urls
    if isinstance(payload, list):
        for item in payload:
            collect_refly_urls(item, urls)
        return urls
    if isinstance(payload, dict):
        for value in payload.values():
            collect_refly_urls(value, urls)
    return urls


def extract_pdf_url_from_output(payload) -> Optional[str]:
    if not isinstance(payload, dict):
        return None
    data = payload.get("data") if isinstance(payload.get("data"), dict) else {}
    files = data.get("files") if isinstance(data.get("files"), list) else []
    for item in files:
        if not isinstance(item, dict):
            continue
        url = item.get("url")
        name = (item.get("name") or "").lower()
        file_type = (item.get("type") or "").lower()
        if isinstance(url, str) and url.startswith("http"):
            if url.lower().endswith(".pdf") or name.endswith(".pdf") or "pdf" in file_type:
                return url

    outputs = data.get("output") if isinstance(data.get("output"), list) else []
    for node in outputs:
        if not isinstance(node, dict):
            continue
        messages = node.get("messages") if isinstance(node.get("messages"), list) else []
        for msg in messages:
            if not isinstance(msg, dict):
                continue
            content = msg.get("content")
            if isinstance(content, str):
                match = re.search(r"https?://\\S+?\\.pdf", content)
                if match:
                    return match.group(0)

    for url in collect_refly_urls(payload, []):
        if isinstance(url, str) and url.lower().endswith(".pdf"):
            return url
    return None


def fetch_result_info_json(payload) -> Optional[dict]:
    if not isinstance(payload, dict):
        return None
    data = payload.get("data") if isinstance(payload.get("data"), dict) else {}
    files = data.get("files") if isinstance(data.get("files"), list) else []
    result_url = None
    for item in files:
        if not isinstance(item, dict):
            continue
        name = (item.get("name") or "").lower()
        url = item.get("url")
        if name.endswith("result_info.json") and isinstance(url, str) and url.startswith("http"):
            result_url = url
            break
    if not result_url:
        return None

    headers = {}
    try:
        base_host = urlparse(get_refly_base_url()).netloc
        url_host = urlparse(result_url).netloc
        if base_host and url_host.endswith(base_host):
            headers["Authorization"] = f"Bearer {REFLY_API_KEY}"
    except Exception:
        headers = {}

    try:
        response = requests.get(result_url, headers=headers, timeout=REFLY_TIMEOUT)
        response.raise_for_status()
        return response.json()
    except Exception:
        return None


def build_pdf_url_from_result_info_file(payload) -> Optional[str]:
    result_info = fetch_result_info_json(payload)
    if not isinstance(result_info, dict):
        return None
    pdf_filename = result_info.get("pdf_filename")
    if not isinstance(pdf_filename, str) or not pdf_filename:
        return None
    file_id = extract_refly_file_id(payload)
    if not file_id:
        return None
    base = get_refly_base_url()
    if not base:
        return None
    return f"{base}/drive/file/content/{file_id}/{quote(pdf_filename)}"


def get_refly_file_candidates(output_response: dict) -> list:
    files = []
    if isinstance(output_response, dict):
        files = (
            output_response.get("data", {}).get("files")
            or output_response.get("files")
            or []
        )
    if not isinstance(files, list):
        return []
    candidates = []
    for item in files:
        if not isinstance(item, dict):
            continue
        url = item.get("url")
        name = item.get("name") or item.get("fileName") or item.get("filename") or ""
        if isinstance(url, str) and url.startswith("http"):
            candidates.append({"url": url, "name": name})
    return candidates


def score_refly_url(url: str, name: str = "") -> int:
    lower_url = (url or "").lower()
    lower_name = (name or "").lower()
    target = lower_name or lower_url
    match = re.search(r"\.[a-z0-9]+(?=$|\?)", target)
    ext = match.group(0) if match else ""
    score = 0

    if any(key in lower_url for key in ("share", "preview", "presentation")):
        score += 80
    if "slide" in lower_url:
        score += 10

    if ext == ".pptx":
        score += 100
    elif ext == ".pdf":
        score += 90
    elif ext in (".ppt", ".key"):
        score += 80
    elif ext in (".html", ".htm"):
        score += 70
    elif ext in (".png", ".jpg", ".jpeg"):
        score += 50
    elif ext == ".json":
        score -= 10

    return score


def select_best_refly_candidate(output_response: dict, presentation_url: Optional[str]) -> Optional[dict]:
    candidates = []

    def add_candidate(url: Optional[str], name: str = ""):
        if not isinstance(url, str) or not url.startswith("http"):
            return
        candidates.append({"url": url, "name": name or ""})

    if isinstance(presentation_url, str) and presentation_url.startswith("http"):
        if not presentation_url.lower().endswith(".json"):
            return {"url": presentation_url, "name": "presentation_url"}
        add_candidate(presentation_url, "presentation_url")

    for item in get_refly_file_candidates(output_response):
        add_candidate(item.get("url"), item.get("name", ""))

    for url in collect_refly_urls(output_response, []):
        add_candidate(url, "")

    if not candidates:
        return None

    deduped = {}
    for item in candidates:
        deduped.setdefault(item["url"], item)

    sorted_candidates = sorted(
        deduped.values(),
        key=lambda item: score_refly_url(item.get("url", ""), item.get("name", "")),
        reverse=True,
    )
    return sorted_candidates[0] if sorted_candidates else None


def get_downloads_dir() -> Path:
    env_dir = os.environ.get("DOWNLOADS_DIR", "").strip()
    if env_dir:
        return Path(env_dir).expanduser().resolve()
    home = Path.home()
    candidates = [home / "Downloads", home / "ä¸‹è½½"]
    for candidate in candidates:
        if candidate.exists() and candidate.is_dir():
            return candidate
    return candidates[0]


def sanitize_filename(name: str) -> str:
    safe = Path(name).name if name else ""
    safe = safe.strip().strip(".")
    if not safe:
        return "presentation"
    for ch in ('/', '\\', ':', '*', '?', '"', '<', '>', '|'):
        safe = safe.replace(ch, "_")
    return safe


def ensure_unique_path(directory: Path, filename: str) -> Path:
    path = directory / filename
    if not path.exists():
        return path
    stem = path.stem
    suffix = path.suffix
    counter = 1
    while True:
        candidate = directory / f"{stem}-{counter}{suffix}"
        if not candidate.exists():
            return candidate
        counter += 1


def infer_extension(content_type: str) -> str:
    if not content_type:
        return ""
    lower = content_type.lower()
    if "presentationml" in lower or "pptx" in lower:
        return ".pptx"
    if "ms-powerpoint" in lower:
        return ".ppt"
    if "pdf" in lower:
        return ".pdf"
    if "html" in lower:
        return ".html"
    if "json" in lower:
        return ".json"
    if "png" in lower:
        return ".png"
    if "jpeg" in lower or "jpg" in lower:
        return ".jpg"
    return ""


def build_download_filename(url: str, name: str, content_type: str) -> str:
    raw_name = name or ""
    if raw_name:
        raw_name = sanitize_filename(raw_name)
    if not raw_name:
        parsed = urlparse(url)
        raw_name = sanitize_filename(unquote(Path(parsed.path).name))

    ext = ""
    if raw_name:
        ext_match = re.search(r"\.[a-z0-9]+$", raw_name.lower())
        ext = ext_match.group(0) if ext_match else ""
    if not ext:
        ext = infer_extension(content_type)
    if not raw_name:
        raw_name = "presentation"
    if ext and not raw_name.lower().endswith(ext):
        raw_name = f"{raw_name}{ext}"
    return raw_name


def download_presentation_file(url: str, name: str = "") -> Optional[dict]:
    downloads_dir = get_downloads_dir()
    downloads_dir.mkdir(parents=True, exist_ok=True)

    headers = {}
    try:
        base_host = urlparse(get_refly_base_url()).netloc
        url_host = urlparse(url).netloc
        if base_host and url_host.endswith(base_host):
            headers["Authorization"] = f"Bearer {REFLY_API_KEY}"
    except Exception:
        headers = {}

    response = requests.get(url, headers=headers, stream=True, timeout=REFLY_TIMEOUT)
    response.raise_for_status()
    content_type = response.headers.get("Content-Type", "")
    filename = build_download_filename(url, name, content_type)
    file_path = ensure_unique_path(downloads_dir, filename)

    size = 0
    with open(file_path, "wb") as file_handle:
        for chunk in response.iter_content(chunk_size=1024 * 256):
            if not chunk:
                continue
            file_handle.write(chunk)
            size += len(chunk)
    response.close()

    return {
        "url": url,
        "path": str(file_path),
        "filename": file_path.name,
        "size": size
    }


@app.route('/api/reports/<path:filename>/presentation', methods=['GET'])
def get_report_presentation(filename):
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404

    _report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    record = get_presentation_record(filename)
    if not record:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404
    path_str = record.get("path")
    if not path_str:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404
    file_path = Path(path_str)
    if not file_path.exists():
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404
    downloads_dir = get_downloads_dir()
    if not is_path_under(file_path, downloads_dir):
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä½ç½®æ— æ•ˆ"}), 403
    return send_from_directory(
        directory=file_path.parent,
        path=file_path.name,
        as_attachment=False,
        download_name=record.get("filename") or file_path.name
    )


@app.route('/api/reports/<path:filename>/presentation/status', methods=['GET'])
def get_report_presentation_status(filename):
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"exists": False})

    _report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        return jsonify({"exists": False})

    record = get_presentation_record(filename)
    if not record:
        return jsonify({"exists": False})

    pdf_url = record.get("pdf_url")
    execution_id = record.get("execution_id")
    stopped_at = record.get("stopped_at")

    if execution_id:
        owner = get_execution_owner_report(execution_id)
        if owner and owner != filename:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸ æ¸…ç†è·¨æŠ¥å‘Š execution_id: execution_id={execution_id}, owner={owner}, filename={filename}")
            clear_presentation_execution(filename)
            execution_id = ""

    if execution_id and stopped_at:
        clear_presentation_execution(filename)
        execution_id = ""

    path_str = record.get("path")
    file_exists = False
    if path_str:
        file_path = Path(path_str)
        downloads_dir = get_downloads_dir()
        if file_path.exists() and is_path_under(file_path, downloads_dir):
            file_exists = True

    processing = bool(execution_id and not pdf_url and not stopped_at)
    if processing:
        try:
            output_response = fetch_refly_output(execution_id)
            pdf_url = (
                extract_pdf_url_from_output(output_response)
                or build_pdf_url_from_result_info_file(output_response)
                or build_pdf_url_from_result_info(output_response)
            )
            if pdf_url:
                record_presentation_file(filename, None, pdf_url=pdf_url)
                processing = False
        except Exception:
            pass

    return jsonify({
        "exists": bool(pdf_url or file_exists),
        "pdf_url": pdf_url,
        "presentation_local_url": f"/api/reports/{quote(filename)}/presentation" if file_exists else "",
        "execution_id": execution_id,
        "processing": processing,
        "stopped": bool(stopped_at)
    })


@app.route('/api/reports/<path:filename>/presentation/link', methods=['GET'])
def get_report_presentation_link(filename):
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404

    _report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    record = get_presentation_record(filename)
    if not record:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404
    pdf_url = record.get("pdf_url")
    if not pdf_url:
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿ä¸å­˜åœ¨"}), 404
    return redirect(pdf_url, code=302)


# ============ æŠ¥å‘Š API ============

@app.route('/api/reports', methods=['GET'])
def list_reports():
    """è·å–æ‰€æœ‰æŠ¥å‘Šï¼ˆæ’é™¤å·²åˆ é™¤çš„ï¼‰"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    deleted = get_deleted_reports()
    reports = []
    for f in REPORTS_DIR.glob("*.md"):
        # è·³è¿‡å·²æ ‡è®°ä¸ºåˆ é™¤çš„æŠ¥å‘Š
        if f.name in deleted:
            continue
        if not ensure_report_owner(f.name, user_id):
            continue
        stat = f.stat()
        reports.append({
            "name": f.name,
            "path": str(f),
            "size": stat.st_size,
            "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat()
        })

    reports.sort(key=lambda x: x["created_at"], reverse=True)
    return jsonify(reports)


@app.route('/api/reports/<path:filename>', methods=['GET'])
def get_report(filename):
    """è·å–æŠ¥å‘Šå†…å®¹"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    content = report_file.read_text(encoding="utf-8")
    return jsonify({"name": filename, "content": content})


@app.route('/api/reports/<path:filename>/refly', methods=['POST'])
def send_report_to_refly(filename):
    """å°†æŠ¥å‘Šå‘é€åˆ°æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"error": "æŠ¥å‘Šæ–‡ä»¶åæ— æ•ˆ"}), 400

    report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    if not is_refly_configured():
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡æœªé…ç½®"}), 400

    clear_presentation_stopped(filename)
    execution_id = ""

    try:
        report_content = report_file.read_text(encoding="utf-8")
        report_title = report_content.strip().splitlines()[0].lstrip("#").strip() if report_content else ""
        upload_response = upload_refly_file(report_file)
        file_keys = extract_refly_file_keys(upload_response)
        run_response = run_refly_workflow(report_content, file_keys=file_keys)
        execution_id = (
            run_response.get("data", {}).get("executionId")
            or run_response.get("executionId")
        )
        if not execution_id:
            return jsonify({
                "error": "æœåŠ¡æœªè¿”å› executionId",
                "refly_upload": upload_response,
                "refly_response": run_response
            }), 502

        owner = get_execution_owner_report(execution_id)
        if owner and owner != filename:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸ æ‹¦æˆªè·¨æŠ¥å‘Š execution_id å¤ç”¨: execution_id={execution_id}, owner={owner}, filename={filename}")
            return jsonify({
                "error": "ç”Ÿæˆä»»åŠ¡å½’å±å†²çªï¼Œè¯·é‡è¯•",
                "mismatch": True,
                "owner_report": owner,
                "execution_id": execution_id
            }), 409

        if is_presentation_execution_stopped(filename, execution_id):
            try:
                url = get_refly_abort_url(execution_id)
                headers = {"Authorization": f"Bearer {REFLY_API_KEY}"}
                requests.post(url, headers=headers, timeout=REFLY_TIMEOUT)
            except Exception:
                pass
            return jsonify({
                "processing": False,
                "execution_id": "",
                "stopped": True,
                "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢"
            })

        record_presentation_execution(filename, execution_id)

        status_response = poll_refly_execution(execution_id)
        output_response = wait_for_refly_output_ready(execution_id)
        pdf_url = (
            extract_pdf_url_from_output(output_response)
            or build_pdf_url_from_result_info_file(output_response)
            or build_pdf_url_from_result_info(output_response)
        )
        if not pdf_url:
            if is_presentation_execution_stopped(filename, execution_id):
                return jsonify({
                    "processing": False,
                    "execution_id": "",
                    "stopped": True,
                    "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢"
                })
            record_presentation_execution(filename, execution_id)
            return jsonify({
                "message": "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•",
                "execution_id": execution_id,
                "refly_status": status_response,
                "refly_response": output_response,
                "processing": True
            })

        if is_presentation_execution_stopped(filename, execution_id):
            return jsonify({
                "processing": False,
                "execution_id": "",
                "stopped": True,
                "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢"
            })

        presentation_url = pdf_url
        download_info = None
        presentation_local_url = None
        try:
            candidate = select_best_refly_candidate(output_response, presentation_url)
            if candidate:
                download_info = download_presentation_file(candidate["url"], candidate.get("name", ""))
        except Exception as download_exc:
            if ENABLE_DEBUG_LOG:
                print(f"âš ï¸ ä¸‹è½½æ¼”ç¤ºæ–‡ç¨¿å¤±è´¥: {download_exc}")

        if download_info or pdf_url:
            record_presentation_file(filename, download_info, pdf_url=pdf_url)
            if download_info:
                presentation_local_url = f"/api/reports/{quote(filename)}/presentation"

        response_payload = {
            "message": "å·²æäº¤ç”Ÿæˆä»»åŠ¡",
            "report_filename": filename,
            "report_title": report_title,
            "execution_id": execution_id,
            "refly_upload": upload_response,
            "refly_status": status_response,
            "refly_response": output_response,
            "presentation_url": presentation_url
        }
        if pdf_url:
            response_payload["pdf_url"] = pdf_url
        if download_info:
            response_payload.update({
                "download_url": download_info.get("url"),
                "download_path": download_info.get("path"),
                "download_filename": download_info.get("filename"),
                "download_size": download_info.get("size"),
                "presentation_local_url": presentation_local_url
            })

        return jsonify(response_payload)
    except requests.HTTPError as exc:
        if ENABLE_DEBUG_LOG:
            try:
                detail = exc.response.text
            except Exception:
                detail = str(exc)
            print(f"âš ï¸ æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡ HTTP é”™è¯¯: {detail}")
        return jsonify({"error": "æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡è¯·æ±‚å¤±è´¥"}), 502
    except requests.exceptions.Timeout:
        stopped = is_presentation_execution_stopped(filename, execution_id)
        if execution_id and not stopped:
            record_presentation_execution(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        }), 202
    except requests.exceptions.SSLError:
        stopped = is_presentation_execution_stopped(filename, execution_id)
        if execution_id and not stopped:
            record_presentation_execution(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        }), 202
    except requests.RequestException as exc:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡è¯·æ±‚å¼‚å¸¸: {exc}")
        stopped = is_presentation_execution_stopped(filename, execution_id)
        if execution_id and not stopped:
            record_presentation_execution(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        }), 202
    except Exception as exc:
        return jsonify({"error": f"æäº¤ç”Ÿæˆä»»åŠ¡å¤±è´¥: {str(exc)}"}), 500


@app.route('/api/reports/<path:filename>/refly/status', methods=['GET'])
def check_refly_status(filename):
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"error": "filename ç¼ºå¤±"}), 400

    _report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    execution_id = request.args.get("execution_id", "").strip()
    if not execution_id:
        return jsonify({"error": "execution_id ç¼ºå¤±"}), 400

    current_record = get_presentation_record(filename) or {}
    stopped_at = current_record.get("stopped_at")
    stopped_execution_id = str(current_record.get("stopped_execution_id") or "").strip()
    if stopped_at and (not stopped_execution_id or stopped_execution_id == execution_id):
        return jsonify({
            "processing": False,
            "execution_id": "",
            "stopped": True,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢"
        })

    owner = get_execution_owner_report(execution_id)
    if owner and owner != filename:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ æ‹¦æˆªè·¨æŠ¥å‘ŠçŠ¶æ€æŸ¥è¯¢: execution_id={execution_id}, owner={owner}, filename={filename}")
        return jsonify({
            "processing": False,
            "execution_id": execution_id,
            "mismatch": True,
            "owner_report": owner,
            "message": "execution_id ä¸æŠ¥å‘Šä¸åŒ¹é…"
        }), 409
    try:
        status_response = fetch_refly_status_once(execution_id)
        output_response = fetch_refly_output(execution_id)
        pdf_url = (
            extract_pdf_url_from_output(output_response)
            or build_pdf_url_from_result_info_file(output_response)
            or build_pdf_url_from_result_info(output_response)
        )
        if not pdf_url:
            latest_record = get_presentation_record(filename) or {}
            latest_stopped_at = latest_record.get("stopped_at")
            latest_stopped_execution_id = str(latest_record.get("stopped_execution_id") or "").strip()
            if latest_stopped_at and (not latest_stopped_execution_id or latest_stopped_execution_id == execution_id):
                return jsonify({
                    "processing": False,
                    "execution_id": "",
                    "stopped": True,
                    "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢"
                })
            record_presentation_execution(filename, execution_id)
            return jsonify({
                "processing": True,
                "execution_id": execution_id,
                "refly_status": status_response,
                "refly_response": output_response
            })
        record_presentation_file(filename, None, pdf_url=pdf_url)
        return jsonify({
            "execution_id": execution_id,
            "pdf_url": pdf_url
        })
    except requests.exceptions.Timeout:
        stopped = is_presentation_execution_stopped(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        })
    except requests.exceptions.SSLError:
        stopped = is_presentation_execution_stopped(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        })
    except requests.RequestException as exc:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ æ¼”ç¤ºæ–‡ç¨¿æœåŠ¡è¯·æ±‚å¼‚å¸¸: {exc}")
        stopped = is_presentation_execution_stopped(filename, execution_id)
        return jsonify({
            "processing": not stopped,
            "execution_id": "" if stopped else execution_id,
            "stopped": stopped,
            "message": "æ¼”ç¤ºæ–‡ç¨¿ç”Ÿæˆå·²åœæ­¢" if stopped else "æ¼”ç¤ºæ–‡ç¨¿ä»åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åé‡è¯•"
        })
    except Exception as exc:
        return jsonify({"error": f"æŸ¥è¯¢ç”ŸæˆçŠ¶æ€å¤±è´¥: {str(exc)}"}), 500


@app.route('/api/reports/<path:filename>/presentation/abort', methods=['POST'])
def abort_report_presentation(filename):
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    filename = normalize_presentation_report_filename(filename)
    if not filename:
        return jsonify({"error": "filename ç¼ºå¤±"}), 400

    _report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    execution_id = request.args.get("execution_id", "").strip()
    if not execution_id:
        record = get_presentation_record(filename) or {}
        execution_id = str(record.get("execution_id") or "").strip()
    if not execution_id:
        mark_presentation_stopped(filename)
        return jsonify({"success": True, "execution_id": ""})
    try:
        url = get_refly_abort_url(execution_id)
        headers = {"Authorization": f"Bearer {REFLY_API_KEY}"}
        response = requests.post(url, headers=headers, timeout=REFLY_TIMEOUT)
        response.raise_for_status()
        mark_presentation_stopped(filename, execution_id=execution_id)
        return jsonify({"success": True, "execution_id": execution_id})
    except requests.RequestException as exc:
        if ENABLE_DEBUG_LOG:
            print(f"âš ï¸ æ¼”ç¤ºæ–‡ç¨¿ä¸­æ­¢å¤±è´¥: {exc}")
        mark_presentation_stopped(filename, execution_id=execution_id)
        return jsonify({"success": True, "execution_id": execution_id, "warning": "ä¸­æ­¢è¯·æ±‚å¤±è´¥ï¼Œå·²æœ¬åœ°æ ‡è®°åœæ­¢"})


@app.route('/api/reports/<path:filename>', methods=['DELETE'])
def delete_report(filename):
    """åˆ é™¤æŠ¥å‘Šï¼ˆä»…æ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä¿ç•™æ–‡ä»¶å­˜æ¡£ï¼‰"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    report_file, owner_error = enforce_report_owner_or_404(filename, user_id)
    if owner_error:
        response, status_code = owner_error
        return response, status_code

    try:
        # åªæ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä¸çœŸæ­£åˆ é™¤æ–‡ä»¶
        mark_report_as_deleted(filename)
        return jsonify({
            "message": "æŠ¥å‘Šå·²ä»åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆæ–‡ä»¶å·²å­˜æ¡£ï¼‰",
            "name": filename
        })
    except Exception as e:
        return jsonify({"error": f"æ ‡è®°åˆ é™¤å¤±è´¥: {str(e)}"}), 500


@app.route('/api/reports/batch-delete', methods=['POST'])
def batch_delete_reports():
    """æ‰¹é‡åˆ é™¤æŠ¥å‘Šï¼ˆè½¯åˆ é™¤ï¼‰ã€‚"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    data = request.get_json(silent=True) or {}
    report_names = unique_non_empty_strings(data.get("report_names", []))

    if not report_names:
        return jsonify({"error": "report_names ä¸èƒ½ä¸ºç©º"}), 400

    deleted_reports = []
    skipped_reports = []
    missing_reports = []

    for report_name in report_names:
        report_file, owner_error = enforce_report_owner_or_404(report_name, user_id)
        if owner_error:
            if report_file is None and not (REPORTS_DIR / report_name).exists():
                missing_reports.append(report_name)
            else:
                skipped_reports.append({"name": report_name, "reason": "æ— æƒé™æˆ–æŠ¥å‘Šä¸å­˜åœ¨"})
            continue

        try:
            mark_report_as_deleted(report_name)
            deleted_reports.append(report_name)
        except Exception as exc:
            skipped_reports.append({"name": report_name, "reason": f"æ ‡è®°åˆ é™¤å¤±è´¥: {str(exc)}"})

    return jsonify({
        "success": True,
        "requested": len(report_names),
        "deleted_reports": deleted_reports,
        "skipped_reports": skipped_reports,
        "missing_reports": missing_reports
    })


# ============ çŠ¶æ€ API ============

@app.route('/api/status', methods=['GET'])
def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    mode_names = ["quick", "standard", "deep"]
    mode_configs_effective = {
        mode: get_interview_mode_display_config(mode)
        for mode in mode_names
    }
    return jsonify({
        "status": "running",
        "ai_available": claude_client is not None,
        "model": QUESTION_MODEL_NAME if claude_client else None,
        "question_model": QUESTION_MODEL_NAME if claude_client else None,
        "report_model": REPORT_MODEL_NAME if claude_client else None,
        "sessions_dir": str(SESSIONS_DIR),
        "reports_dir": str(REPORTS_DIR),
        "interview_depth_v2": {
            "enabled": True,
            "modes": mode_names,
            "deep_mode_skip_followup_confirm": DEEP_MODE_SKIP_FOLLOWUP_CONFIRM,
            "mode_configs": mode_configs_effective,
        }
    })


@app.route('/api/status/web-search', methods=['GET'])
def get_web_search_status():
    """è·å– Web Search API è°ƒç”¨çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯å‘¼å¸ç¯æ•ˆæœï¼‰"""
    return jsonify({
        "active": web_search_active
    })


@app.route('/api/status/thinking/<session_id>', methods=['GET'])
def get_thinking_status(session_id):
    """è·å– AI æ€è€ƒè¿›åº¦çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯åˆ†é˜¶æ®µè¿›åº¦å±•ç¤ºï¼‰"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        return jsonify({"active": False})

    with thinking_status_lock:
        status = thinking_status.get(session_id)

    if status:
        return jsonify({
            "active": True,
            "stage": status["stage"],
            "stage_index": status["stage_index"],
            "total_stages": status["total_stages"],
            "message": status["message"],
        })
    else:
        return jsonify({"active": False})


@app.route('/api/status/report-generation/<session_id>', methods=['GET'])
def get_report_generation_status(session_id):
    """è·å–æŠ¥å‘Šç”Ÿæˆè¿›åº¦çŠ¶æ€"""
    user_id = get_current_user_id_or_none()
    if not user_id:
        return jsonify({"error": "è¯·å…ˆç™»å½•"}), 401

    loaded = load_session_for_user(session_id, user_id)
    if len(loaded) == 3:
        return jsonify({"active": False})

    status = get_report_generation_record(session_id)
    if status and not bool(status.get("active")) and is_report_generation_worker_alive(session_id):
        state = str(status.get("state") or "").strip()
        if state not in {"completed", "failed", "cancelled"}:
            update_report_generation_status(session_id, "queued", message="æŠ¥å‘Šä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­...")
            status = get_report_generation_record(session_id)

    if status:
        return jsonify(build_report_generation_payload(status))

    return jsonify({"active": False, "processing": False})


@app.route('/api/metrics', methods=['GET'])
def get_metrics():
    """è·å– API æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯"""
    last_n = request.args.get('last_n', type=int)
    stats = metrics_collector.get_statistics(last_n=last_n)
    return jsonify(stats)


@app.route('/api/metrics/reset', methods=['POST'])
def reset_metrics():
    """é‡ç½®æ€§èƒ½æŒ‡æ ‡ï¼ˆæ¸…ç©ºå†å²æ•°æ®ï¼‰"""
    try:
        metrics_collector.metrics_file.write_text(json.dumps({
            "calls": [],
            "summary": {
                "total_calls": 0,
                "total_timeouts": 0,
                "total_truncations": 0,
                "avg_response_time": 0,
                "avg_prompt_length": 0
            }
        }, ensure_ascii=False, indent=2), encoding="utf-8")
        return jsonify({"success": True, "message": "æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®"})
    except Exception as e:
        return jsonify({"error": f"é‡ç½®å¤±è´¥: {e}"}), 500


@app.route('/api/summaries', methods=['GET'])
def get_summaries_info():
    """è·å–æ™ºèƒ½æ‘˜è¦ç¼“å­˜ä¿¡æ¯"""
    try:
        cache_files = list(SUMMARIES_DIR.glob("*.txt"))
        total_size = sum(f.stat().st_size for f in cache_files)

        return jsonify({
            "enabled": ENABLE_SMART_SUMMARY,
            "cache_enabled": SUMMARY_CACHE_ENABLED,
            "threshold": SMART_SUMMARY_THRESHOLD,
            "target_length": SMART_SUMMARY_TARGET,
            "cached_count": len(cache_files),
            "cache_size_bytes": total_size,
            "cache_size_kb": round(total_size / 1024, 2),
            "cache_directory": str(SUMMARIES_DIR)
        })
    except Exception as e:
        return jsonify({"error": f"è·å–æ‘˜è¦ä¿¡æ¯å¤±è´¥: {e}"}), 500


@app.route('/api/summaries/clear', methods=['POST'])
def clear_summaries_cache():
    """æ¸…ç©ºæ™ºèƒ½æ‘˜è¦ç¼“å­˜"""
    try:
        cache_files = list(SUMMARIES_DIR.glob("*.txt"))
        deleted_count = 0
        for f in cache_files:
            try:
                f.unlink()
                deleted_count += 1
            except Exception:
                pass

        return jsonify({
            "success": True,
            "message": f"å·²æ¸…ç©º {deleted_count} ä¸ªæ‘˜è¦ç¼“å­˜",
            "deleted_count": deleted_count
        })
    except Exception as e:
        return jsonify({"error": f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}"}), 500


if __name__ == '__main__':
    print("=" * 60)
    print("Deep Vision Web Server - AI é©±åŠ¨ç‰ˆæœ¬")
    print("=" * 60)
    print(f"Sessions: {SESSIONS_DIR}")
    print(f"Reports: {REPORTS_DIR}")
    print(f"AI çŠ¶æ€: {'å·²å¯ç”¨' if claude_client else 'æœªå¯ç”¨'}")
    if claude_client:
        if QUESTION_MODEL_NAME == REPORT_MODEL_NAME:
            print(f"æ¨¡å‹: {QUESTION_MODEL_NAME}")
        else:
            print(f"é—®é¢˜æ¨¡å‹: {QUESTION_MODEL_NAME}")
            print(f"æŠ¥å‘Šæ¨¡å‹: {REPORT_MODEL_NAME}")

    # æœç´¢åŠŸèƒ½çŠ¶æ€
    search_enabled = ENABLE_WEB_SEARCH and ZHIPU_API_KEY and ZHIPU_API_KEY != "your-zhipu-api-key-here"
    print(f"è”ç½‘æœç´¢: {'âœ… å·²å¯ç”¨ (æ™ºè°±AI MCP)' if search_enabled else 'âš ï¸  æœªå¯ç”¨'}")
    if not search_enabled and ENABLE_WEB_SEARCH:
        print("   æç¤º: é…ç½® ZHIPU_API_KEY ä»¥å¯ç”¨è”ç½‘æœç´¢åŠŸèƒ½")

    print()
    print(f"è®¿é—®: http://localhost:{SERVER_PORT}")
    print("=" * 60)
    app.run(host=SERVER_HOST, port=SERVER_PORT, debug=DEBUG_MODE)
